<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-18T02:13:56+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "uk"
}
-->
# Генерація з доповненням пошуку (RAG) та векторні бази даних

[![Генерація з доповненням пошуку (RAG) та векторні бази даних](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.uk.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

У уроці про пошукові застосунки ми коротко розглянули, як інтегрувати власні дані у великі мовні моделі (LLMs). У цьому уроці ми детальніше розглянемо концепцію прив'язки ваших даних до застосунків LLM, механізми цього процесу та методи зберігання даних, включаючи як векторні представлення, так і текст.

> **Відео скоро з'явиться**

## Вступ

У цьому уроці ми розглянемо наступне:

- Вступ до RAG: що це таке і чому його використовують в штучному інтелекті.

- Розуміння, що таке векторні бази даних, і створення однієї для нашого застосунку.

- Практичний приклад інтеграції RAG у застосунок.

## Цілі навчання

Після завершення цього уроку ви зможете:

- Пояснити важливість RAG у пошуку та обробці даних.

- Налаштувати застосунок RAG і прив'язати ваші дані до LLM.

- Ефективно інтегрувати RAG та векторні бази даних у застосунки LLM.

## Наш сценарій: покращення LLM за допомогою власних даних

У цьому уроці ми хочемо додати власні нотатки до освітнього стартапу, що дозволить чат-боту отримати більше інформації про різні теми. Використовуючи наші нотатки, учні зможуть краще вивчати матеріал і розуміти різні теми, що полегшить підготовку до іспитів. Для створення нашого сценарію ми використаємо:

- `Azure OpenAI:` LLM, який ми будемо використовувати для створення нашого чат-бота.

- `Урок для початківців з нейронних мереж:` це будуть дані, на яких ми будемо базувати наш LLM.

- `Azure AI Search` та `Azure Cosmos DB:` векторна база даних для зберігання наших даних і створення пошукового індексу.

Користувачі зможуть створювати практичні тести з своїх нотаток, картки для повторення та стислий огляд матеріалу. Щоб розпочати, давайте розглянемо, що таке RAG і як він працює:

## Генерація з доповненням пошуку (RAG)

Чат-бот, який працює на основі LLM, обробляє запити користувачів для генерації відповідей. Він створений для інтерактивного спілкування і може обговорювати широкий спектр тем. Однак його відповіді обмежені контекстом, який йому надано, і його базовими навчальними даними. Наприклад, GPT-4 має обмеження знань до вересня 2021 року, тобто він не знає про події, які сталися після цього періоду. Крім того, дані, які використовуються для навчання LLM, виключають конфіденційну інформацію, таку як особисті нотатки або інструкції до продуктів компанії.

### Як працюють RAG (Генерація з доповненням пошуку)

![схема роботи RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.uk.png)

Припустимо, ви хочете запустити чат-бота, який створює тести з ваших нотаток, вам знадобиться підключення до бази знань. Тут на допомогу приходить RAG. RAG працює наступним чином:

- **База знань:** Перед пошуком ці документи потрібно завантажити та попередньо обробити, зазвичай розбиваючи великі документи на менші частини, перетворюючи їх у текстові векторні представлення та зберігаючи їх у базі даних.

- **Запит користувача:** користувач задає питання.

- **Пошук:** Коли користувач задає питання, модель векторного представлення знаходить відповідну інформацію в нашій базі знань, щоб надати більше контексту, який буде включений у запит.

- **Доповнена генерація:** LLM покращує свою відповідь на основі отриманих даних. Це дозволяє генерувати відповідь, яка базується не лише на попередньо навчених даних, але й на актуальній інформації з доданого контексту. Отримані дані використовуються для доповнення відповідей LLM. Потім LLM повертає відповідь на запит користувача.

![схема архітектури RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.uk.png)

Архітектура RAG реалізується за допомогою трансформерів, які складаються з двох частин: енкодера та декодера. Наприклад, коли користувач задає питання, текст запиту "кодується" у вектори, які захоплюють значення слів, а потім ці вектори "декодуються" у наш індекс документів і генерують новий текст на основі запиту користувача. LLM використовує модель енкодера-декодера для генерації вихідних даних.

Два підходи до реалізації RAG, згідно з запропонованою статтею: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst):

- **_RAG-Sequence_** використовує отримані документи для прогнозування найкращої можливої відповіді на запит користувача.

- **RAG-Token** використовує документи для генерації наступного токена, а потім отримує їх для відповіді на запит користувача.

### Чому варто використовувати RAG? 

- **Інформаційна насиченість:** забезпечує актуальність і сучасність текстових відповідей. Це покращує продуктивність у завданнях, специфічних для певної галузі, завдяки доступу до внутрішньої бази знань.

- Зменшує вигадування, використовуючи **перевірені дані** з бази знань для надання контексту до запитів користувачів.

- Це **економічно вигідно**, оскільки вони дешевші порівняно з тонким налаштуванням LLM.

## Створення бази знань

Наш застосунок базується на наших особистих даних, тобто на уроці про нейронні мережі з навчальної програми AI For Beginners.

### Векторні бази даних

Векторна база даних, на відміну від традиційних баз даних, є спеціалізованою базою даних, призначеною для зберігання, управління та пошуку векторних представлень. Вона зберігає числові представлення документів. Розбиття даних на числові векторні представлення полегшує розуміння та обробку даних нашою системою штучного інтелекту.

Ми зберігаємо наші векторні представлення у векторних базах даних, оскільки LLM мають обмеження на кількість токенів, які вони приймають як вхідні дані. Оскільки неможливо передати всі векторні представлення до LLM, нам потрібно розбити їх на частини, і коли користувач задає питання, векторні представлення, найбільш схожі на запит, будуть повернуті разом із запитом. Розбиття також знижує витрати на кількість токенів, які проходять через LLM.

Деякі популярні векторні бази даних включають Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant і DeepLake. Ви можете створити модель Azure Cosmos DB за допомогою Azure CLI за допомогою наступної команди:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Від тексту до векторних представлень

Перед тим як зберігати наші дані, нам потрібно перетворити їх у векторні представлення перед збереженням у базі даних. Якщо ви працюєте з великими документами або довгими текстами, ви можете розбити їх на частини, виходячи з очікуваних запитів. Розбиття можна виконати на рівні речення або абзацу. Оскільки розбиття отримує значення з слів навколо них, ви можете додати деякий контекст до частини, наприклад, додавши заголовок документа або включивши текст перед або після частини. Ви можете розбити дані наступним чином:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Після розбиття ми можемо перетворити наш текст у векторні представлення, використовуючи різні моделі векторизації. Деякі моделі, які ви можете використовувати, включають: word2vec, ada-002 від OpenAI, Azure Computer Vision та багато інших. Вибір моделі залежатиме від мов, які ви використовуєте, типу контенту, що кодується (текст/зображення/аудіо), розміру вхідних даних, які вона може кодувати, та довжини вихідного векторного представлення.

Приклад векторного представлення тексту за допомогою моделі OpenAI `text-embedding-ada-002`:
![векторне представлення слова cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.uk.png)

## Пошук і векторний пошук

Коли користувач задає питання, система перетворює його у вектор за допомогою енкодера запитів, а потім шукає у нашому індексі документів відповідні вектори, пов'язані з введенням. Після цього вона перетворює як вхідний вектор, так і вектори документів у текст і передає їх через LLM.

### Пошук

Пошук відбувається, коли система намагається швидко знайти документи з індексу, які відповідають критеріям пошуку. Мета пошукової системи — отримати документи, які будуть використані для надання контексту та прив'язки LLM до ваших даних.

Існує кілька способів виконання пошуку в нашій базі даних, таких як:

- **Пошук за ключовими словами** - використовується для текстових пошуків.

- **Семантичний пошук** - використовує семантичне значення слів.

- **Векторний пошук** - перетворює документи з тексту у векторні представлення за допомогою моделей векторизації. Пошук буде виконуватися шляхом запиту документів, чиї векторні представлення найближчі до запиту користувача.

- **Гібридний** - комбінація пошуку за ключовими словами та векторного пошуку.

Проблема з пошуком виникає, коли в базі даних немає схожої відповіді на запит, система тоді повертає найкращу інформацію, яку вона може знайти. Однак ви можете використовувати такі тактики, як встановлення максимальної відстані для релевантності або використання гібридного пошуку, який комбінує пошук за ключовими словами та векторний пошук. У цьому уроці ми будемо використовувати гібридний пошук, комбінацію векторного та пошуку за ключовими словами. Ми збережемо наші дані у фреймі даних з колонками, що містять частини тексту та векторні представлення.

### Векторна схожість

Пошукова система буде шукати у базі знань векторні представлення, які знаходяться близько один до одного, найближчого сусіда, оскільки це тексти, які схожі. У випадку, коли користувач задає запит, він спочатку перетворюється у вектор, а потім порівнюється з подібними векторними представленнями. Загальним методом вимірювання схожості між різними векторами є косинусна схожість, яка базується на куті між двома векторами.

Ми можемо вимірювати схожість, використовуючи інші альтернативи, такі як евклідова відстань, яка є прямою лінією між кінцевими точками векторів, та скалярний добуток, який вимірює суму добутків відповідних елементів двох векторів.

### Пошуковий індекс

Під час виконання пошуку нам потрібно створити пошуковий індекс для нашої бази знань перед виконанням пошуку. Індекс буде зберігати наші векторні представлення і зможе швидко знаходити найбільш схожі частини навіть у великій базі даних. Ми можемо створити наш індекс локально, використовуючи:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Перерейтинг

Після того, як ви виконали запит до бази даних, можливо, вам знадобиться відсортувати результати від найбільш релевантних. Перерейтинг LLM використовує машинне навчання для покращення релевантності результатів пошуку, впорядковуючи їх від найбільш релевантних. Використовуючи Azure AI Search, перерейтинг виконується автоматично за допомогою семантичного перерейтера. Приклад того, як працює перерейтинг, використовуючи найближчих сусідів:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Об'єднання всього разом

Останній крок — додати наш LLM до системи, щоб отримувати відповіді, які базуються на наших даних. Ми можемо реалізувати це наступним чином:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Оцінка нашого застосунку

### Метрики оцінки

- Якість наданих відповідей, забезпечення їх природності, плавності та схожості на людські.

- Прив'язаність даних: оцінка того, чи відповідь походить з наданих документів.

- Релевантність: оцінка того, чи відповідає відповідь запиту і чи пов'язана з ним.

- Плавність - чи відповідає відповідь граматичним нормам.

## Сфери застосування RAG (Генерація з доповненням пошуку) та векторних баз даних

Існує багато різних сфер застосування, де виклики функцій можуть покращити ваш застосунок, наприклад:

- Питання та відповіді: прив'язка даних вашої компанії до чату, який можуть використовувати співробітники для запитів.

- Системи рекомендацій: створення системи, яка підбирає найбільш схожі значення, наприклад, фільми, ресторани тощо.

- Послуги чат-ботів: зберігання історії чату та персоналізація розмови на основі даних користувача.

- Пошук зображень на основі векторних представлень, корисний для розпізнавання зображень та виявлення аномалій.

## Підсумок

Ми розглянули основні аспекти RAG, від додавання наших даних до застосунку, до запиту користувача та отримання результату. Щоб спростити створення RAG, ви можете використовувати такі фреймворки, як Semantic Kernel, Langchain або Autogen.

## Завдання

Для продовження навчання про Генерацію з доповненням пошуку (RAG) ви можете:

- Створити фронтенд для застосунку, використовуючи обраний вами фреймворк.

- Використати фреймворк, наприклад LangChain або Semantic Kernel, і відтворити ваш застосунок.

Вітаємо з завершенням уроку 👏.

## Навчання не закінчується тут, продовжуйте свій шлях

Після завершення цього уроку ознайомтеся з нашою [колекцією навчальних матеріалів про генеративний AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб продовжити вдосконалювати свої знання про генеративний AI!

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.