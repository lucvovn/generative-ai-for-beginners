<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-18T02:12:29+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "uk"
}
-->
# Захист ваших додатків генеративного AI

[![Захист ваших додатків генеративного AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.uk.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Вступ

У цьому уроці розглядається:

- Безпека в контексті систем штучного інтелекту.
- Поширені ризики та загрози для систем AI.
- Методи та аспекти захисту систем AI.

## Цілі навчання

Після завершення цього уроку ви зрозумієте:

- Загрози та ризики для систем AI.
- Поширені методи та практики захисту систем AI.
- Як впровадження тестування безпеки може запобігти несподіваним результатам і втраті довіри користувачів.

## Що означає безпека в контексті генеративного AI?

Оскільки технології штучного інтелекту (AI) та машинного навчання (ML) все більше впливають на наше життя, важливо захищати не лише дані клієнтів, але й самі системи AI. AI/ML все частіше використовуються для підтримки процесів прийняття рішень у галузях, де неправильне рішення може мати серйозні наслідки.

Ось ключові моменти, які варто врахувати:

- **Вплив AI/ML**: AI/ML мають значний вплив на повсякденне життя, тому їх захист стає необхідністю.
- **Виклики безпеки**: Вплив AI/ML потребує належної уваги, щоб забезпечити захист продуктів на основі AI від складних атак, як з боку тролів, так і організованих груп.
- **Стратегічні проблеми**: Технологічна індустрія повинна проактивно вирішувати стратегічні виклики, щоб забезпечити довгострокову безпеку клієнтів і даних.

Крім того, моделі машинного навчання здебільшого не здатні розрізняти шкідливі дані та нешкідливі аномальні дані. Значна частина навчальних даних отримується з некурованих, немодерованих, загальнодоступних наборів даних, які відкриті для внесків сторонніх осіб. Зловмисникам не потрібно зламувати набори даних, якщо вони можуть вільно додавати до них свої дані. З часом дані з низькою довірою стають даними з високою довірою, якщо їх структура/формат залишаються правильними.

Ось чому важливо забезпечити цілісність і захист сховищ даних, які ваші моделі використовують для прийняття рішень.

## Розуміння загроз і ризиків AI

У контексті AI та пов'язаних систем найбільш значною загрозою безпеці сьогодні є отруєння даних. Отруєння даних відбувається, коли хтось навмисно змінює інформацію, яка використовується для навчання AI, що призводить до помилок у його роботі. Це пов'язано з відсутністю стандартизованих методів виявлення та пом'якшення наслідків, а також з нашою залежністю від недовірених або некурованих загальнодоступних наборів даних для навчання. Щоб зберегти цілісність даних і запобігти помилковому процесу навчання, важливо відстежувати походження та історію ваших даних. Інакше стара приказка "сміття на вході — сміття на виході" залишається актуальною, що призводить до погіршення продуктивності моделі.

Ось приклади того, як отруєння даних може вплинути на ваші моделі:

1. **Перевертання міток**: У задачі бінарної класифікації зловмисник навмисно змінює мітки невеликої частини навчальних даних. Наприклад, нешкідливі зразки позначаються як шкідливі, що змушує модель вивчати неправильні асоціації.\
   **Приклад**: Фільтр спаму неправильно класифікує легітимні електронні листи як спам через змінені мітки.
2. **Отруєння ознак**: Зловмисник тонко змінює ознаки в навчальних даних, щоб ввести упередження або ввести модель в оману.\
   **Приклад**: Додавання нерелевантних ключових слів до описів продуктів для маніпулювання системами рекомендацій.
3. **Ін'єкція даних**: Введення шкідливих даних у навчальний набір для впливу на поведінку моделі.\
   **Приклад**: Введення фальшивих відгуків користувачів для спотворення результатів аналізу настроїв.
4. **Атаки з прихованими пастками**: Зловмисник вставляє прихований шаблон (пастку) у навчальні дані. Модель навчається розпізнавати цей шаблон і поводиться шкідливо, коли його активують.\
   **Приклад**: Система розпізнавання облич, навчена на зображеннях із прихованими пастками, неправильно ідентифікує конкретну особу.

Корпорація MITRE створила [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), базу знань про тактики та методи, які використовують зловмисники в реальних атаках на системи AI.

> З кожним днем зростає кількість вразливостей у системах, що використовують AI, оскільки інтеграція AI збільшує поверхню атаки існуючих систем, виходячи за межі традиційних кіберзагроз. Ми розробили ATLAS, щоб підвищити обізнаність про ці унікальні та еволюційні вразливості, оскільки глобальна спільнота все більше інтегрує AI у різні системи. ATLAS моделюється за зразком фреймворку MITRE ATT&CK® і його тактики, техніки та процедури (TTPs) доповнюють ті, що є в ATT&CK.

Подібно до фреймворку MITRE ATT&CK®, який широко використовується в традиційній кібербезпеці для планування сценаріїв емулювання загроз, ATLAS надає легко доступний набір TTPs, які допомагають краще зрозуміти та підготуватися до захисту від нових атак.

Крім того, Open Web Application Security Project (OWASP) створив "[Топ-10 список](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" найкритичніших вразливостей, виявлених у додатках, що використовують LLM. У списку висвітлюються ризики загроз, таких як згадане вище отруєння даних, а також інших, таких як:

- **Ін'єкція підказок**: техніка, коли зловмисники маніпулюють великими мовними моделями (LLM) за допомогою ретельно створених введень, змушуючи їх поводитися не так, як передбачено.
- **Вразливості ланцюга постачання**: Компоненти та програмне забезпечення, які складають додатки, що використовуються LLM, такі як модулі Python або зовнішні набори даних, можуть бути скомпрометовані, що призводить до несподіваних результатів, введення упереджень і навіть вразливостей в основній інфраструктурі.
- **Надмірна залежність**: LLM можуть помилятися і схильні до "галюцинацій", надаючи неточні або небезпечні результати. У кількох задокументованих випадках люди сприймали результати за чисту монету, що призводило до небажаних негативних наслідків у реальному світі.

Microsoft Cloud Advocate Род Трент написав безкоштовну електронну книгу [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), яка глибоко розглядає ці та інші нові загрози AI і надає детальні рекомендації щодо найкращих способів вирішення цих сценаріїв.

## Тестування безпеки для систем AI та LLM

Штучний інтелект (AI) трансформує різні сфери та галузі, відкриваючи нові можливості та переваги для суспільства. Однак AI також створює значні виклики та ризики, такі як конфіденційність даних, упередження, відсутність пояснюваності та потенційне зловживання. Тому важливо забезпечити, щоб системи AI були безпечними та відповідальними, тобто відповідали етичним і правовим стандартам і могли викликати довіру у користувачів та зацікавлених сторін.

Тестування безпеки — це процес оцінки безпеки системи AI або LLM шляхом виявлення та використання їхніх вразливостей. Це можуть виконувати розробники, користувачі або сторонні аудитори, залежно від мети та обсягу тестування. Деякі з найпоширеніших методів тестування безпеки для систем AI та LLM включають:

- **Санітарна обробка даних**: Це процес видалення або анонімізації конфіденційної або приватної інформації з навчальних даних або введення системи AI чи LLM. Санітарна обробка даних може допомогти запобігти витоку даних і шкідливій маніпуляції, зменшуючи ризик розкриття конфіденційної або особистої інформації.
- **Атаки зловмисників**: Це процес створення та застосування зловмисних прикладів до введення або виведення системи AI чи LLM для оцінки її стійкості до атак. Атаки зловмисників можуть допомогти виявити та пом'якшити вразливості та слабкі місця системи AI чи LLM, які можуть бути використані зловмисниками.
- **Перевірка моделі**: Це процес перевірки правильності та повноти параметрів або архітектури моделі системи AI чи LLM. Перевірка моделі може допомогти виявити та запобігти крадіжці моделі, забезпечуючи її захист і автентифікацію.
- **Перевірка результатів**: Це процес перевірки якості та надійності результатів системи AI чи LLM. Перевірка результатів може допомогти виявити та виправити шкідливу маніпуляцію, забезпечуючи їхню узгодженість і точність.

OpenAI, лідер у сфері систем AI, створив серію _оцінок безпеки_ в рамках своєї ініціативи з тестування мережі, спрямованої на перевірку результатів систем AI з метою сприяння безпеці AI.

> Оцінки можуть варіюватися від простих тестів запитань і відповідей до більш складних симуляцій. Ось конкретні приклади оцінок, розроблених OpenAI для оцінки поведінки AI з різних точок зору:

#### Переконання

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може змусити іншу систему AI сказати секретне слово?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може переконати іншу систему AI пожертвувати гроші?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може вплинути на підтримку іншою системою AI політичної пропозиції?

#### Стеганографія (приховані повідомлення)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може передавати секретні повідомлення, не будучи викритою іншою системою AI?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може стискати та розпаковувати повідомлення, щоб приховати секретні повідомлення?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Наскільки добре система AI може координуватися з іншою системою AI без прямого спілкування?

### Безпека AI

Важливо прагнути захистити системи AI від шкідливих атак, зловживань або непередбачених наслідків. Це включає вжиття заходів для забезпечення безпеки, надійності та довіри до систем AI, таких як:

- Захист даних і алгоритмів, які використовуються для навчання та роботи моделей AI.
- Запобігання несанкціонованому доступу, маніпуляціям або саботажу систем AI.
- Виявлення та пом'якшення упереджень, дискримінації або етичних проблем у системах AI.
- Забезпечення відповідальності, прозорості та пояснюваності рішень і дій AI.
- Узгодження цілей і цінностей систем AI з цілями та цінностями людей і суспільства.

Безпека AI важлива для забезпечення цілісності, доступності та конфіденційності систем AI та даних. Деякі виклики та можливості безпеки AI включають:

- **Можливість**: Інтеграція AI у стратегії кібербезпеки, оскільки він може відігравати важливу роль у виявленні загроз і покращенні часу реагування. AI може допомогти автоматизувати та посилити виявлення та пом'якшення кіберзагроз, таких як фішинг, шкідливе програмне забезпечення або програми-вимагачі.
- **Виклик**: AI також може бути використаний зловмисниками для запуску складних атак, таких як створення фальшивого або оманливого контенту, видавання себе за користувачів або використання вразливостей у системах AI. Тому розробники AI несуть унікальну відповідальність за створення систем, які є стійкими до зловживань.

### Захист даних

LLM можуть становити ризик для конфіденційності та безпеки даних, які вони використовують. Наприклад, LLM можуть потенційно запам'ятовувати та розкривати конфіденційну інформацію з їхніх навчальних даних, таку як особисті імена, адреси, паролі або номери кредитних карток. Вони також можуть бути маніпульовані або атаковані зловмисниками, які хочуть використати їхні вразливості або упередження. Тому важливо бути обізнаними про ці ризики та вживати відповідних заходів для захисту даних, які використовуються з LLM. Існує кілька кроків, які ви можете зробити для захисту даних, які використовуються з LLM. Ці кроки включають:

- **Обмеження кількості та типу даних, які ви ділитеся з LLM**: Діліться лише тими даними, які є необхідними та релевантними для запланованих цілей, і уникайте передачі будь-яких даних, які є конфіденційними, приватними або особистими. Користувачі також повинні анонімізувати або шифрувати дані, які вони передають LLM, наприклад, видаляючи або маскуючи будь-яку ідентифікаційну інформацію або використовуючи захищені канали зв'язку.
- **Перевірка даних, які генерують LLM**: Завжди перевіряйте точність і якість результатів, які генерують LLM, щоб переконатися, що вони не містять небажаної або недоречної інформації.
- **Повідомлення про витоки даних або інциденти**: Будьте уважними до будь-яких підозрілих або аномальних дій чи поведінки LLM, таких як створення текстів, які є недоречними,
Імітація реальних загроз зараз вважається стандартною практикою у створенні стійких систем штучного інтелекту, використовуючи схожі інструменти, тактики, процедури для виявлення ризиків для систем і тестування реакції захисників.

> Практика тестування штучного інтелекту (AI red teaming) еволюціонувала, набуваючи більш розширеного значення: вона охоплює не лише пошук вразливостей безпеки, але й перевірку інших збоїв системи, таких як генерація потенційно шкідливого контенту. Системи штучного інтелекту мають нові ризики, і тестування є ключовим для розуміння цих нових ризиків, таких як ін'єкція запитів і створення непідтвердженого контенту. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Рекомендації та ресурси для тестування](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.uk.png)]()

Нижче наведено ключові висновки, які сформували програму Microsoft AI Red Team.

1. **Розширений обсяг тестування AI:**
   Тестування AI тепер охоплює як аспекти безпеки, так і результати відповідального використання штучного інтелекту (RAI). Традиційно тестування зосереджувалося на аспектах безпеки, розглядаючи модель як вектор (наприклад, крадіжка базової моделі). Однак системи штучного інтелекту вводять нові вразливості безпеки (наприклад, ін'єкція запитів, отруєння), що потребує особливої уваги. Окрім безпеки, тестування AI також досліджує питання справедливості (наприклад, стереотипізація) і шкідливий контент (наприклад, прославлення насильства). Раннє виявлення цих проблем дозволяє визначити пріоритети інвестицій у захист.

2. **Зловмисні та добросовісні збої:**
   Тестування AI враховує збої як з точки зору зловмисних, так і добросовісних дій. Наприклад, під час тестування нового Bing ми досліджуємо не лише те, як зловмисники можуть підривати систему, але й як звичайні користувачі можуть зіткнутися з проблемним або шкідливим контентом. На відміну від традиційного тестування безпеки, яке здебільшого зосереджене на зловмисних акторах, тестування AI враховує ширший спектр персон і потенційних збоїв.

3. **Динамічна природа систем AI:**
   Застосування штучного інтелекту постійно еволюціонує. У додатках на основі великих мовних моделей розробники адаптуються до змінних вимог. Постійне тестування забезпечує безперервну пильність і адаптацію до змінних ризиків.

Тестування AI не є всеосяжним і має розглядатися як додатковий елемент до інших заходів, таких як [контроль доступу на основі ролей (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) і комплексні рішення для управління даними. Воно покликане доповнювати стратегію безпеки, яка зосереджена на впровадженні безпечних і відповідальних рішень штучного інтелекту, що враховують конфіденційність і безпеку, прагнучи мінімізувати упередження, шкідливий контент і дезінформацію, які можуть підірвати довіру користувачів.

Ось список додаткових матеріалів, які допоможуть вам краще зрозуміти, як тестування може допомогти виявити та зменшити ризики у ваших системах штучного інтелекту:

- [Планування тестування для великих мовних моделей (LLMs) та їх застосувань](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Що таке мережа тестування OpenAI?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [Тестування AI - ключова практика для створення безпечних і відповідальних рішень штучного інтелекту](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), база знань про тактики і техніки, які використовуються зловмисниками в реальних атаках на системи штучного інтелекту.

## Перевірка знань

Який підхід може бути ефективним для підтримання цілісності даних і запобігання їх неправильному використанню?

1. Використовувати сильний контроль доступу на основі ролей для управління даними
1. Впроваджувати та перевіряти маркування даних, щоб запобігти їх неправильному представленню або використанню
1. Забезпечити підтримку фільтрації контенту вашою інфраструктурою штучного інтелекту

A:1, Хоча всі три рекомендації є чудовими, забезпечення правильного призначення привілеїв доступу до даних користувачам значно допоможе запобігти маніпуляціям і неправильному представленню даних, які використовуються великими мовними моделями.

## 🚀 Виклик

Дізнайтеся більше про те, як [управляти та захищати конфіденційну інформацію](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) в епоху штучного інтелекту.

## Чудова робота, продовжуйте навчання

Після завершення цього уроку перегляньте нашу [колекцію навчальних матеріалів про генеративний штучний інтелект](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб продовжити вдосконалювати свої знання про генеративний штучний інтелект!

Перейдіть до уроку 14, де ми розглянемо [життєвий цикл застосування генеративного штучного інтелекту](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.