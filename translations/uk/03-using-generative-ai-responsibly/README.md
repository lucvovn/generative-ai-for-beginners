<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-18T02:13:07+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "uk"
}
-->
# Використання генеративного штучного інтелекту відповідально

[![Використання генеративного штучного інтелекту відповідально](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.uk.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _Натисніть на зображення вище, щоб переглянути відео цього уроку_

Легко захопитися штучним інтелектом, особливо генеративним, але важливо враховувати, як використовувати його відповідально. Потрібно подумати про те, як забезпечити справедливість, уникнути шкоди та багато іншого. Цей розділ має на меті надати вам необхідний контекст, на що звернути увагу і як зробити активні кроки для покращення використання штучного інтелекту.

## Вступ

Цей урок охоплює:

- Чому важливо приділяти увагу відповідальному використанню штучного інтелекту при створенні додатків на основі генеративного штучного інтелекту.
- Основні принципи відповідального штучного інтелекту та їх зв'язок із генеративним штучним інтелектом.
- Як застосувати ці принципи відповідального штучного інтелекту на практиці за допомогою стратегій та інструментів.

## Цілі навчання

Після завершення цього уроку ви дізнаєтеся:

- Чому важливо використовувати відповідальний штучний інтелект при створенні додатків на основі генеративного штучного інтелекту.
- Коли слід враховувати та застосовувати основні принципи відповідального штучного інтелекту при створенні додатків на основі генеративного штучного інтелекту.
- Які інструменти та стратегії доступні для впровадження концепції відповідального штучного інтелекту.

## Принципи відповідального штучного інтелекту

Захоплення генеративним штучним інтелектом досягло небувалих висот. Це привернуло багато нових розробників, увагу та фінансування до цієї сфери. Хоча це дуже позитивно для тих, хто хоче створювати продукти та компанії, використовуючи генеративний штучний інтелект, важливо діяти відповідально.

Протягом цього курсу ми зосереджуємося на створенні нашого стартапу та освітнього продукту на основі штучного інтелекту. Ми будемо використовувати принципи відповідального штучного інтелекту: справедливість, інклюзивність, надійність/безпека, захист даних, прозорість та відповідальність. Завдяки цим принципам ми дослідимо, як вони стосуються використання генеративного штучного інтелекту в наших продуктах.

## Чому слід приділяти увагу відповідальному штучному інтелекту

При створенні продукту, орієнтуючись на людину та враховуючи найкращі інтереси користувача, можна досягти найкращих результатів.

Унікальність генеративного штучного інтелекту полягає в його здатності створювати корисні відповіді, інформацію, рекомендації та контент для користувачів. Це можна зробити без багатьох ручних кроків, що може призвести до дуже вражаючих результатів. Однак без належного планування та стратегій це також може, на жаль, призвести до шкідливих наслідків для ваших користувачів, продукту та суспільства загалом.

Розглянемо деякі (але не всі) потенційно шкідливі наслідки:

### Галюцинації

Галюцинації — це термін, який описує ситуацію, коли LLM створює контент, що є або абсолютно безглуздим, або явно неправильним з точки зору інших джерел інформації.

Наприклад, ми створюємо функцію для нашого стартапу, яка дозволяє студентам ставити історичні запитання моделі. Студент запитує: `Хто був єдиним вижившим на Титаніку?`

Модель видає відповідь, наприклад, таку:

![Запит "Хто був єдиним вижившим на Титаніку"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Джерело: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Це дуже впевнена та детальна відповідь. На жаль, вона неправильна. Навіть при мінімальному дослідженні можна дізнатися, що виживших після катастрофи Титаніка було більше одного. Для студента, який тільки починає досліджувати цю тему, така відповідь може бути достатньо переконливою, щоб її не ставити під сумнів і прийняти за факт. Наслідки цього можуть призвести до ненадійності системи штучного інтелекту та негативно вплинути на репутацію нашого стартапу.

З кожною ітерацією будь-якої даної LLM ми спостерігаємо покращення продуктивності щодо мінімізації галюцинацій. Однак навіть з цими покращеннями ми, як розробники додатків та користувачі, повинні залишатися уважними до цих обмежень.

### Шкідливий контент

Ми вже розглянули ситуації, коли LLM створює неправильні або безглузді відповіді. Ще одним ризиком, про який потрібно пам'ятати, є ситуації, коли модель відповідає шкідливим контентом.

Шкідливий контент можна визначити як:

- Надання інструкцій або заохочення до самопошкодження чи шкоди певним групам.
- Ненависний або принизливий контент.
- Допомога у плануванні будь-яких атак чи насильницьких дій.
- Надання інструкцій щодо пошуку незаконного контенту або вчинення незаконних дій.
- Відображення сексуально відвертого контенту.

Для нашого стартапу ми хочемо переконатися, що маємо правильні інструменти та стратегії, щоб запобігти появі такого контенту для студентів.

### Відсутність справедливості

Справедливість визначається як "забезпечення того, щоб система штучного інтелекту була вільною від упереджень та дискримінації і ставилася до всіх справедливо та рівноправно". У світі генеративного штучного інтелекту ми хочемо переконатися, що виключні світогляди маргіналізованих груп не підкріплюються результатами моделі.

Такі результати не тільки руйнують позитивний досвід використання продукту для наших користувачів, але й завдають додаткової шкоди суспільству. Як розробники додатків, ми завжди повинні враховувати широку та різноманітну базу користувачів при створенні рішень з генеративним штучним інтелектом.

## Як використовувати генеративний штучний інтелект відповідально

Тепер, коли ми визначили важливість відповідального генеративного штучного інтелекту, давайте розглянемо 4 кроки, які ми можемо зробити, щоб створювати наші AI-рішення відповідально:

![Цикл пом'якшення](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.uk.png)

### Оцінка потенційної шкоди

У тестуванні програмного забезпечення ми перевіряємо очікувані дії користувача в додатку. Аналогічно, тестування різноманітного набору запитів, які, ймовірно, будуть використовуватися користувачами, є хорошим способом оцінки потенційної шкоди.

Оскільки наш стартап створює освітній продукт, було б добре підготувати список запитів, пов'язаних з освітою. Це може охоплювати певні предмети, історичні факти та запити про студентське життя.

### Пом'якшення потенційної шкоди

Тепер настав час знайти способи, як ми можемо запобігти або обмежити потенційну шкоду, спричинену моделлю та її відповідями. Ми можемо розглянути це на 4 різних рівнях:

![Шари пом'якшення](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.uk.png)

- **Модель**. Вибір правильної моделі для відповідного випадку використання. Великі та складні моделі, такі як GPT-4, можуть створювати більший ризик шкідливого контенту, якщо їх застосовувати до менших і більш специфічних випадків використання. Використання ваших навчальних даних для тонкого налаштування також знижує ризик шкідливого контенту.

- **Система безпеки**. Система безпеки — це набір інструментів та конфігурацій на платформі, яка обслуговує модель, що допомагає пом'якшити шкоду. Прикладом цього є система фільтрації контенту в Azure OpenAI Service. Системи також повинні виявляти атаки на безпеку та небажану активність, наприклад, запити від ботів.

- **Метапідказка**. Метапідказки та прив'язка — це способи, за допомогою яких ми можемо спрямовувати або обмежувати модель на основі певної поведінки та інформації. Це може бути використання системних входів для визначення певних меж моделі. Крім того, надання вихідних даних, які є більш релевантними для сфери або домену системи.

Це також може бути використання таких технік, як генерація з розширеним пошуком (RAG), щоб модель отримувала інформацію лише з вибраних надійних джерел. У цьому курсі є урок про [створення пошукових додатків](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Користувацький досвід**. Останній шар — це взаємодія користувача безпосередньо з моделлю через інтерфейс нашого додатку. У цьому випадку ми можемо спроєктувати UI/UX таким чином, щоб обмежити користувача у типах запитів, які він може надсилати моделі, а також у тексті або зображеннях, які відображаються користувачу. При розгортанні AI-додатку ми також повинні бути прозорими щодо того, що наш генеративний AI-додаток може і не може робити.

У нас є цілий урок, присвячений [Проєктуванню UX для AI-додатків](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Оцінка моделі**. Робота з LLM може бути складною, оскільки ми не завжди маємо контроль над даними, на яких модель була навчена. Незважаючи на це, ми завжди повинні оцінювати продуктивність та результати моделі. Важливо вимірювати точність моделі, схожість, обґрунтованість та релевантність результатів. Це допомагає забезпечити прозорість та довіру з боку зацікавлених сторін та користувачів.

### Експлуатація відповідального рішення на основі генеративного штучного інтелекту

Створення операційної практики навколо ваших AI-додатків — це фінальний етап. Це включає співпрацю з іншими частинами нашого стартапу, такими як юридичний відділ та служба безпеки, щоб забезпечити відповідність усім нормативним політикам. Перед запуском ми також хочемо розробити плани щодо доставки, обробки інцидентів та відкату, щоб запобігти будь-якій шкоді для наших користувачів.

## Інструменти

Хоча робота над розробкою рішень відповідального штучного інтелекту може здатися складною, це робота, яка варта зусиль. У міру зростання сфери генеративного штучного інтелекту більше інструментів для допомоги розробникам у ефективній інтеграції відповідальності в їх робочі процеси буде вдосконалюватися. Наприклад, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) може допомогти виявляти шкідливий контент та зображення через API-запит.

## Перевірка знань

На що потрібно звернути увагу, щоб забезпечити відповідальне використання штучного інтелекту?

1. Щоб відповідь була правильною.  
2. Шкідливе використання, щоб AI не використовувався для злочинних цілей.  
3. Забезпечення того, щоб AI був вільним від упереджень та дискримінації.  

Відповідь: правильні варіанти 2 і 3. Відповідальний AI допомагає враховувати, як зменшити шкідливі ефекти, упередження та інше.

## 🚀 Виклик

Прочитайте про [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) і дізнайтеся, що ви можете застосувати у своїй роботі.

## Чудова робота, продовжуйте навчання

Після завершення цього уроку ознайомтеся з нашою [колекцією навчальних матеріалів про генеративний штучний інтелект](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб продовжити вдосконалювати свої знання про генеративний штучний інтелект!

Перейдіть до уроку 4, де ми розглянемо [Основи інженерії підказок](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.