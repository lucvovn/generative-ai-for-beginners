<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T15:17:15+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "mo"
}
-->
[![開源模型](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.mo.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# 微調您的大型語言模型 (LLM)

使用大型語言模型來構建生成式 AI 應用程式會帶來新的挑戰。一個關鍵問題是確保模型生成的內容在回應使用者請求時的質量（準確性和相關性）。在之前的課程中，我們討論了像提示工程和檢索增強生成這樣的技術，這些技術試圖通過_修改現有模型的提示輸入_來解決這個問題。

今天的課程中，我們將討論第三種技術，**微調**，它試圖通過_使用額外的數據重新訓練模型本身_來解決這一挑戰。讓我們深入了解細節。

## 學習目標

本課程介紹了對預訓練語言模型進行微調的概念，探討了這種方法的優勢和挑戰，並提供了何時以及如何使用微調來提高生成式 AI 模型性能的指導。

完成本課程後，您應該能回答以下問題：

- 什麼是語言模型的微調？
- 微調在什麼時候以及為什麼有用？
- 我如何微調預訓練模型？
- 微調的局限性是什麼？

準備好了嗎？讓我們開始吧。

## 圖解指南

想在深入學習之前了解我們將涵蓋的內容的大致框架嗎？查看這份圖解指南，它描述了本課程的學習旅程——從學習微調的核心概念和動機，到理解執行微調任務的過程和最佳實踐。這是一個值得探索的有趣主題，因此不要忘記查看[資源](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面，獲取支持您自主學習旅程的額外鏈接！

![語言模型微調圖解指南](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.mo.png)

## 什麼是語言模型的微調？

根據定義，大型語言模型是基於從包括互聯網在內的多種來源獲取的大量文本進行_預訓練_的。正如我們在之前的課程中所學，我們需要像_提示工程_和_檢索增強生成_這樣的技術來提高模型對使用者問題（“提示”）的回應質量。

一種流行的提示工程技術是通過提供_指令_（明確指導）或_提供一些示例_（隱含指導）來給模型更多關於期望回應的指導。這被稱為_少量示例學習_，但它有兩個局限性：

- 模型的 token 限制可能會限制您能提供的示例數量，並降低效果。
- 模型的 token 成本可能會使每次提示添加示例變得昂貴，並限制靈活性。

微調是機器學習系統中的一種常見做法，我們通過使用新數據重新訓練預訓練模型來提高其在特定任務上的性能。在語言模型的背景下，我們可以通過_為特定任務或應用領域準備的精選示例集_來微調預訓練模型，從而創建一個**定制模型**，該模型可能在特定任務或領域上更準確和相關。微調的一個附帶好處是，它還可以減少少量示例學習所需的示例數量——降低 token 使用量和相關成本。

## 什麼時候以及為什麼需要微調模型？

在_這個_背景下，當我們談論微調時，我們指的是**監督式**微調，其中重新訓練是通過**添加新數據**（這些數據不屬於原始訓練數據集）來完成的。這與非監督式微調方法不同，後者是在原始數據上重新訓練模型，但使用不同的超參數。

需要記住的關鍵是，微調是一種需要一定專業知識才能獲得預期結果的高級技術。如果操作不當，它可能無法提供預期的改進，甚至可能降低模型在目標領域的性能。

因此，在學習“如何”微調語言模型之前，您需要知道“為什麼”應該採取這條路徑，以及“什麼時候”開始微調過程。首先問自己以下問題：

- **使用案例**：您的微調_使用案例_是什麼？您希望改進當前預訓練模型的哪個方面？
- **替代方案**：您是否嘗試過_其他技術_來實現預期結果？使用它們來創建比較基準。
  - 提示工程：嘗試使用相關提示回應示例的少量示例提示技術。評估回應的質量。
  - 檢索增強生成：嘗試通過搜索您的數據檢索查詢結果來增強提示。評估回應的質量。
- **成本**：您是否確定了微調的成本？
  - 可調性——預訓練模型是否可進行微調？
  - 努力——準備訓練數據、評估和改進模型所需的工作量。
  - 計算——運行微調作業和部署微調模型所需的計算資源。
  - 數據——是否有足夠的高質量示例來影響微調效果。
- **收益**：您是否確認了微調的收益？
  - 質量——微調模型是否超越了基準？
  - 成本——是否通過簡化提示減少了 token 使用量？
  - 可擴展性——是否可以將基礎模型重新用於新領域？

通過回答這些問題，您應該能夠決定微調是否適合您的使用案例。理想情況下，只有當收益超過成本時，這種方法才是有效的。一旦您決定採取行動，就可以開始思考_如何_微調預訓練模型。

想了解更多關於決策過程的見解？觀看[是否需要微調](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## 我們如何微調預訓練模型？

要微調預訓練模型，您需要具備：

- 一個可進行微調的預訓練模型
- 用於微調的數據集
- 運行微調作業的訓練環境
- 部署微調模型的託管環境

## 微調實踐

以下資源提供了逐步教程，通過選定模型和精選數據集的實例向您展示微調的過程。要完成這些教程，您需要在特定提供商處擁有帳戶，並獲得相關模型和數據集的訪問權限。

| 提供商       | 教程                                                                                                                                                                       | 描述                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [如何微調聊天模型](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                        | 學習如何微調 `gpt-35-turbo` 以適應特定領域（“食譜助手”），包括準備訓練數據、運行微調作業以及使用微調模型進行推理。                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo 微調教程](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst)         | 學習如何在 **Azure** 上微調 `gpt-35-turbo-0613` 模型，包括創建和上傳訓練數據、運行微調作業、部署和使用新模型的步驟。                                                                                                                                                                                                                                                                 |
| Hugging Face | [使用 Hugging Face 微調 LLM](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                                   | 這篇博客文章向您展示如何使用 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 庫和 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) 微調_開源 LLM_（例如：`CodeLlama 7B`），並使用 Hugging Face 上的開源[數據集](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)。 |
|              |                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🤗 AutoTrain | [使用 AutoTrain 微調 LLM](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                             | AutoTrain（或 AutoTrain Advanced）是由 Hugging Face 開發的 Python 庫，支持多種任務的微調，包括 LLM 微調。AutoTrain 是一種無需編碼的解決方案，微調可以在您自己的雲端、Hugging Face Spaces 或本地完成。它支持基於網頁的 GUI、CLI 和通過 yaml 配置文件進行的訓練。                                                                               |
|              |                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## 作業

選擇上述教程之一並完成它們。_我們可能會在此 repo 中複製這些教程的 Jupyter Notebook 版本僅供參考。請直接使用原始來源以獲取最新版本_。

## 幹得好！繼續學習。

完成本課程後，查看我們的[生成式 AI 學習合集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，繼續提升您的生成式 AI 知識！

恭喜您！您已完成本課程 v2 系列的最後一課！不要停止學習和構建。\*\*查看[資源](RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面，獲取僅針對此主題的額外建議列表。

我們的 v1 系列課程也已更新，增加了更多作業和概念。因此花點時間刷新您的知識——並請[分享您的問題和反饋](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)，幫助我們為社群改進這些課程。

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。