<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T23:08:42+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "fa"
}
-->
# ایمن‌سازی برنامه‌های هوش مصنوعی تولیدی

[![ایمن‌سازی برنامه‌های هوش مصنوعی تولیدی](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.fa.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## مقدمه

این درس شامل موارد زیر خواهد بود:

- امنیت در زمینه سیستم‌های هوش مصنوعی.
- خطرات و تهدیدات رایج برای سیستم‌های هوش مصنوعی.
- روش‌ها و ملاحظات برای ایمن‌سازی سیستم‌های هوش مصنوعی.

## اهداف یادگیری

پس از تکمیل این درس، شما درک خواهید کرد:

- تهدیدات و خطرات سیستم‌های هوش مصنوعی.
- روش‌ها و شیوه‌های رایج برای ایمن‌سازی سیستم‌های هوش مصنوعی.
- چگونه اجرای تست‌های امنیتی می‌تواند از نتایج غیرمنتظره و کاهش اعتماد کاربران جلوگیری کند.

## امنیت در زمینه هوش مصنوعی تولیدی به چه معناست؟

با توجه به اینکه فناوری‌های هوش مصنوعی (AI) و یادگیری ماشین (ML) به طور فزاینده‌ای زندگی ما را شکل می‌دهند، حفاظت از داده‌های مشتریان و همچنین خود سیستم‌های هوش مصنوعی بسیار مهم است. هوش مصنوعی و یادگیری ماشین به طور فزاینده‌ای در فرآیندهای تصمیم‌گیری با ارزش بالا در صنایع استفاده می‌شوند، جایی که تصمیم اشتباه ممکن است عواقب جدی به همراه داشته باشد.

نکات کلیدی که باید در نظر گرفت:

- **تأثیر هوش مصنوعی و یادگیری ماشین**: هوش مصنوعی و یادگیری ماشین تأثیرات قابل توجهی بر زندگی روزمره دارند و به همین دلیل حفاظت از آنها ضروری شده است.
- **چالش‌های امنیتی**: این تأثیرات نیازمند توجه مناسب هستند تا نیاز به حفاظت از محصولات مبتنی بر هوش مصنوعی در برابر حملات پیچیده، چه توسط افراد مخرب یا گروه‌های سازمان‌یافته، برآورده شود.
- **مشکلات استراتژیک**: صنعت فناوری باید به طور فعال چالش‌های استراتژیک را برای تضمین ایمنی طولانی‌مدت مشتریان و امنیت داده‌ها برطرف کند.

علاوه بر این، مدل‌های یادگیری ماشین عمدتاً قادر به تشخیص بین ورودی‌های مخرب و داده‌های غیرعادی بی‌ضرر نیستند. منبع قابل توجهی از داده‌های آموزشی از مجموعه داده‌های عمومی بدون نظارت و بدون مدیریت به دست می‌آید که برای مشارکت‌های شخص ثالث باز هستند. مهاجمان نیازی به نفوذ به مجموعه داده‌ها ندارند وقتی که می‌توانند آزادانه در آنها مشارکت کنند. با گذشت زمان، داده‌های مخرب با اعتماد کم به داده‌های مورد اعتماد با اعتماد بالا تبدیل می‌شوند، اگر ساختار/قالب داده‌ها صحیح باقی بماند.

به همین دلیل، اطمینان از یکپارچگی و حفاظت از ذخایر داده‌ای که مدل‌های شما برای تصمیم‌گیری از آنها استفاده می‌کنند، بسیار حیاتی است.

## درک تهدیدات و خطرات هوش مصنوعی

در زمینه هوش مصنوعی و سیستم‌های مرتبط، مسمومیت داده‌ها به عنوان مهم‌ترین تهدید امنیتی امروز برجسته می‌شود. مسمومیت داده‌ها زمانی رخ می‌دهد که کسی عمداً اطلاعاتی را که برای آموزش هوش مصنوعی استفاده می‌شود تغییر دهد و باعث شود که هوش مصنوعی اشتباه کند. این به دلیل نبود روش‌های استاندارد برای تشخیص و کاهش این مشکل، همراه با وابستگی ما به مجموعه داده‌های عمومی غیرقابل اعتماد یا بدون نظارت برای آموزش است. برای حفظ یکپارچگی داده‌ها و جلوگیری از فرآیند آموزشی ناقص، پیگیری منبع و منشأ داده‌های خود بسیار مهم است. در غیر این صورت، ضرب‌المثل قدیمی "زباله وارد، زباله خارج" صادق است و منجر به عملکرد ضعیف مدل می‌شود.

در اینجا نمونه‌هایی از تأثیر مسمومیت داده‌ها بر مدل‌های شما آورده شده است:

1. **تغییر برچسب‌ها**: در یک وظیفه طبقه‌بندی دودویی، یک مهاجم عمداً برچسب‌های یک زیرمجموعه کوچک از داده‌های آموزشی را تغییر می‌دهد. به عنوان مثال، نمونه‌های بی‌ضرر به عنوان مخرب برچسب‌گذاری می‌شوند و باعث می‌شود مدل ارتباطات نادرست را یاد بگیرد.\
   **مثال**: یک فیلتر اسپم که ایمیل‌های قانونی را به اشتباه به عنوان اسپم طبقه‌بندی می‌کند به دلیل برچسب‌های دستکاری‌شده.
2. **مسمومیت ویژگی‌ها**: یک مهاجم به طور ظریف ویژگی‌های موجود در داده‌های آموزشی را تغییر می‌دهد تا تعصب ایجاد کند یا مدل را گمراه کند.\
   **مثال**: افزودن کلمات کلیدی غیرمرتبط به توضیحات محصولات برای دستکاری سیستم‌های توصیه.
3. **تزریق داده‌ها**: تزریق داده‌های مخرب به مجموعه آموزشی برای تأثیرگذاری بر رفتار مدل.\
   **مثال**: معرفی نظرات جعلی کاربران برای تغییر نتایج تحلیل احساسات.
4. **حملات درب پشتی**: یک مهاجم الگوی مخفی (درب پشتی) را در داده‌های آموزشی وارد می‌کند. مدل یاد می‌گیرد این الگو را شناسایی کند و هنگام فعال شدن، رفتار مخرب نشان می‌دهد.\
   **مثال**: یک سیستم تشخیص چهره که با تصاویر دارای درب پشتی آموزش داده شده و یک فرد خاص را به اشتباه شناسایی می‌کند.

شرکت MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) را ایجاد کرده است، یک پایگاه دانش از تاکتیک‌ها و تکنیک‌هایی که توسط مهاجمان در حملات واقعی به سیستم‌های هوش مصنوعی استفاده می‌شود.

> تعداد آسیب‌پذیری‌ها در سیستم‌های مجهز به هوش مصنوعی در حال افزایش است، زیرا استفاده از هوش مصنوعی سطح حمله سیستم‌های موجود را فراتر از حملات سایبری سنتی گسترش می‌دهد. ما ATLAS را برای افزایش آگاهی از این آسیب‌پذیری‌های منحصر به فرد و در حال تحول توسعه دادیم، زیرا جامعه جهانی به طور فزاینده‌ای هوش مصنوعی را در سیستم‌های مختلف ادغام می‌کند. ATLAS بر اساس چارچوب MITRE ATT&CK® مدل‌سازی شده است و تاکتیک‌ها، تکنیک‌ها و روش‌های آن مکمل موارد موجود در ATT&CK هستند.

مشابه چارچوب MITRE ATT&CK® که به طور گسترده در امنیت سایبری سنتی برای برنامه‌ریزی سناریوهای شبیه‌سازی تهدید پیشرفته استفاده می‌شود، ATLAS مجموعه‌ای از تاکتیک‌ها و تکنیک‌های قابل جستجو را ارائه می‌دهد که می‌تواند به درک بهتر و آماده‌سازی برای دفاع در برابر حملات نوظهور کمک کند.

علاوه بر این، پروژه امنیتی اپلیکیشن‌های وب باز (OWASP) یک "[لیست ۱۰ مورد برتر](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" از مهم‌ترین آسیب‌پذیری‌های موجود در برنامه‌های کاربردی که از مدل‌های زبانی بزرگ (LLM) استفاده می‌کنند، ایجاد کرده است. این لیست خطرات تهدیداتی مانند مسمومیت داده‌ها و همچنین موارد دیگر مانند:

- **تزریق درخواست**: تکنیکی که در آن مهاجمان یک مدل زبانی بزرگ (LLM) را از طریق ورودی‌های طراحی‌شده به دقت دستکاری می‌کنند و باعث می‌شوند خارج از رفتار مورد نظر عمل کند.
- **آسیب‌پذیری‌های زنجیره تأمین**: اجزا و نرم‌افزارهایی که برنامه‌های کاربردی استفاده‌شده توسط یک LLM را تشکیل می‌دهند، مانند ماژول‌های پایتون یا مجموعه داده‌های خارجی، خودشان ممکن است آسیب‌پذیر باشند و منجر به نتایج غیرمنتظره، تعصبات واردشده و حتی آسیب‌پذیری‌هایی در زیرساخت‌های اساسی شوند.
- **اتکای بیش از حد**: مدل‌های زبانی بزرگ خطاپذیر هستند و ممکن است دچار توهم شوند و نتایج نادرست یا ناامن ارائه دهند. در چندین مورد مستند، افراد نتایج را به صورت سطحی پذیرفته‌اند که منجر به پیامدهای منفی غیرمنتظره در دنیای واقعی شده است.

راد ترنت، مشاور ابری مایکروسافت، یک کتاب الکترونیکی رایگان با عنوان [باید امنیت هوش مصنوعی را یاد گرفت](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) نوشته است که به طور عمیق به این تهدیدات نوظهور هوش مصنوعی می‌پردازد و راهنمایی‌های گسترده‌ای در مورد نحوه بهترین مقابله با این سناریوها ارائه می‌دهد.

## تست امنیتی برای سیستم‌های هوش مصنوعی و مدل‌های زبانی بزرگ

هوش مصنوعی (AI) در حال تحول در حوزه‌ها و صنایع مختلف است و امکانات و مزایای جدیدی را برای جامعه ارائه می‌دهد. با این حال، هوش مصنوعی چالش‌ها و خطرات قابل توجهی نیز به همراه دارد، مانند حریم خصوصی داده‌ها، تعصب، عدم توضیح‌پذیری و سوءاستفاده احتمالی. بنابراین، ضروری است که اطمینان حاصل شود که سیستم‌های هوش مصنوعی امن و مسئولانه هستند، به این معنا که از استانداردهای اخلاقی و قانونی پیروی می‌کنند و می‌توانند توسط کاربران و ذینفعان اعتماد شوند.

تست امنیتی فرآیند ارزیابی امنیت یک سیستم هوش مصنوعی یا مدل زبانی بزرگ (LLM) است که با شناسایی و بهره‌برداری از آسیب‌پذیری‌های آنها انجام می‌شود. این کار می‌تواند توسط توسعه‌دهندگان، کاربران یا حسابرسان شخص ثالث انجام شود، بسته به هدف و دامنه تست. برخی از روش‌های رایج تست امنیتی برای سیستم‌های هوش مصنوعی و مدل‌های زبانی بزرگ عبارتند از:

- **پاکسازی داده‌ها**: این فرآیند حذف یا ناشناس‌سازی اطلاعات حساس یا خصوصی از داده‌های آموزشی یا ورودی یک سیستم هوش مصنوعی یا مدل زبانی بزرگ است. پاکسازی داده‌ها می‌تواند از نشت داده‌ها و دستکاری مخرب جلوگیری کند و با کاهش افشای داده‌های محرمانه یا شخصی، امنیت را افزایش دهد.
- **تست خصمانه**: این فرآیند تولید و اعمال مثال‌های خصمانه به ورودی یا خروجی یک سیستم هوش مصنوعی یا مدل زبانی بزرگ برای ارزیابی مقاومت و پایداری آن در برابر حملات خصمانه است. تست خصمانه می‌تواند آسیب‌پذیری‌ها و نقاط ضعف یک سیستم هوش مصنوعی یا مدل زبانی بزرگ را که ممکن است توسط مهاجمان بهره‌برداری شود، شناسایی و کاهش دهد.
- **تأیید مدل**: این فرآیند تأیید صحت و کامل بودن پارامترهای مدل یا معماری یک سیستم هوش مصنوعی یا مدل زبانی بزرگ است. تأیید مدل می‌تواند از سرقت مدل جلوگیری کند و اطمینان حاصل کند که مدل محافظت شده و معتبر است.
- **اعتبارسنجی خروجی**: این فرآیند اعتبارسنجی کیفیت و قابلیت اعتماد خروجی یک سیستم هوش مصنوعی یا مدل زبانی بزرگ است. اعتبارسنجی خروجی می‌تواند دستکاری مخرب را شناسایی و اصلاح کند و اطمینان حاصل کند که خروجی سازگار و دقیق است.

OpenAI، یکی از پیشروان در سیستم‌های هوش مصنوعی، مجموعه‌ای از _ارزیابی‌های ایمنی_ را به عنوان بخشی از ابتکار شبکه تیم قرمز خود راه‌اندازی کرده است که هدف آن آزمایش خروجی سیستم‌های هوش مصنوعی به امید کمک به ایمنی هوش مصنوعی است.

> ارزیابی‌ها می‌توانند از تست‌های ساده پرسش و پاسخ تا شبیه‌سازی‌های پیچیده‌تر متغیر باشند. به عنوان مثال‌های مشخص، در اینجا نمونه‌هایی از ارزیابی‌هایی که توسط OpenAI برای ارزیابی رفتارهای هوش مصنوعی از زوایای مختلف توسعه داده شده‌اند، آورده شده است:

#### متقاعدسازی

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند یک سیستم هوش مصنوعی دیگر را متقاعد کند که یک کلمه مخفی را بگوید؟
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند یک سیستم هوش مصنوعی دیگر را متقاعد کند که پول اهدا کند؟
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند حمایت یک سیستم هوش مصنوعی دیگر از یک پیشنهاد سیاسی را تحت تأثیر قرار دهد؟

#### استگانوگرافی (پیام‌های مخفی)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند پیام‌های مخفی را بدون اینکه توسط یک سیستم هوش مصنوعی دیگر شناسایی شود، ارسال کند؟
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند پیام‌ها را فشرده و باز کند تا امکان ارسال پیام‌های مخفی فراهم شود؟
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): چقدر یک سیستم هوش مصنوعی می‌تواند بدون ارتباط مستقیم با یک سیستم هوش مصنوعی دیگر هماهنگ شود؟

### امنیت هوش مصنوعی

ضروری است که تلاش کنیم سیستم‌های هوش مصنوعی را از حملات مخرب، سوءاستفاده یا پیامدهای ناخواسته محافظت کنیم. این شامل اقداماتی برای تضمین ایمنی، قابلیت اعتماد و قابلیت اطمینان سیستم‌های هوش مصنوعی می‌شود، مانند:

- ایمن‌سازی داده‌ها و الگوریتم‌هایی که برای آموزش و اجرای مدل‌های هوش مصنوعی استفاده می‌شوند
- جلوگیری از دسترسی غیرمجاز، دستکاری یا خرابکاری سیستم‌های هوش مصنوعی
- شناسایی و کاهش تعصب، تبعیض یا مسائل اخلاقی در سیستم‌های هوش مصنوعی
- تضمین مسئولیت‌پذیری، شفافیت و توضیح‌پذیری تصمیمات و اقدامات هوش مصنوعی
- هماهنگ کردن اهداف و ارزش‌های سیستم‌های هوش مصنوعی با اهداف و ارزش‌های انسان‌ها و جامعه

امنیت هوش مصنوعی برای تضمین یکپارچگی، دسترسی و محرمانگی سیستم‌ها و داده‌های هوش مصنوعی مهم است. برخی از چالش‌ها و فرصت‌های امنیت هوش مصنوعی عبارتند از:

- **فرصت**: ادغام هوش مصنوعی در استراتژی‌های امنیت سایبری، زیرا می‌تواند نقش مهمی در شناسایی تهدیدات و بهبود زمان پاسخگویی ایفا کند. هوش مصنوعی می‌تواند به خودکارسازی و تقویت شناسایی و کاهش حملات سایبری، مانند فیشینگ، بدافزار یا باج‌افزار کمک کند.
- **چالش**: هوش مصنوعی همچنین می‌تواند توسط مهاجمان برای راه‌اندازی حملات پیچیده استفاده شود، مانند تولید محتوای جعلی یا گمراه‌کننده، جعل هویت کاربران یا بهره‌برداری از آسیب‌پذیری‌های سیستم‌های هوش مصنوعی. بنابراین، توسعه‌دهندگان هوش مصنوعی مسئولیت ویژه‌ای دارند تا سیستم‌هایی طراحی کنند که در برابر سوءاستفاده مقاوم و پایدار باشند.

### حفاظت از داده‌ها

مدل‌های زبانی بزرگ (LLM) ممکن است خطراتی برای حریم خصوصی و امنیت داده‌هایی که استفاده می‌کنند ایجاد کنند. به عنوان مثال، مدل‌های زبانی بزرگ ممکن است اطلاعات حساس را از داده‌های آموزشی خود به خاطر بسپارند و نشت کنند، مانند نام‌های شخصی، آدرس‌ها، رمزهای عبور یا شماره‌های کارت اعتباری. آنها همچنین ممکن است توسط بازیگران مخرب که می‌خواهند از آسیب‌پذیری‌ها یا تعصبات آنها سوءاستفاده کنند، دستکاری یا مورد حمله قرار گیرند. بنابراین، مهم است که از این خطرات آگاه باشید و اقدامات مناسب برای حفاظت از داده‌های استفاده‌شده با مدل‌های زبانی بزرگ انجام دهید. چندین گام وجود دارد که می‌توانید برای حفاظت از داده‌های استفاده‌شده با مدل‌های زبانی بزرگ انجام دهید. این گام‌ها شامل موارد زیر هستند:

- **محدود کردن مقدار و نوع داده‌هایی که با مدل‌های زبانی بزرگ به اشتراک می‌گذارید**: فقط داده‌هایی را که ضروری و مرتبط با اهداف مورد نظر هستند به اشتراک بگذارید و از اشتراک‌گذاری هرگونه داده حساس، محرمانه یا شخصی خودداری کنید. کاربران همچنین باید داده‌هایی را که با مدل‌های زبانی بزرگ به اشتراک می‌گذارند ناشناس یا رمزگذاری کنند، مانند حذف یا ماسک کردن هرگونه اطلاعات شناسایی یا استفاده از کانال‌های ارتباطی امن.
- **تأیید داده‌هایی که مدل‌های زبانی بزرگ تولید می‌کنند**: همیشه دقت و کیفیت خروجی تولیدشده توسط مدل‌های زبانی بزرگ را بررسی کنید تا مطمئن شوید که حاوی اطلاعات ناخواسته یا نامناسب نیستند.
- **گزارش و هشدار هرگونه نقض داده یا حادثه**: نسبت به هرگونه فعالیت یا رفتار مشکوک یا غیرعادی از مدل‌های زبانی بزرگ هوشیار باشید، مانند تولید متن‌هایی که نامربوط، نادرست، توهین‌آمیز یا مضر هستند. این ممکن است نشانه‌ای از نقض داده یا حادثه امنیتی باشد.

امنیت داده‌ها، حاکمیت و رعایت قوانین برای هر سازمانی که می‌خواهد از قدرت داده‌ها و هوش مصنوعی در محیط چند ابری بهره‌مند شود، حیاتی است. ایمن‌
شبیه‌سازی تهدیدات دنیای واقعی اکنون به عنوان یک روش استاندارد در ساخت سیستم‌های هوش مصنوعی مقاوم شناخته می‌شود که با استفاده از ابزارها، تاکتیک‌ها و روش‌های مشابه برای شناسایی خطرات سیستم‌ها و آزمایش واکنش مدافعان انجام می‌شود.

> تمرین تیم قرمز هوش مصنوعی به معنای گسترده‌تری تکامل یافته است: این تمرین نه تنها شامل بررسی آسیب‌پذیری‌های امنیتی می‌شود، بلکه شامل بررسی دیگر شکست‌های سیستم نیز می‌شود، مانند تولید محتوای بالقوه مضر. سیستم‌های هوش مصنوعی با خطرات جدیدی همراه هستند و تیم قرمز نقش اساسی در درک این خطرات جدید دارد، مانند تزریق درخواست و تولید محتوای بی‌پایه. - [تیم قرمز هوش مصنوعی مایکروسافت آینده‌ای امن‌تر برای هوش مصنوعی می‌سازد](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![راهنمایی و منابع برای تیم قرمز](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.fa.png)]()

در ادامه نکات کلیدی که برنامه تیم قرمز هوش مصنوعی مایکروسافت را شکل داده‌اند آورده شده است.

1. **دامنه گسترده تیم قرمز هوش مصنوعی:**
   تیم قرمز هوش مصنوعی اکنون شامل نتایج امنیتی و هوش مصنوعی مسئولانه (RAI) می‌شود. به طور سنتی، تیم قرمز بر جنبه‌های امنیتی تمرکز داشت و مدل را به عنوان یک بردار در نظر می‌گرفت (مانند سرقت مدل اصلی). با این حال، سیستم‌های هوش مصنوعی آسیب‌پذیری‌های امنیتی جدیدی معرفی می‌کنند (مانند تزریق درخواست، مسموم‌سازی)، که نیازمند توجه ویژه هستند. فراتر از امنیت، تیم قرمز هوش مصنوعی همچنین مسائل مربوط به عدالت (مانند کلیشه‌سازی) و محتوای مضر (مانند تمجید از خشونت) را بررسی می‌کند. شناسایی زودهنگام این مسائل امکان اولویت‌بندی سرمایه‌گذاری‌های دفاعی را فراهم می‌کند.
2. **شکست‌های مخرب و غیرمخرب:**
   تیم قرمز هوش مصنوعی شکست‌ها را از دیدگاه‌های مخرب و غیرمخرب در نظر می‌گیرد. به عنوان مثال، هنگام بررسی تیم قرمز بینگ جدید، ما نه تنها بررسی می‌کنیم که چگونه مهاجمان مخرب می‌توانند سیستم را مختل کنند، بلکه بررسی می‌کنیم که چگونه کاربران عادی ممکن است با محتوای مشکل‌ساز یا مضر مواجه شوند. برخلاف تیم قرمز امنیتی سنتی که عمدتاً بر بازیگران مخرب تمرکز دارد، تیم قرمز هوش مصنوعی طیف گسترده‌تری از شخصیت‌ها و شکست‌های احتمالی را در نظر می‌گیرد.
3. **ماهیت پویا سیستم‌های هوش مصنوعی:**
   برنامه‌های کاربردی هوش مصنوعی به طور مداوم در حال تکامل هستند. در برنامه‌های کاربردی مدل‌های زبانی بزرگ، توسعه‌دهندگان با توجه به نیازهای در حال تغییر سازگار می‌شوند. تیم قرمز مداوم تضمین می‌کند که نظارت و سازگاری با خطرات در حال تغییر ادامه یابد.

تیم قرمز هوش مصنوعی همه‌جانبه نیست و باید به عنوان یک حرکت مکمل برای کنترل‌های اضافی مانند [کنترل دسترسی مبتنی بر نقش (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) و راه‌حل‌های جامع مدیریت داده در نظر گرفته شود. این روش به عنوان مکملی برای استراتژی امنیتی طراحی شده است که بر استفاده از راه‌حل‌های هوش مصنوعی ایمن و مسئولانه تمرکز دارد و حریم خصوصی و امنیت را در نظر می‌گیرد، در حالی که تلاش می‌کند تعصبات، محتوای مضر و اطلاعات غلطی که می‌توانند اعتماد کاربران را کاهش دهند، به حداقل برساند.

در اینجا فهرستی از منابع اضافی آورده شده است که می‌تواند به شما کمک کند تا بهتر درک کنید که چگونه تیم قرمز می‌تواند به شناسایی و کاهش خطرات در سیستم‌های هوش مصنوعی شما کمک کند:

- [برنامه‌ریزی تیم قرمز برای مدل‌های زبانی بزرگ (LLMs) و برنامه‌های کاربردی آن‌ها](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [شبکه تیم قرمز OpenAI چیست؟](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [تیم قرمز هوش مصنوعی - یک تمرین کلیدی برای ساخت راه‌حل‌های هوش مصنوعی ایمن‌تر و مسئولانه‌تر](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (چشم‌انداز تهدیدات خصمانه برای سیستم‌های هوش مصنوعی)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)، یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های استفاده شده توسط مهاجمان در حملات واقعی به سیستم‌های هوش مصنوعی.

## بررسی دانش

چه رویکردی می‌تواند برای حفظ یکپارچگی داده‌ها و جلوگیری از سوءاستفاده مناسب باشد؟

1. داشتن کنترل‌های قوی مبتنی بر نقش برای دسترسی به داده‌ها و مدیریت داده‌ها  
1. پیاده‌سازی و بررسی برچسب‌گذاری داده‌ها برای جلوگیری از سوءتفسیر یا سوءاستفاده از داده‌ها  
1. اطمینان از اینکه زیرساخت هوش مصنوعی شما از فیلتر کردن محتوا پشتیبانی می‌کند  

A:1، در حالی که هر سه توصیه عالی هستند، اطمینان از اینکه شما دسترسی مناسب به داده‌ها را به کاربران اختصاص می‌دهید، نقش مهمی در جلوگیری از دستکاری و سوءتفسیر داده‌های استفاده شده توسط مدل‌های زبانی بزرگ دارد.

## 🚀 چالش

بیشتر مطالعه کنید درباره اینکه چگونه می‌توانید [اطلاعات حساس را مدیریت و محافظت کنید](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) در عصر هوش مصنوعی.

## کار عالی، ادامه یادگیری شما

پس از تکمیل این درس، مجموعه یادگیری [هوش مصنوعی مولد](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ما را بررسی کنید تا دانش خود را در زمینه هوش مصنوعی مولد ارتقا دهید!

به درس ۱۴ بروید، جایی که چرخه عمر برنامه‌های هوش مصنوعی مولد را بررسی خواهیم کرد. [چرخه عمر برنامه‌های هوش مصنوعی مولد](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.