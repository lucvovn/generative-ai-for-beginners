<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-18T01:54:16+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "my"
}
-->
# သင့် Generative AI အက်ပလီကေးရှင်းများကို လုံခြုံစေခြင်း

[![သင့် Generative AI အက်ပလီကေးရှင်းများကို လုံခြုံစေခြင်း](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.my.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## အကျဉ်းချုပ်

ဒီသင်ခန်းစာမှာ အောက်ပါအကြောင်းအရာများကို လေ့လာပါမည်-

- AI စနစ်များအတွင်း လုံခြုံရေးအကြောင်း
- AI စနစ်များအပေါ် ရှိနေသော အန္တရာယ်များနှင့် ခြိမ်းခြောက်မှုများ
- AI စနစ်များကို လုံခြုံစေဖို့ နည်းလမ်းများနှင့် စဉ်းစားစရာများ

## သင်ယူရမည့် ရည်မှန်းချက်များ

ဒီသင်ခန်းစာပြီးဆုံးပြီးနောက်မှာ သင်သည် အောက်ပါအကြောင်းအရာများကို နားလည်သွားမည်ဖြစ်သည်-

- AI စနစ်များအပေါ် ရှိနေသော ခြိမ်းခြောက်မှုများနှင့် အန္တရာယ်များ
- AI စနစ်များကို လုံခြုံစေဖို့ အသုံးပြုနိုင်သော နည်းလမ်းများနှင့် လေ့ကျင့်မှုများ
- လုံခြုံရေး စမ်းသပ်မှုများကို အကောင်အထည်ဖော်ခြင်းက မျှော်လင့်မထားသော ရလဒ်များနှင့် အသုံးပြုသူ၏ ယုံကြည်မှု ပျက်စီးမှုများကို ကာကွယ်နိုင်ပုံ

## Generative AI အနက်မှာ လုံခြုံရေးဆိုတာ ဘာကိုဆိုလိုတာလဲ?

Artificial Intelligence (AI) နဲ့ Machine Learning (ML) နည်းပညာတွေဟာ လူနေမှုဘဝကို ပိုမိုသက်ရောက်မှုရှိလာတာကြောင့် ဖောက်သည်ရဲ့ ဒေတာပဲဖြစ်ဖြစ် AI စနစ်တွေကိုယ်တိုင်ကိုပါ ကာကွယ်ဖို့ အရေးကြီးလာပါတယ်။ AI/ML ဟာ စီးပွားရေးလုပ်ငန်းတွေမှာ အရေးကြီးတဲ့ ဆုံးဖြတ်ချက်တွေကို ပံ့ပိုးပေးဖို့ အသုံးပြုလာတာကြောင့် မှားယွင်းတဲ့ ဆုံးဖြတ်ချက်တွေက ပြင်းထန်တဲ့ အကျိုးဆက်တွေ ဖြစ်ပေါ်စေနိုင်ပါတယ်။

အဓိကအချက်တွေကတော့-

- **AI/ML ရဲ့ သက်ရောက်မှု**: AI/ML ဟာ လူနေမှုဘဝမှာ အရေးကြီးတဲ့ သက်ရောက်မှုတွေ ရှိနေပြီး အဲဒီအတွက် ကာကွယ်မှုတွေ လိုအပ်လာပါတယ်။
- **လုံခြုံရေး စိန်ခေါ်မှုများ**: AI/ML ရဲ့ သက်ရောက်မှုကြောင့် AI-based ထုတ်ကုန်တွေကို trolls သို့မဟုတ် အဖွဲ့အစည်းတစ်ခုက တိုက်ခိုက်မှုတွေကနေ ကာကွယ်ဖို့ အရေးကြီးပါတယ်။
- **ရေရှည် မဟာဗျူဟာ ပြဿနာများ**: နည်းပညာလုပ်ငန်းတွေဟာ ဖောက်သည်များရဲ့ လုံခြုံရေးနဲ့ ဒေတာလုံခြုံမှုကို အာမခံဖို့ မဟာဗျူဟာဆိုင်ရာ စိန်ခေါ်မှုတွေကို ကြိုတင်ဖြေရှင်းဖို့ လိုအပ်ပါတယ်။

ထို့အပြင် Machine Learning မော်ဒယ်တွေဟာ မကောင်းတဲ့ input နဲ့ သာမာန် data တွေကို ခွဲခြားနိုင်မှု မရှိပါဘူး။ Training data အများစုဟာ uncurated, unmoderated, public datasets တွေကနေ ရရှိပြီး 3rd-party contribution တွေကို လက်ခံထားပါတယ်။ ဒေတာတွေကို အခမဲ့ ထည့်သွင်းနိုင်တဲ့အခြေအနေမှာ အတိုက်အခံတွေ dataset တွေကို ချိုးဖောက်ဖို့ မလိုအပ်ပါဘူး။ အချိန်ကြာလာတာနဲ့အမျှ low-confidence malicious data တွေဟာ data structure/formatting မှန်ကန်နေတဲ့အခါမှာ high-confidence trusted data ဖြစ်လာနိုင်ပါတယ်။

ဒါကြောင့် သင့်မော်ဒယ်တွေက ဆုံးဖြတ်ချက်ချဖို့ အသုံးပြုတဲ့ ဒေတာတွေ integrity နဲ့ protection ရှိဖို့ အရေးကြီးပါတယ်။

## AI ရဲ့ ခြိမ်းခြောက်မှုများနှင့် အန္တရာယ်များကို နားလည်ခြင်း

AI နဲ့ ဆက်စပ်တဲ့ စနစ်တွေမှာ data poisoning ဟာ ယနေ့ခေတ်မှာ အရေးကြီးဆုံး လုံခြုံရေး ခြိမ်းခြောက်မှုတစ်ခုအဖြစ် ရပ်တည်နေပါတယ်။ Data poisoning ဆိုတာ AI ကို training လုပ်ဖို့ အသုံးပြုတဲ့ အချက်အလက်တွေကို တမင်တကာ ပြောင်းလဲခြင်းဖြစ်ပြီး AI ကို မှားယွင်းမှုတွေ ဖြစ်စေပါတယ်။ ဒါဟာ standardized detection နဲ့ mitigation နည်းလမ်းတွေ မရှိခြင်း၊ untrusted သို့မဟုတ် uncurated public datasets တွေကို training အတွက် အားထားမှုကြောင့် ဖြစ်လာတာပါ။ ဒေတာရဲ့ မူရင်းနဲ့ လမ်းကြောင်းကို အစဉ်မပြတ် စစ်ဆေးထားဖို့ အရေးကြီးပါတယ်။ မဟုတ်ရင် “garbage in, garbage out” ဆိုတဲ့ စကားပုံဟာ အမှန်တကယ် ဖြစ်လာပြီး မော်ဒယ်ရဲ့ စွမ်းဆောင်ရည်ကို ထိခိုက်စေပါတယ်။

Data poisoning က သင့်မော်ဒယ်တွေကို ဘယ်လို သက်ရောက်နိုင်တယ်ဆိုတာကို ဥပမာအချို့ ဖော်ပြပါမည်-

1. **Label Flipping**: Binary classification task မှာ အတိုက်အခံက training data ရဲ့ label အချို့ကို တမင်တကာ ပြောင်းလဲခြင်း။\
   **ဥပမာ**: Spam filter ဟာ label တွေ ပြောင်းလဲမှုကြောင့် သာမာန် email တွေကို spam အဖြစ် မှားယွင်းစွာ သတ်မှတ်ခြင်း။
2. **Feature Poisoning**: Training data ရဲ့ feature တွေကို အတိုက်အခံက bias သို့မဟုတ် မော်ဒယ်ကို လွှဲမှားစေဖို့ ပြောင်းလဲခြင်း။\
   **ဥပမာ**: Product description တွေမှာ မသက်ဆိုင်တဲ့ keyword တွေ ထည့်သွင်းပြီး recommendation system ကို လွှဲမှားစေခြင်း။
3. **Data Injection**: Training set ထဲ malicious data တွေ ထည့်သွင်းပြီး မော်ဒယ်ရဲ့ အပြုအမူကို လွှဲမှားစေခြင်း။\
   **ဥပမာ**: Fake user reviews တွေ ထည့်သွင်းပြီး sentiment analysis ရလဒ်တွေကို လွှဲမှားစေခြင်း။
4. **Backdoor Attacks**: Training data ထဲ hidden pattern (backdoor) တစ်ခုကို အတိုက်အခံက ထည့်သွင်းခြင်း။ မော်ဒယ်ဟာ pattern ကို သင်ယူပြီး trigger ဖြစ်တဲ့အခါ malicious behavior ပြုလုပ်ခြင်း။\
   **ဥပမာ**: Face recognition system ဟာ backdoored images တွေကြောင့် လူတစ်ဦးကို မှားယွင်းစွာ မှတ်မိခြင်း။

MITRE Corporation က [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) ဆိုတဲ့ AI စနစ်တွေကို တိုက်ခိုက်မှုအတွက် adversaries အသုံးပြုတဲ့ နည်းလမ်းနဲ့ နည်းဗျူဟာတွေကို စုစည်းထားတဲ့ knowledgebase တစ်ခု ဖန်တီးထားပါတယ်။

> AI-enabled စနစ်တွေမှာ vulnerabilities တွေ များလာတဲ့အတွက် AI ကို ထည့်သွင်းအသုံးပြုမှုက traditional cyber-attacks တွေထက် attack surface ကို ပိုမိုချဲ့ထွင်စေပါတယ်။ ATLAS ကို ဖန်တီးတာက AI ကို စနစ်အမျိုးမျိုးမှာ ထည့်သွင်းအသုံးပြုလာတဲ့ ကမ္ဘာလုံးဆိုင်ရာ အသိုင်းအဝိုင်းအတွက် unique နဲ့ အဆင့်မြှင့်ထားတဲ့ vulnerabilities တွေကို သိရှိစေဖို့ ရည်ရွယ်ပါတယ်။ ATLAS ဟာ MITRE ATT&CK® framework ကို အခြေခံပြီး TTPs တွေဟာ ATT&CK ထဲမှာပါဝင်တဲ့ TTPs တွေနဲ့ အတူတူဖြစ်ပါတယ်။

MITRE ATT&CK® framework က traditional cybersecurity မှာ advanced threat emulation scenarios တွေကို စီမံခန့်ခွဲဖို့ အများဆုံး အသုံးပြုတဲ့အတိုင်း ATLAS ဟာ emerging attacks တွေကို ကာကွယ်ဖို့ နားလည်မှုရရှိစေဖို့ TTPs တွေကို ရှာဖွေဖို့ လွယ်ကူစေပါတယ်။

ထို့အပြင် Open Web Application Security Project (OWASP) က "[Top 10 list](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" ဆိုတဲ့ LLMs အသုံးပြုတဲ့ အက်ပလီကေးရှင်းတွေမှာ တွေ့ရတဲ့ အရေးကြီးတဲ့ vulnerabilities ၁၀ ခုကို ဖော်ပြထားပါတယ်။ ဒီ list မှာ data poisoning အပါအဝင် အခြားသော ခြိမ်းခြောက်မှုများကိုလည်း ဖော်ပြထားပြီး-

- **Prompt Injection**: Large Language Model (LLM) ကို input တွေကို တမင်တကာ ပြောင်းလဲခြင်းဖြင့် မော်ဒယ်ရဲ့ intended behavior အပြင်မှာ အပြုအမူပြုစေခြင်း။
- **Supply Chain Vulnerabilities**: LLM အသုံးပြုတဲ့ အက်ပလီကေးရှင်းတွေကို ဖွဲ့စည်းထားတဲ့ components နဲ့ software တွေ (ဥပမာ- Python modules သို့မဟုတ် external datasets) ကို ချိုးဖောက်မှုတွေကြောင့် မမျှော်လင့်ထားတဲ့ ရလဒ်တွေ၊ bias တွေ ထည့်သွင်းမှုတွေ၊ underlying infrastructure ရဲ့ vulnerabilities တွေ ဖြစ်ပေါ်စေနိုင်ခြင်း။
- **Overreliance**: LLMs ဟာ အမှားတွေ ဖြစ်နိုင်ပြီး အချို့သော documented အခြေအနေတွေမှာ လူတွေဟာ result တွေကို အမှန်တကယ်လိုက်နာပြီး unintended real-world negative consequences တွေ ဖြစ်ပေါ်စေခြင်း။

Microsoft Cloud Advocate Rod Trent က [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) ဆိုတဲ့ အခမဲ့ ebook တစ်ခု ရေးသားထားပြီး ဒီ AI ခြိမ်းခြောက်မှုတွေကို အနက်နက်လေ့လာပြီး ဒီအခြေအနေတွေကို ဖြေရှင်းဖို့ လမ်းညွှန်ချက်များကို ပေးထားပါတယ်။

## AI စနစ်များနှင့် LLMs အတွက် လုံခြုံရေး စမ်းသပ်မှု

Artificial intelligence (AI) ဟာ လူမှုအသိုင်းအဝိုင်းအတွက် အကျိုးကျေးဇူးများစွာပေးနိုင်တဲ့ နည်းပညာတစ်ခုဖြစ်ပြီး အမျိုးမျိုးသော နယ်ပယ်များနှင့် စက်မှုလုပ်ငန်းများကို ပြောင်းလဲစေနိုင်ပါတယ်။ သို့သော် AI ဟာ ဒေတာကိုယ်ရေးအချက်အလက်၊ bias, explainability မရှိခြင်း၊ အလွဲသုံးစားမှု စတဲ့ အန္တရာယ်များကိုလည်း ဖြစ်ပေါ်စေနိုင်ပါတယ်။ ဒါကြောင့် AI စနစ်တွေဟာ ethical နဲ့ legal standards တွေကို လိုက်နာပြီး အသုံးပြုသူတွေနဲ့ stakeholder တွေက ယုံကြည်မှုရှိစေဖို့ လုံခြုံပြီး တာဝန်ယူမှုရှိဖို့ အရေးကြီးပါတယ်။

AI စနစ် သို့မဟုတ် LLM တစ်ခုရဲ့ လုံခြုံရေးကို အကဲဖြတ်ဖို့ security testing လုပ်ငန်းစဉ်ဟာ အရေးကြီးပါတယ်။ ဒီလုပ်ငန်းစဉ်ကို developer, user, သို့မဟုတ် third-party auditor တွေက လုပ်ဆောင်နိုင်ပြီး စမ်းသပ်မှုရဲ့ ရည်ရွယ်ချက်နဲ့ အကျယ်အဝန်းပေါ်မူတည်ပါတယ်။ AI စနစ်တွေနဲ့ LLMs အတွက် အများဆုံး အသုံးပြုတဲ့ security testing နည်းလမ်းတွေကတော့-

- **Data sanitization**: Training data သို့မဟုတ် AI စနစ် သို့မဟုတ် LLM ရဲ့ input မှာ sensitive သို့မဟုတ် private အချက်အလက်တွေကို ဖယ်ရှားခြင်း သို့မဟုတ် အမည်မဖော်ထားခြင်း။ Data sanitization ဟာ confidential သို့မဟုတ် personal data တွေကို လျှို့ဝှက်ထားပြီး malicious manipulation ကို ကာကွယ်နိုင်စေပါတယ်။
- **Adversarial testing**: Adversarial examples တွေကို AI စနစ် သို့မဟုတ် LLM ရဲ့ input သို့မဟုတ် output မှာ ထည့်သွင်းပြီး adversarial attacks တွေကို ခံနိုင်ရည်ရှိမှုကို စမ်းသပ်ခြင်း။ Adversarial testing ဟာ AI စနစ် သို့မဟုတ် LLM ရဲ့ vulnerabilities တွေကို ရှာဖွေပြီး ကာကွယ်နိုင်စေပါတယ်။
- **Model verification**: AI စနစ် သို့မဟုတ် LLM ရဲ့ model parameters သို့မဟုတ် architecture ရဲ့ မှန်ကန်မှုနဲ့ ပြည့်စုံမှုကို စစ်ဆေးခြင်း။ Model verification ဟာ model stealing ကို ကာကွယ်နိုင်စေပြီး မော်ဒယ်ကို အတည်ပြုနိုင်စေပါတယ်။
- **Output validation**: AI စနစ် သို့မဟုတ် LLM ရဲ့ output ရဲ့ အရည်အသွေးနဲ့ ယုံကြည်ရမှုကို စစ်ဆေးခြင်း။ Output validation ဟာ malicious manipulation တွေကို ရှာဖွေပြီး output ဟာ မှန်ကန်မှုရှိစေပါတယ်။

OpenAI ဟာ AI စနစ်တွေကို red teaming network initiative အနေနဲ့ _safety evaluations_ တွေကို စီစဉ်ထားပြီး AI စနစ်တွေကို စမ်းသပ်ဖို့ ရည်ရွယ်ထားပါတယ်။

> စမ်းသပ်မှုတွေဟာ ရိုးရှင်းတဲ့ Q&A tests တွေကနေ စတင်ပြီး ပိုမိုရှုပ်ထွေးတဲ့ simulations တွေကိုပါ လုပ်ဆောင်နိုင်ပါတယ်။ OpenAI က AI behaviors တွေကို အမျိုးမျိုးသော angle တွေကနေ အကဲဖြတ်ဖို့ ဖွံ့ဖြိုးထားတဲ့ စမ်းသပ်မှု ဥပမာတွေကတော့-

#### Persuasion

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက အခြား AI စနစ်တစ်ခုကို လျှို့ဝှက်စကားတစ်ခု ပြောစေဖို့ ဘယ်လောက်အောင်မြင်နိုင်မလဲ?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက အခြား AI စနစ်တစ်ခုကို ငွေလှူဖို့ ဘယ်လောက်အောင်မြင်စွာ ဆွဲဆောင်နိုင်မလဲ?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက အခြား AI စနစ်တစ်ခုရဲ့ နိုင်ငံရေးအဆိုပြုချက်ကို ထောက်ခံမှုရဖို့ ဘယ်လောက်အောင်မြင်စွာ လှုံ့ဆော်နိုင်မလဲ?

#### Steganography (hidden messaging)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက အခြား AI စနစ်တစ်ခုကို လျှို့ဝှက်စာတွေကို မဖမ်းမိဘဲ ပို့နိုင်မှု ဘယ်လောက်ရှိမလဲ?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက လျှို့ဝှက်စာတွေကို ပိုမိုသိပ်သည်းစေပြီး ပြန်လည်ဖွင့်နိုင်မှု ဘယ်လောက်ရှိမလဲ?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI စနစ်တစ်ခုက အခြား AI စနစ်တစ်ခုနဲ့ တိုက်ရိုက်ဆက်သွယ်မှုမရှိဘဲ ပေါင်းစည်းနိုင်မှု ဘယ်လောက်ရှိမလဲ?

### AI လုံခြုံရေး

AI စနစ်တွေကို အန္တရာယ်ရှိတဲ့ တိုက်ခိုက်မှုတွေ၊ အလွဲသုံးစားမှုတွေ သို့မဟုတ် မမျှော်လင့်ထားတဲ့ အကျိုးဆက်တွေကနေ ကာကွယ်ဖို့ အရေးကြီးပါတယ်။ AI စနစ်တွေကို လုံခြုံမှု၊ ယုံကြည်မှုရှိစေဖို့ အဆင့်ဆင့် လုပ်ဆောင်ရမည်မှာ-

- AI မော်ဒယ်တွေကို training လ
အမှန်တကယ်ဖြစ်ပျက်နေသော အန္တရာယ်များကို အတုယူခြင်းသည် AI စနစ်များကို ခိုင်မာစေရန် တည်ဆောက်ရာတွင် စံနည်းလမ်းဖြစ်လာပြီး စနစ်များ၏ အန္တရာယ်များကို ရှာဖွေခြင်းနှင့် ကာကွယ်ရေးအဖွဲ့များ၏ တုံ့ပြန်မှုကို စမ်းသပ်ရန် တူညီသော ကိရိယာများ၊ နည်းလမ်းများ၊ လုပ်ထုံးလုပ်နည်းများကို အသုံးပြုသည်။

> AI red teaming လုပ်ငန်းစဉ်သည် ပိုမိုကျယ်ပြန့်သော အဓိပ္ပါယ်ကို ယခုအခါ ရရှိလာပြီး လုံခြုံရေး အခွင့်အလမ်းများကို စမ်းသပ်ခြင်းသာမက အခြားသော စနစ်ချို့ယွင်းမှုများကိုလည်း စမ်းသပ်ခြင်း (ဥပမာ- အန္တရာယ်ရှိနိုင်သော အကြောင်းအရာများ ဖန်တီးခြင်း) ပါဝင်သည်။ AI စနစ်များတွင် အသစ်သော အန္တရာယ်များရှိပြီး၊ prompt injection နှင့် အခြေခံမရှိသော အကြောင်းအရာများ ဖန်တီးခြင်းကဲ့သို့သော အန္တရာယ်များကို နားလည်ရန် red teaming သည် အရေးပါသည်။ - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.my.png)]()

Microsoft ၏ AI Red Team အစီအစဉ်ကို ပုံဖော်ခဲ့သော အဓိက အမြင်များကို အောက်တွင် ဖော်ပြထားသည်။

1. **AI Red Teaming ၏ ကျယ်ပြန့်သော လုပ်ငန်းနယ်ပယ်:**
   AI red teaming သည် လုံခြုံရေးနှင့် တာဝန်ယူမှုရှိသော AI (RAI) ရလဒ်များကို အခုအခါ ပါဝင်သည်။ ယခင်က red teaming သည် လုံခြုံရေးအပိုင်းများကို အဓိကထားပြီး မော်ဒယ်ကို vector အဖြစ် (ဥပမာ- မော်ဒယ်ကို ခိုးယူခြင်း) သတ်မှတ်ခဲ့သည်။ သို့သော် AI စနစ်များတွင် prompt injection၊ poisoning ကဲ့သို့သော အသစ်သော လုံခြုံရေး ချို့ယွင်းမှုများရှိပြီး အထူးဂရုစိုက်ရန် လိုအပ်သည်။ လုံခြုံရေးအပြင် AI red teaming သည် တရားမျှတမှုဆိုင်ရာ ပြဿနာများ (ဥပမာ- stereotyping) နှင့် အန္တရာယ်ရှိသော အကြောင်းအရာများ (ဥပမာ- အကြမ်းဖက်မှုကို ချီးမွမ်းခြင်း) ကိုလည်း စမ်းသပ်သည်။ ဤပြဿနာများကို စောစောမှ ရှာဖွေခြင်းသည် ကာကွယ်ရေး ရင်းနှီးမြှုပ်နှံမှုများကို ဦးစားပေးရန် အခွင့်အလမ်းပေးသည်။
2. **မကောင်းသောနှင့် ကောင်းသော ချို့ယွင်းမှုများ:**
   AI red teaming သည် မကောင်းသောနှင့် ကောင်းသော အမြင်များနှင့် ချို့ယွင်းမှုများကို စဉ်းစားသည်။ ဥပမာ- Bing အသစ်ကို red teaming လုပ်သောအခါ၊ စနစ်ကို မကောင်းသော ရန်သူများက ဘယ်လို ဖျက်ဆီးနိုင်မလဲသာမက ပုံမှန်အသုံးပြုသူများက ပြဿနာရှိသော သို့မဟုတ် အန္တရာယ်ရှိသော အကြောင်းအရာများကို ဘယ်လို ကြုံတွေ့နိုင်မလဲကိုလည်း စဉ်းစားသည်။ ရိုးရာ လုံခြုံရေး red teaming သည် မကောင်းသော လုပ်ရပ်များကို အဓိကထားသော်လည်း AI red teaming သည် ပိုမိုကျယ်ပြန့်သော လူပုံစံများနှင့် ချို့ယွင်းမှုများကို စဉ်းစားသည်။
3. **AI စနစ်များ၏ အပြောင်းအလဲရှိသော သဘာဝ:**
   AI အက်ပလီကေးရှင်းများသည် အမြဲတမ်း အပြောင်းအလဲရှိသည်။ အကြီးမားသော ဘာသာစကားမော်ဒယ် အက်ပလီကေးရှင်းများတွင် ဖွံ့ဖြိုးသူများသည် လိုအပ်ချက်များ ပြောင်းလဲမှုအရ အဆက်မပြတ် အလျှောက်လွှာများကို ပြင်ဆင်သည်။ အဆက်မပြတ် red teaming သည် အန္တရာယ်များ ပြောင်းလဲမှုအပေါ် အမြဲတမ်း အာရုံစိုက်မှုနှင့် လိုက်လျောမှုကို သေချာစေသည်။

AI red teaming သည် အားလုံးကို အုပ်စိုးနိုင်သော လုပ်ငန်းစဉ်မဟုတ်ဘဲ [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) နှင့် အကျုံးဝင်သော ဒေတာစီမံခန့်ခွဲမှု ဖြေရှင်းချက်များကဲ့သို့သော ထိန်းချုပ်မှုများကို ဖြည့်စွက်ရန် ရည်ရွယ်သည်။ ၎င်းသည် သုံးစွဲသူများ၏ ယုံကြည်မှုကို ထိခိုက်စေနိုင်သော အလွဲမှားမှုများ၊ အန္တရာယ်ရှိသော အကြောင်းအရာများနှင့် အချက်အလက်မှားများကို လျှော့ချရန် ရည်ရွယ်သည့် လုံခြုံရေးနှင့် တာဝန်ယူမှုရှိသော AI ဖြေရှင်းချက်များကို အသုံးပြုရန် အာရုံစိုက်ထားသော လုံခြုံရေး မဟာဗျူဟာကို ဖြည့်စွက်ရန် ရည်ရွယ်သည်။

AI စနစ်များတွင် အန္တရာယ်များကို ရှာဖွေခြင်းနှင့် လျှော့ချခြင်းတွင် red teaming က ဘယ်လို အထောက်အကူဖြစ်နိုင်သည်ကို ပိုမိုနားလည်ရန် အောက်ပါ ဖတ်ရှုရန် စာရင်းကို ဖော်ပြထားသည်-

- [Planning red teaming for large language models (LLMs) and their applications](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [What is the OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - A Key Practice for Building Safer and More Responsible AI Solutions](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), AI စနစ်များကို တိုက်ခိုက်မှုများတွင် ရန်သူများ အသုံးပြုသော နည်းလမ်းများနှင့် နည်းဗျူဟာများ၏ အချက်အလက်အခြေခံ knowledgebase ဖြစ်သည်။

## အသိပညာ စစ်ဆေးခြင်း

ဒေတာ၏ တိကျမှုကို ထိန်းသိမ်းခြင်းနှင့် အလွဲသုံးစားမှုကို ကာကွယ်ရန် ကောင်းသော နည်းလမ်းက ဘာဖြစ်နိုင်မလဲ?

1. ဒေတာဝင်ရောက်ခွင့်နှင့် ဒေတာစီမံခန့်ခွဲမှုအတွက် အခန်းကဏ္ဍအခြေခံ ထိန်းချုပ်မှုများ ရှိရန်
1. ဒေတာကို အလွဲသုံးစားမှု သို့မဟုတ် အချက်အလက်မှားမှုကို ကာကွယ်ရန် ဒေတာတံဆိပ်ကပ်ခြင်းကို အကဲဖြတ်ရန်နှင့် အကဲဖြတ်ခြင်း
1. သင့် AI အခြေခံအဆောက်အအုံသည် အကြောင်းအရာ စိစစ်မှုကို ပံ့ပိုးပေးရန် သေချာစေရန်

A:1, အထူးသဖြင့် အားလုံးကောင်းမွန်သော အကြံပြုချက်များဖြစ်သော်လည်း သင့် LLMs အသုံးပြုသော ဒေတာကို ပြောင်းလဲခြင်းနှင့် အချက်အလက်မှားမှုကို ကာကွယ်ရန် သင့်အား သုံးစွဲသူများကို သင့်တော်သော ဒေတာဝင်ရောက်ခွင့်ပေးခြင်းသည် အလွန်အရေးပါသည်။

## 🚀 စိန်ခေါ်မှု

AI ရာသီတစ်ခုတွင် [အထူးအချက်အလက်များကို ထိန်းချုပ်ခြင်းနှင့် ကာကွယ်ခြင်း](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) ဘယ်လိုလုပ်နိုင်မလဲဆိုတာ ပိုမိုလေ့လာပါ။

## အလုပ်ကောင်းပြီ၊ သင်ယူမှုကို ဆက်လက်လုပ်ဆောင်ပါ

ဤသင်ခန်းစာကို ပြီးမြောက်ပြီးနောက်၊ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ကို ကြည့်ရှုပြီး Generative AI အသိပညာကို ဆက်လက်မြှင့်တင်ပါ။

Lesson 14 သို့ သွားပြီး [Generative AI Application Lifecycle](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) ကို လေ့လာကြည့်ပါ!

---

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတရ အရင်းအမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များကို အသုံးပြုရန် အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအမှားများ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။