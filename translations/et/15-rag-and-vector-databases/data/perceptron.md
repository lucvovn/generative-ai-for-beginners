<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-10-11T11:16:45+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "et"
}
-->
# Sissejuhatus n√§rviv√µrkudesse: Perceptron

√úks esimesi katseid luua midagi t√§nap√§evase n√§rviv√µrgu sarnast tehti 1957. aastal Frank Rosenblatti poolt Cornell Aeronautical Laboratory's. See oli riistvaraline lahendus nimega "Mark-1", mis oli loodud primitiivsete geomeetriliste kujundite, nagu kolmnurgad, ruudud ja ringid, √§ratundmiseks.

|      |      |
|--------------|-----------|
|<img src='../../../../translated_images/Rosenblatt-wikipedia.1d205667acda28c0f97ad594eb6dadfa0485605f3fb2155eca46a0255e98efac.et.jpg' alt='Frank Rosenblatt'/> | <img src='../../../../translated_images/Mark_I_perceptron_wikipedia.434e46ca39e2be801976110f8b1b75b13d1197f69e3a5f8b7537b43d35413e6f.et.jpg' alt='Mark 1 Perceptron' />|

> Pildid Wikipediast

Sisendpilt esitati 20x20 fototundliku elemendi maatriksina, seega oli n√§rviv√µrgul 400 sisendit ja √ºks binaarne v√§ljund. Lihtne v√µrk koosnes √ºhest neuronist, mida nimetatakse ka **l√§ve loogika √ºksuseks**. N√§rviv√µrgu kaalu v√§√§rtused toimisid nagu potentsiomeetrid, mida tuli treeningfaasis k√§sitsi reguleerida.

> ‚úÖ Potentsiomeeter on seade, mis v√µimaldab kasutajal reguleerida vooluringi takistust.

> The New York Times kirjutas tol ajal perceptroni kohta: *elektroonilise arvuti embr√ºo, millest [merev√§gi] loodab, et see suudab k√µndida, r√§√§kida, n√§ha, kirjutada, end paljundada ja olla teadlik oma olemasolust.*

## Perceptroni mudel

Oletame, et meie mudelis on N tunnust, sel juhul oleks sisendvektor suurusega N. Perceptron on **binaarne klassifitseerimismudel**, st see suudab eristada kahte sisendandmete klassi. Eeldame, et iga sisendvektori x korral on meie perceptroni v√§ljund kas +1 v√µi -1, s√µltuvalt klassist. V√§ljund arvutatakse j√§rgmise valemi abil:

y(x) = f(w<sup>T</sup>x)

kus f on astmelise aktiveerimise funktsioon.

## Perceptroni treenimine

Perceptroni treenimiseks peame leidma kaaluvektori w, mis klassifitseerib enamik v√§√§rtusi √µigesti, st mille tulemuseks on k√µige v√§iksem **viga**. See viga m√§√§ratletakse **perceptroni kriteeriumi** j√§rgi j√§rgmiselt:

E(w) = -&sum;w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

kus:

* summa v√µetakse nende treeningandmepunktide i kohta, mis annavad vale klassifikatsiooni
* x<sub>i</sub> on sisendandmed ja t<sub>i</sub> on vastavalt -1 v√µi +1 negatiivsete ja positiivsete n√§idete jaoks.

Seda kriteeriumi k√§sitletakse kaalu w funktsioonina ja me peame selle minimeerima. Sageli kasutatakse meetodit nimega **gradientlangus**, mille puhul alustame m√µnest esialgsest kaalust w<sup>(0)</sup> ja seej√§rel igal sammul uuendame kaale j√§rgmise valemi j√§rgi:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - &eta;&nabla;E(w)

Siin on &eta; nn **√µppem√§√§r** ja &nabla;E(w) t√§histab E **gradienti**. P√§rast gradiendi arvutamist j√µuame valemini:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + &sum;&eta;x<sub>i</sub>t<sub>i</sub>

Algoritm Pythonis n√§eb v√§lja selline:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Kokkuv√µte

Selles √µppet√ºkis √µppisite perceptroni kohta, mis on binaarne klassifitseerimismudel, ja kuidas seda treenida kaaluvektori abil.

## üöÄ V√§ljakutse

Kui soovite proovida ise perceptroni luua, proovige seda laborit Microsoft Learnis, mis kasutab Azure ML disainerit.

## √úlevaade ja iseseisev √µppimine

Et n√§ha, kuidas perceptronit saab kasutada nii m√§nguliste kui ka p√§riselu probleemide lahendamiseks, ja √µppimist j√§tkata, minge Perceptroni m√§rkmikusse.

Siin on ka huvitav artikkel perceptronite kohta.

## √úlesanne

Selles √µppet√ºkis rakendasime perceptroni binaarse klassifitseerimise √ºlesande jaoks ja kasutasime seda kahe k√§sitsi kirjutatud numbri eristamiseks. Selles laboris palutakse teil lahendada numbrite klassifitseerimise probleem t√§ielikult, st m√§√§rata, milline number k√µige t√µen√§olisemalt vastab antud pildile.

* Juhised
* M√§rkmik

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud, kasutades AI t√µlketeenust [Co-op Translator](https://github.com/Azure/co-op-translator). Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algset dokumenti selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valede t√µlgenduste eest.