<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-10-11T11:16:07+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "et"
}
-->
# Neuraalv√µrkude raamistikud

Nagu me juba √µppisime, on neuraalv√µrkude t√µhusaks treenimiseks vaja teha kahte asja:

* T√∂√∂tada tensoritega, n√§iteks korrutada, liita ja arvutada m√µningaid funktsioone nagu sigmoid v√µi softmax
* Arvutada k√µigi avaldiste tuletised, et teostada gradientide languse optimeerimist

Kuigi `numpy` teek suudab t√§ita esimest osa, vajame mehhanismi gradientide arvutamiseks. Eelmises osas v√§lja t√∂√∂tatud raamistikus pidime k√µik tuletisfunktsioonid k√§sitsi programmeerima `backward` meetodis, mis teostab tagasipropageerimist. Ideaalis peaks raamistik v√µimaldama arvutada gradienti *mis tahes avaldise* jaoks, mida suudame defineerida.

Teine oluline aspekt on arvutuste tegemine GPU-l v√µi m√µnel muul spetsialiseeritud arvutus√ºksusel, n√§iteks TPU-l. S√ºgavate neuraalv√µrkude treenimine n√µuab *v√§ga palju* arvutusi, ja nende arvutuste paralleelne teostamine GPU-l on v√§ga oluline.

> ‚úÖ Termin 'paralleelne teostamine' t√§hendab arvutuste jaotamist mitme seadme vahel.

Praegu on kaks k√µige populaarsemat neuraalv√µrkude raamistikku: TensorFlow ja PyTorch. M√µlemad pakuvad madala taseme API-d tensoritega t√∂√∂tamiseks nii CPU-l kui GPU-l. Madala taseme API peale on olemas ka k√µrgema taseme API, vastavalt Keras ja PyTorch Lightning.

Madala taseme API | TensorFlow | PyTorch
------------------|-----------------------|-----------------------
K√µrgema taseme API | Keras | PyTorch Lightning

**Madala taseme API-d** m√µlemas raamistikus v√µimaldavad luua nn **arvutusgraafikuid**. See graafik m√§√§ratleb, kuidas arvutada v√§ljundit (tavaliselt kaotuse funktsiooni) antud sisendparameetritega ja seda saab GPU-le arvutamiseks saata, kui see on saadaval. Graafiku diferentseerimiseks ja gradientide arvutamiseks on olemas funktsioonid, mida saab seej√§rel kasutada mudeli parameetrite optimeerimiseks.

**K√µrgema taseme API-d** k√§sitlevad neuraalv√µrke peamiselt kui **kihtide j√§rjestust** ja muudavad enamiku neuraalv√µrkude konstrueerimise palju lihtsamaks. Mudeli treenimine n√µuab tavaliselt andmete ettevalmistamist ja seej√§rel `fit` funktsiooni kutsumist, et t√∂√∂ √§ra teha.

K√µrgema taseme API v√µimaldab t√º√ºpilisi neuraalv√µrke v√§ga kiiresti konstrueerida, muretsemata paljude detailide p√§rast. Samal ajal pakub madala taseme API palju rohkem kontrolli treenimisprotsessi √ºle ja seet√µttu kasutatakse seda palju teadust√∂√∂s, kui tegeletakse uute neuraalv√µrkude arhitektuuridega.

Samuti on oluline m√µista, et m√µlemat API-d saab koos kasutada, n√§iteks saate madala taseme API abil v√§lja t√∂√∂tada oma v√µrgu kihi arhitektuuri ja seej√§rel kasutada seda suuremas v√µrgus, mis on konstrueeritud ja treenitud k√µrgema taseme API abil. V√µi saate defineerida v√µrgu k√µrgema taseme API abil kihtide j√§rjestusena ja seej√§rel kasutada oma madala taseme treenimists√ºklit optimeerimise teostamiseks. M√µlemad API-d kasutavad samu p√µhilisi aluskontseptsioone ja on loodud h√§sti koos t√∂√∂tama.

## √ïppimine

Selles kursuses pakume enamikku sisust nii PyTorchile kui TensorFlowle. Saate valida oma eelistatud raamistikku ja l√§bida ainult vastavad m√§rkmikud. Kui te pole kindel, millist raamistikku valida, lugege internetis arutelusid teemal **PyTorch vs. TensorFlow**. Samuti v√µite m√µlemat raamistikku uurida, et paremini aru saada.

V√µimaluse korral kasutame lihtsuse huvides k√µrgema taseme API-sid. Kuid usume, et on oluline m√µista, kuidas neuraalv√µrgud t√∂√∂tavad algtasemel, seega alustame madala taseme API ja tensoritega t√∂√∂tamisest. Kui aga soovite kiiresti alustada ja ei taha nende detailide √µppimisele palju aega kulutada, v√µite need vahele j√§tta ja minna otse k√µrgema taseme API m√§rkmike juurde.

## ‚úçÔ∏è Harjutused: Raamistikud

J√§tkake √µppimist j√§rgmistes m√§rkmikes:

Madala taseme API | TensorFlow+Keras m√§rkmik | PyTorch
------------------|--------------------------|-----------------------
K√µrgema taseme API | Keras | *PyTorch Lightning*

P√§rast raamistikuga tutvumist vaatame √ºle √ºleliigse sobitamise (overfitting) m√µiste.

# √úleliigne sobitamine

√úleliigne sobitamine on masin√µppes √§√§rmiselt oluline m√µiste ja on v√§ga t√§htis sellest √µigesti aru saada!

Vaatleme j√§rgmist probleemi, kus tuleb ligikaudselt m√§√§rata 5 punkti (graafikutel t√§histatud `x`-ga):

!lineaarne | √ºleliigne sobitamine
-------------------------|--------------------------
**Lineaarne mudel, 2 parameetrit** | **Mitte-lineaarne mudel, 7 parameetrit**
Treeningu viga = 5.3 | Treeningu viga = 0
Valideerimise viga = 5.1 | Valideerimise viga = 20

* Vasakul n√§eme head sirgjoonelist ligikaudset m√§√§ratlust. Kuna parameetrite arv on piisav, saab mudel punktide jaotuse olemusest √µigesti aru.
* Paremal on mudel liiga v√µimas. Kuna meil on ainult 5 punkti ja mudelil on 7 parameetrit, saab see kohanduda nii, et l√§bib k√µik punktid, muutes treeningu vea nulliks. Kuid see takistab mudelil andmete √µiget mustrit m√µista, mist√µttu valideerimise viga on v√§ga suur.

On v√§ga oluline leida √µige tasakaal mudeli rikkuse (parameetrite arv) ja treeningn√§idiste arvu vahel.

## Miks √ºleliigne sobitamine tekib

  * Ebapiisav treeningandmete hulk
  * Liiga v√µimas mudel
  * Liiga palju m√ºra sisendandmetes

## Kuidas √ºleliigset sobitamist tuvastada

Nagu √ºlaltoodud graafikult n√§ha, saab √ºleliigset sobitamist tuvastada v√§ga madala treeningu vea ja k√µrge valideerimise vea j√§rgi. Tavaliselt n√§eme treenimise ajal, et nii treeningu kui valideerimise vead hakkavad v√§henema, kuid mingil hetkel valideerimise viga v√µib l√µpetada v√§henemise ja hakata suurenema. See on m√§rk √ºleliigsest sobitamisest ja indikaator, et treenimine tuleks t√µen√§oliselt l√µpetada (v√µi v√§hemalt mudelist hetkeseis salvestada).

## Kuidas √ºleliigset sobitamist v√§ltida

Kui n√§ete, et √ºleliigne sobitamine toimub, saate teha j√§rgmist:

 * Suurendage treeningandmete hulka
 * V√§hendage mudeli keerukust
 * Kasutage m√µnda regulatsioonitehnikat, n√§iteks Dropout, mida k√§sitleme hiljem.

## √úleliigne sobitamine ja Bias-Variance Tradeoff

√úleliigne sobitamine on tegelikult statistikas tuntud √ºldisema probleemi, Bias-Variance Tradeoff, juhtum. Kui kaalume oma mudeli v√µimalikke veallikaid, n√§eme kahte t√º√ºpi vigu:

* **Bias-vead** tekivad siis, kui meie algoritm ei suuda treeningandmete seost √µigesti tabada. See v√µib tuleneda sellest, et meie mudel pole piisavalt v√µimas (**alaliigne sobitamine**).
* **Variance-vead**, mis tekivad mudeli poolt sisendandmete m√ºra ligikaudse m√§√§ratlemise asemel t√§hendusliku seose (**√ºleliigne sobitamine**) t√µttu.

Treeningu ajal bias-vead v√§henevad (kuna meie mudel √µpib andmeid ligikaudselt m√§√§ratlema) ja variance-vead suurenevad. On oluline treenimine l√µpetada - kas k√§sitsi (kui tuvastame √ºleliigse sobitamise) v√µi automaatselt (regulatsiooni kasutuselev√µtuga) - et v√§ltida √ºleliigset sobitamist.

## Kokkuv√µte

Selles √µppetunnis √µppisite erinevusi kahe k√µige populaarsema AI-raamistiku, TensorFlow ja PyTorch, erinevate API-de vahel. Lisaks √µppisite v√§ga olulist teemat, √ºleliigset sobitamist.

## üöÄ V√§ljakutse

Kaasaolevates m√§rkmikes leiate '√ºlesanded' allosas; t√∂√∂tage m√§rkmikud l√§bi ja t√§itke √ºlesanded.

## √úlevaade ja iseseisev √µppimine

Tehke uurimist√∂√∂d j√§rgmiste teemade kohta:

- TensorFlow
- PyTorch
- √úleliigne sobitamine

Esitage endale j√§rgmised k√ºsimused:

- Mis vahe on TensorFlow ja PyTorch vahel?
- Mis vahe on √ºleliigsel sobitamisel ja alaliigsel sobitamisel?

## √úlesanne

Selles laboris palutakse teil lahendada kaks klassifitseerimisprobleemi, kasutades √ºhe- ja mitmekihilisi t√§ielikult √ºhendatud v√µrke PyTorch v√µi TensorFlow abil.

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.