{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta perekonna mudelitega töötamine\n",
    "\n",
    "## Sissejuhatus\n",
    "\n",
    "Selles õppetükis käsitletakse:\n",
    "\n",
    "- Meta perekonna kahe peamise mudeli - Llama 3.1 ja Llama 3.2 - uurimist\n",
    "- Iga mudeli kasutusjuhtude ja stsenaariumide mõistmist\n",
    "- Koodinäidet, mis näitab iga mudeli unikaalseid omadusi\n",
    "\n",
    "## Meta perekonna mudelid\n",
    "\n",
    "Selles õppetükis uurime Meta perekonna või \"Llama karja\" kahte mudelit - Llama 3.1 ja Llama 3.2.\n",
    "\n",
    "Need mudelid on saadaval erinevates variantides ja neid saab leida Githubi mudelite turuplatsilt. Siin on rohkem teavet Githubi mudelite kasutamise kohta [AI mudelite prototüüpimiseks](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Mudelivariandid:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*NB! Llama 3 on samuti saadaval Githubi mudelite seas, kuid seda õppetükis ei käsitleta.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "405 miljardi parameetriga kuulub Llama 3.1 avatud lähtekoodiga LLM-i kategooriasse.\n",
    "\n",
    "See versioon on Llama 3 varasemast väljaandest edasiarendus, pakkudes:\n",
    "\n",
    "- Suuremat kontekstiakent - 128k tokenit vs 8k tokenit\n",
    "- Suuremat maksimaalset väljundtokenite arvu - 4096 vs 2048\n",
    "- Paremat mitmekeelset tuge - tänu treeningtokenite arvu suurenemisele\n",
    "\n",
    "Need omadused võimaldavad Llama 3.1-l käsitleda keerukamaid kasutusjuhtumeid GenAI rakenduste loomisel, sealhulgas:\n",
    "- Natiivne funktsioonide kutsumine - võime kutsuda väliseid tööriistu ja funktsioone väljaspool LLM-i töövoogu\n",
    "- Parem RAG jõudlus - tänu suuremale kontekstiaknale\n",
    "- Sünteetiliste andmete genereerimine - võime luua tõhusaid andmeid ülesannete jaoks, nagu peenhäälestamine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktsioonide kutsumine\n",
    "\n",
    "Llama 3.1 on peenhäälestatud, et olla tõhusam funktsioonide või tööriistade kutsumisel. Sellel on ka kaks sisseehitatud tööriista, mida mudel suudab kasutaja sisendi põhjal vajadusel ära tunda ja kasutada. Need tööriistad on:\n",
    "\n",
    "- **Brave Search** - Saab kasutada ajakohase teabe, näiteks ilma, hankimiseks veebipäringu abil.\n",
    "- **Wolfram Alpha** - Saab kasutada keerukamate matemaatiliste arvutuste tegemiseks, nii et oma funktsioonide kirjutamine pole vajalik.\n",
    "\n",
    "Samuti saate luua oma kohandatud tööriistu, mida LLM saab kutsuda.\n",
    "\n",
    "Allolevas koodinäites:\n",
    "\n",
    "- Määratleme süsteemi sisendis saadaval olevad tööriistad (brave_search, wolfram_alpha).\n",
    "- Saadame kasutaja sisendi, mis küsib ilma kohta teatud linnas.\n",
    "- LLM vastab tööriista kutsumisega Brave Search tööriista kaudu, mis näeb välja selline: `<|python_tag|>brave_search.call(query=\"Stockholmi ilm\")`\n",
    "\n",
    "*NB! See näide teeb ainult tööriista kutsumise. Kui soovite tulemusi saada, peate looma tasuta konto Brave API lehel ja määratlema funktsiooni ise.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Kuigi Llama 3.1 on LLM, on sellel üks piirang – multimodaalsus. See tähendab, et mudel ei suuda kasutada erinevat tüüpi sisendeid, nagu näiteks pilte, ja vastata nende põhjal. Multimodaalsus on üks peamisi Llama 3.2 omadusi. Need omadused hõlmavad ka:\n",
    "\n",
    "- Multimodaalsus – suudab analüüsida nii tekstilisi kui ka visuaalseid sisendeid\n",
    "- Väikesed kuni keskmise suurusega variatsioonid (11B ja 90B) – see pakub paindlikke juurutamisvõimalusi\n",
    "- Ainult tekstipõhised variatsioonid (1B ja 3B) – võimaldab mudelit juurutada serva-/mobiilseadmetes ja tagab madala latentsuse\n",
    "\n",
    "Multimodaalne tugi tähistab suurt edasiminekut avatud lähtekoodiga mudelite maailmas. Allolev koodinäide kasutab nii pilti kui ka tekstilist sisendit, et saada Llama 3.2 90B mudelilt pildi analüüs.\n",
    "\n",
    "### Multimodaalne tugi Llama 3.2-ga\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Õppimine ei lõpe siin, jätka teekonda\n",
    "\n",
    "Pärast selle õppetunni lõpetamist tutvu meie [Generatiivse tehisintellekti õppekollektsiooniga](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), et jätkata oma generatiivse tehisintellekti teadmiste arendamist!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Lahtiütlus**:  \nSee dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-10-11T12:07:45+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "et"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}