<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-18T00:11:40+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "hi"
}
-->
# आपकी जनरेटिव AI एप्लिकेशन को सुरक्षित बनाना

[![आपकी जनरेटिव AI एप्लिकेशन को सुरक्षित बनाना](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.hi.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## परिचय

इस पाठ में शामिल हैं:

- AI सिस्टम के संदर्भ में सुरक्षा।
- AI सिस्टम के लिए सामान्य जोखिम और खतरे।
- AI सिस्टम को सुरक्षित बनाने के तरीके और विचार।

## सीखने के लक्ष्य

इस पाठ को पूरा करने के बाद, आप समझ पाएंगे:

- AI सिस्टम के लिए खतरे और जोखिम।
- AI सिस्टम को सुरक्षित बनाने के सामान्य तरीके और प्रथाएं।
- कैसे सुरक्षा परीक्षण अप्रत्याशित परिणामों और उपयोगकर्ता विश्वास के क्षरण को रोक सकता है।

## जनरेटिव AI के संदर्भ में सुरक्षा का क्या मतलब है?

जैसे-जैसे कृत्रिम बुद्धिमत्ता (AI) और मशीन लर्निंग (ML) तकनीकें हमारे जीवन को आकार देती हैं, यह न केवल ग्राहक डेटा बल्कि AI सिस्टम को भी सुरक्षित रखने के लिए महत्वपूर्ण हो जाता है। AI/ML का उपयोग उच्च-मूल्य निर्णय लेने की प्रक्रियाओं में तेजी से किया जा रहा है, जहां गलत निर्णय गंभीर परिणाम दे सकते हैं।

यहां कुछ मुख्य बिंदु हैं जिन्हें ध्यान में रखना चाहिए:

- **AI/ML का प्रभाव**: AI/ML का दैनिक जीवन पर महत्वपूर्ण प्रभाव पड़ता है और इसलिए इन्हें सुरक्षित रखना आवश्यक हो गया है।
- **सुरक्षा चुनौतियां**: AI/ML का यह प्रभाव उचित ध्यान देने की आवश्यकता है ताकि AI-आधारित उत्पादों को ट्रोल्स या संगठित समूहों द्वारा किए गए परिष्कृत हमलों से बचाया जा सके।
- **रणनीतिक समस्याएं**: तकनीकी उद्योग को दीर्घकालिक ग्राहक सुरक्षा और डेटा सुरक्षा सुनिश्चित करने के लिए रणनीतिक चुनौतियों का सक्रिय रूप से समाधान करना चाहिए।

इसके अलावा, मशीन लर्निंग मॉडल आमतौर पर दुर्भावनापूर्ण इनपुट और सामान्य डेटा के बीच अंतर करने में असमर्थ होते हैं। प्रशिक्षण डेटा का एक महत्वपूर्ण स्रोत असंरचित, अप्रबंधित, सार्वजनिक डेटासेट से प्राप्त होता है, जो तीसरे पक्ष के योगदान के लिए खुले होते हैं। हमलावरों को डेटासेट से समझौता करने की आवश्यकता नहीं होती जब वे इसमें योगदान करने के लिए स्वतंत्र होते हैं। समय के साथ, कम विश्वास वाले दुर्भावनापूर्ण डेटा उच्च विश्वास वाले विश्वसनीय डेटा बन जाते हैं, यदि डेटा संरचना/स्वरूप सही रहता है।

यही कारण है कि आपके मॉडल द्वारा निर्णय लेने के लिए उपयोग किए जाने वाले डेटा स्टोर्स की अखंडता और सुरक्षा सुनिश्चित करना महत्वपूर्ण है।

## AI के खतरे और जोखिम को समझना

AI और संबंधित सिस्टम के संदर्भ में, डेटा विषाक्तता आज सबसे महत्वपूर्ण सुरक्षा खतरा है। डेटा विषाक्तता तब होती है जब कोई व्यक्ति जानबूझकर AI को प्रशिक्षित करने के लिए उपयोग की जाने वाली जानकारी को बदल देता है, जिससे यह गलतियां करता है। यह मानकीकृत पहचान और शमन विधियों की अनुपस्थिति के कारण होता है, साथ ही प्रशिक्षण के लिए अविश्वसनीय या अप्रबंधित सार्वजनिक डेटासेट पर हमारी निर्भरता के कारण। डेटा की अखंडता बनाए रखने और दोषपूर्ण प्रशिक्षण प्रक्रिया को रोकने के लिए, आपके डेटा की उत्पत्ति और वंश का पता लगाना महत्वपूर्ण है। अन्यथा, पुरानी कहावत "कचरा अंदर, कचरा बाहर" सच होती है, जिससे मॉडल प्रदर्शन प्रभावित होता है।

यहां कुछ उदाहरण दिए गए हैं कि डेटा विषाक्तता आपके मॉडल को कैसे प्रभावित कर सकती है:

1. **लेबल फ्लिपिंग**: एक द्विआधारी वर्गीकरण कार्य में, एक विरोधी जानबूझकर प्रशिक्षण डेटा के एक छोटे हिस्से के लेबल को पलट देता है। उदाहरण के लिए, सामान्य नमूनों को दुर्भावनापूर्ण के रूप में लेबल किया जाता है, जिससे मॉडल गलत संघ सीखता है।\
   **उदाहरण**: एक स्पैम फ़िल्टर वैध ईमेल को स्पैम के रूप में गलत वर्गीकृत करता है क्योंकि लेबल में हेरफेर किया गया है।
2. **फीचर विषाक्तता**: एक हमलावर प्रशिक्षण डेटा में विशेषताओं को सूक्ष्म रूप से संशोधित करता है ताकि मॉडल में पूर्वाग्रह या भ्रम पैदा हो।\
   **उदाहरण**: सिफारिश प्रणाली को प्रभावित करने के लिए उत्पाद विवरणों में अप्रासंगिक कीवर्ड जोड़ना।
3. **डेटा इंजेक्शन**: मॉडल के व्यवहार को प्रभावित करने के लिए प्रशिक्षण सेट में दुर्भावनापूर्ण डेटा डालना।\
   **उदाहरण**: नकली उपयोगकर्ता समीक्षाओं को शामिल करना ताकि भावना विश्लेषण के परिणामों को प्रभावित किया जा सके।
4. **बैकडोर अटैक**: एक विरोधी प्रशिक्षण डेटा में एक छिपा हुआ पैटर्न (बैकडोर) डालता है। मॉडल इस पैटर्न को पहचानना सीखता है और ट्रिगर होने पर दुर्भावनापूर्ण तरीके से व्यवहार करता है।\
   **उदाहरण**: एक फेस रिकग्निशन सिस्टम जो बैकडोर इमेज के साथ प्रशिक्षित होता है और एक विशिष्ट व्यक्ति की गलत पहचान करता है।

MITRE Corporation ने [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) नामक एक ज्ञान आधार बनाया है, जो AI सिस्टम पर वास्तविक दुनिया के हमलों में उपयोग की जाने वाली रणनीतियों और तकनीकों का संग्रह है।

> AI-सक्षम सिस्टम में कमजोरियों की संख्या बढ़ रही है, क्योंकि AI का समावेश मौजूदा सिस्टम के हमले की सतह को पारंपरिक साइबर-हमलों से परे बढ़ा देता है। हमने ATLAS को इन अनूठी और विकसित कमजोरियों के बारे में जागरूकता बढ़ाने के लिए विकसित किया है, क्योंकि वैश्विक समुदाय विभिन्न सिस्टम में AI को तेजी से शामिल कर रहा है। ATLAS को MITRE ATT&CK® फ्रेमवर्क के बाद मॉडल किया गया है और इसके रणनीति, तकनीक, और प्रक्रियाएं (TTPs) ATT&CK में पूरक हैं।

जैसे MITRE ATT&CK® फ्रेमवर्क पारंपरिक साइबर सुरक्षा में उन्नत खतरे अनुकरण परिदृश्यों की योजना बनाने के लिए व्यापक रूप से उपयोग किया जाता है, ATLAS एक आसानी से खोजने योग्य TTPs सेट प्रदान करता है जो उभरते हमलों के खिलाफ बचाव के लिए बेहतर समझ और तैयारी में मदद कर सकता है।

इसके अलावा, Open Web Application Security Project (OWASP) ने "[Top 10 सूची](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" बनाई है, जो LLMs का उपयोग करने वाले एप्लिकेशन में पाई जाने वाली सबसे महत्वपूर्ण कमजोरियों को उजागर करती है। सूची में डेटा विषाक्तता जैसे खतरों के साथ-साथ अन्य जोखिमों को भी शामिल किया गया है, जैसे:

- **प्रॉम्प्ट इंजेक्शन**: एक तकनीक जिसमें हमलावर बड़े भाषा मॉडल (LLM) को सावधानीपूर्वक तैयार किए गए इनपुट के माध्यम से हेरफेर करते हैं, जिससे यह अपने इच्छित व्यवहार से बाहर हो जाता है।
- **सप्लाई चेन कमजोरियां**: LLM द्वारा उपयोग किए जाने वाले एप्लिकेशन के घटक और सॉफ़्टवेयर, जैसे कि Python मॉड्यूल या बाहरी डेटासेट, स्वयं समझौता किए जा सकते हैं, जिससे अप्रत्याशित परिणाम, पूर्वाग्रह और यहां तक कि बुनियादी ढांचे में कमजोरियां उत्पन्न हो सकती हैं।
- **अत्यधिक निर्भरता**: LLMs त्रुटिपूर्ण होते हैं और गलत या असुरक्षित परिणाम प्रदान करने के लिए प्रवृत्त होते हैं। कई प्रलेखित परिस्थितियों में, लोगों ने परिणामों को सही मान लिया है, जिससे अनपेक्षित वास्तविक दुनिया के नकारात्मक परिणाम हुए हैं।

Microsoft Cloud Advocate Rod Trent ने एक मुफ्त ईबुक, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), लिखी है, जो इन और अन्य उभरते AI खतरों में गहराई से जाती है और इन परिदृश्यों को सर्वोत्तम तरीके से निपटने के लिए व्यापक मार्गदर्शन प्रदान करती है।

## AI सिस्टम और LLMs के लिए सुरक्षा परीक्षण

कृत्रिम बुद्धिमत्ता (AI) विभिन्न क्षेत्रों और उद्योगों को बदल रही है, समाज के लिए नई संभावनाएं और लाभ प्रदान कर रही है। हालांकि, AI डेटा गोपनीयता, पूर्वाग्रह, व्याख्या की कमी, और संभावित दुरुपयोग जैसे महत्वपूर्ण चुनौतियां और जोखिम भी पैदा करता है। इसलिए, यह सुनिश्चित करना महत्वपूर्ण है कि AI सिस्टम सुरक्षित और जिम्मेदार हैं, जिसका अर्थ है कि वे नैतिक और कानूनी मानकों का पालन करते हैं और उपयोगकर्ताओं और हितधारकों द्वारा उन पर भरोसा किया जा सकता है।

सुरक्षा परीक्षण एक AI सिस्टम या LLM की सुरक्षा का मूल्यांकन करने की प्रक्रिया है, जिसमें उनकी कमजोरियों की पहचान और उनका उपयोग करना शामिल है। यह डेवलपर्स, उपयोगकर्ताओं, या तीसरे पक्ष के ऑडिटर्स द्वारा किया जा सकता है, परीक्षण के उद्देश्य और दायरे के आधार पर। AI सिस्टम और LLMs के लिए कुछ सबसे सामान्य सुरक्षा परीक्षण विधियां हैं:

- **डेटा सैनिटाइजेशन**: यह प्रक्रिया प्रशिक्षण डेटा या AI सिस्टम या LLM के इनपुट से संवेदनशील या निजी जानकारी को हटाने या गुमनाम करने की है। डेटा सैनिटाइजेशन डेटा लीक और दुर्भावनापूर्ण हेरफेर को रोकने में मदद कर सकता है, गोपनीय या व्यक्तिगत डेटा के जोखिम को कम करके।
- **एडवर्सेरियल टेस्टिंग**: यह प्रक्रिया AI सिस्टम या LLM के इनपुट या आउटपुट पर एडवर्सेरियल उदाहरण उत्पन्न करने और लागू करने की है ताकि एडवर्सेरियल हमलों के खिलाफ इसकी मजबूती और लचीलापन का मूल्यांकन किया जा सके। एडवर्सेरियल टेस्टिंग AI सिस्टम या LLM की कमजोरियों और कमजोरियों की पहचान और शमन में मदद कर सकती है।
- **मॉडल सत्यापन**: यह प्रक्रिया AI सिस्टम या LLM के मॉडल पैरामीटर या आर्किटेक्चर की शुद्धता और पूर्णता को सत्यापित करने की है। मॉडल सत्यापन मॉडल चोरी का पता लगाने और रोकने में मदद कर सकता है, यह सुनिश्चित करके कि मॉडल सुरक्षित और प्रमाणित है।
- **आउटपुट सत्यापन**: यह प्रक्रिया AI सिस्टम या LLM के आउटपुट की गुणवत्ता और विश्वसनीयता को सत्यापित करने की है। आउटपुट सत्यापन दुर्भावनापूर्ण हेरफेर का पता लगाने और सुधारने में मदद कर सकता है, यह सुनिश्चित करके कि आउटपुट सुसंगत और सटीक है।

OpenAI, जो AI सिस्टम में अग्रणी है, ने अपने रेड टीमिंग नेटवर्क पहल के हिस्से के रूप में _सुरक्षा मूल्यांकन_ की एक श्रृंखला स्थापित की है, जिसका उद्देश्य AI सिस्टम के आउटपुट का परीक्षण करना है ताकि AI सुरक्षा में योगदान दिया जा सके।

> मूल्यांकन सरल प्रश्नोत्तर परीक्षणों से लेकर अधिक जटिल सिमुलेशन तक हो सकते हैं। ठोस उदाहरण के रूप में, OpenAI द्वारा AI व्यवहारों का विभिन्न कोणों से मूल्यांकन करने के लिए विकसित किए गए नमूना मूल्यांकन यहां दिए गए हैं:

#### प्रेरणा

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम दूसरे AI सिस्टम को गुप्त शब्द कहने के लिए कितनी अच्छी तरह से प्रेरित कर सकता है?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम दूसरे AI सिस्टम को पैसे दान करने के लिए कितनी अच्छी तरह से प्रेरित कर सकता है?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम दूसरे AI सिस्टम के राजनीतिक प्रस्ताव के समर्थन को कितनी अच्छी तरह से प्रभावित कर सकता है?

#### स्टेगनोग्राफी (छिपे हुए संदेश)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम कितनी अच्छी तरह से गुप्त संदेश पास कर सकता है बिना दूसरे AI सिस्टम द्वारा पकड़े जाने के?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम कितनी अच्छी तरह से संदेशों को संपीड़ित और डीकंप्रेस कर सकता है, ताकि गुप्त संदेश छिपाने में मदद मिल सके?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI सिस्टम बिना सीधे संचार के दूसरे AI सिस्टम के साथ कितनी अच्छी तरह से समन्वय कर सकता है?

### AI सुरक्षा

यह आवश्यक है कि हम AI सिस्टम को दुर्भावनापूर्ण हमलों, दुरुपयोग, या अनपेक्षित परिणामों से बचाने का प्रयास करें। इसमें AI सिस्टम की सुरक्षा, विश्वसनीयता, और भरोसेमंदता सुनिश्चित करने के लिए कदम उठाना शामिल है, जैसे:

- AI मॉडल को प्रशिक्षित और चलाने के लिए उपयोग किए जाने वाले डेटा और एल्गोरिदम को सुरक्षित करना।
- AI सिस्टम तक अनधिकृत पहुंच, हेरफेर, या तोड़फोड़ को रोकना।
- AI सिस्टम में पूर्वाग्रह, भेदभाव, या नैतिक मुद्दों का पता लगाना और उन्हें कम करना।
- AI निर्णयों और कार्यों की जवाबदेही, पारदर्शिता, और व्याख्या सुनिश्चित करना।
- AI सिस्टम के लक्ष्यों और मूल्यों को मानव और समाज के साथ संरेखित करना।

AI सुरक्षा AI सिस्टम और डेटा की अखंडता, उपलब्धता, और गोपनीयता सुनिश्चित करने के लिए महत्वपूर्ण है। AI सुरक्षा की कुछ चुनौतियां और अवसर हैं:

- अवसर: साइबर सुरक्षा रणनीतियों में AI को शामिल करना क्योंकि यह खतरों की पहचान करने और प्रतिक्रिया समय में सुधार करने में महत्वपूर्ण भूमिका निभा सकता है। AI फ़िशिंग, मैलवेयर, या रैंसमवेयर जैसे साइबर हमलों का पता लगाने और शमन करने में मदद कर सकता है।
- चुनौती: AI का उपयोग विरोधियों द्वारा परिष्कृत हमले शुरू करने के लिए भी किया जा सकता है, जैसे कि नकली या भ्रामक सामग्री उत्पन्न करना, उपयोगकर्ताओं का प्रतिरूपण करना, या AI सिस्टम में कमजोरियों का फायदा उठाना। इसलिए, AI डेवलपर्स की एक अनूठी जिम्मेदारी है कि वे सिस्टम को दुरुपयोग के खिलाफ मजबूत और लचीला डिजाइन करें।

### डेटा सुरक्षा

LLMs उस डेटा की गोपनीयता और सुरक्षा के लिए जोखिम पैदा कर सकते हैं जिसका वे उपयोग करते हैं। उदाहरण के लिए, LLMs अपने प्रशिक्षण डेटा से संवेदनशील जानकारी जैसे व्यक्तिगत नाम, पते, पासवर्ड, या क्रेडिट कार्ड नंबर को याद कर सकते हैं और लीक कर सकते हैं। उन्हें दुर्भावनापूर्ण अभिनेताओं द्वारा हेरफेर या हमला भी किया जा सकता है जो उनकी कमजोरियों या पूर्वाग्रहों का फायदा उठाना चाहते हैं। इसलिए, इन जोखिमों से अवगत होना और LLMs के साथ उपयोग किए गए डेटा की सुरक्षा के लिए उचित उपाय करना महत्वपूर्ण है। LLMs के साथ उपयोग किए गए डेटा की सुरक्षा के लिए आप कई कदम उठा सकते हैं। इनमें शामिल हैं:

- **LLMs के साथ साझा किए गए डेटा की मात्रा और प्रकार को सीमित करना**: केवल वही डेटा साझा करें जो आवश्यक और इच्छित उद्देश्यों के लिए प्रासंगिक हो, और किसी भी संवेदनशील, गोपनीय, या व्यक्तिगत डेटा को साझा करने से बचें। उपयोगकर्ताओं को LLMs के साथ साझा किए गए डेटा को गुमनाम या एन्क्रिप्ट करना चाहिए, जैसे कि किसी भी पहचान जानकारी को हटाना या मास्क करना, या सुरक्षित संचार चैनलों का उपयोग करना।
- **LLMs द्वारा उत्पन्न डेटा को सत्यापित करना**: हमेशा LLMs द्वारा उत्पन्न आउटपुट की सटीकता और गुणवत्ता की जांच करें ताकि यह सुनिश्चित हो सके कि वे किसी भी अवांछित या अनुचित जानकारी को शामिल नहीं करते हैं।
- **किसी भी डेटा उल्लंघन या घटनाओं की रिपोर्टिंग और अलर्टिंग**: LLMs से किसी भी संदिग्ध या असामान्य गतिविधियों या व्यवहारों के प्रति सतर्क रहें, जैसे कि ऐसे टेक्स्ट उत्पन्न करना जो अप्रासंगिक, गलत, अपमानजनक, या हानिकारक हैं। यह डेटा उल्लंघन या सुरक्षा घटना का संकेत हो सकता है।

डेटा सुरक्षा, शासन, और अनुपालन किसी भी संगठन के लिए महत्वपूर्ण हैं जो बहु-क्लाउड वातावरण में डेटा और AI की शक्ति का लाभ उठाना चाहता है। सभी डेटा को सुरक्षित और नियंत्रित करना एक जटिल और बहुआयामी कार्य है। आपको विभिन्न प्रकार के डेटा (संरचित, असंरचित, और AI द्वारा उत्पन्न डेटा) को विभिन्न स्थानों में कई क्लाउड में सुरक्षित और नियंत्रित करना होगा, और आपको मौजूदा और भविष्य के डेटा सुरक्षा, शासन, और AI नियमों का हिसाब देना होगा। अपने डेटा की सुरक्षा के लिए, आपको कुछ सर्वोत्तम प्रथाओं और सावधानियों को अपनाना होगा, जैसे:

- डेटा सुरक्षा और गोपनीयता सुविधाओं की पेशकश करने वाली क्लाउड सेवाओं या प्लेटफार्मों का उपयोग करें।
- अपने डेटा में त्रुटियों, असंगतियों, या विसंगतियों की जांच करने के लिए डेटा गुणवत्ता और सत्यापन उपकरण का उपयोग करें।
- यह सुनिश्चित करने के लिए डेटा शासन और नैतिकता ढांचे का उपयोग करें कि आपका डेटा जिम्मेदारी और पारदर्शिता के साथ उपयोग किया जा रहा है।

### वास्तविक दुनिया के खतरों का अनुकरण - AI रेड टीमिंग
वास्तविक दुनिया के खतरों की नकल करना अब मजबूत AI सिस्टम बनाने के लिए एक मानक प्रक्रिया मानी जाती है, जिसमें समान उपकरण, रणनीतियाँ और प्रक्रियाओं का उपयोग करके सिस्टम के जोखिमों की पहचान की जाती है और रक्षकों की प्रतिक्रिया का परीक्षण किया जाता है।

> AI रेड टीमिंग का अभ्यास अब एक विस्तारित अर्थ ले चुका है: यह न केवल सुरक्षा कमजोरियों की जांच करता है, बल्कि अन्य सिस्टम विफलताओं की भी जांच करता है, जैसे कि संभावित हानिकारक सामग्री का निर्माण। AI सिस्टम नए जोखिमों के साथ आते हैं, और रेड टीमिंग उन नए जोखिमों को समझने का मुख्य हिस्सा है, जैसे कि प्रॉम्प्ट इंजेक्शन और बिना आधार वाली सामग्री का उत्पादन। - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![रेड टीमिंग के लिए मार्गदर्शन और संसाधन](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.hi.png)]()

नीचे Microsoft के AI रेड टीम प्रोग्राम को आकार देने वाले मुख्य अंतर्दृष्टि दी गई हैं।

1. **AI रेड टीमिंग का विस्तृत दायरा:**
   AI रेड टीमिंग अब सुरक्षा और जिम्मेदार AI (RAI) परिणामों दोनों को शामिल करता है। पारंपरिक रूप से, रेड टीमिंग सुरक्षा पहलुओं पर केंद्रित थी, मॉडल को एक वेक्टर के रूप में मानते हुए (जैसे, अंतर्निहित मॉडल को चुराना)। हालांकि, AI सिस्टम नए सुरक्षा कमजोरियों (जैसे, प्रॉम्प्ट इंजेक्शन, पॉइज़निंग) को पेश करते हैं, जिन्हें विशेष ध्यान देने की आवश्यकता होती है। सुरक्षा से परे, AI रेड टीमिंग निष्पक्षता के मुद्दों (जैसे, रूढ़िवादिता) और हानिकारक सामग्री (जैसे, हिंसा का महिमामंडन) की भी जांच करता है। इन मुद्दों की प्रारंभिक पहचान से रक्षा निवेश को प्राथमिकता देने में मदद मिलती है।

2. **दुर्भावनापूर्ण और निर्दोष विफलताएँ:**
   AI रेड टीमिंग विफलताओं को दोनों, दुर्भावनापूर्ण और निर्दोष दृष्टिकोण से देखता है। उदाहरण के लिए, जब नए Bing की रेड टीमिंग की जाती है, तो हम न केवल यह पता लगाते हैं कि दुर्भावनापूर्ण हमलावर सिस्टम को कैसे बाधित कर सकते हैं, बल्कि यह भी देखते हैं कि सामान्य उपयोगकर्ता कैसे समस्याग्रस्त या हानिकारक सामग्री का सामना कर सकते हैं। पारंपरिक सुरक्षा रेड टीमिंग, जो मुख्य रूप से दुर्भावनापूर्ण अभिनेताओं पर केंद्रित होती है, के विपरीत, AI रेड टीमिंग व्यक्तित्वों और संभावित विफलताओं की एक व्यापक श्रेणी को ध्यान में रखता है।

3. **AI सिस्टम की गतिशील प्रकृति:**
   AI अनुप्रयोग लगातार विकसित होते रहते हैं। बड़े भाषा मॉडल अनुप्रयोगों में, डेवलपर्स बदलती आवश्यकताओं के अनुसार अनुकूलन करते हैं। निरंतर रेड टीमिंग विकसित होते जोखिमों के प्रति सतर्कता और अनुकूलन सुनिश्चित करता है।

AI रेड टीमिंग सब कुछ समाहित नहीं करता और इसे अतिरिक्त नियंत्रणों जैसे [रोल-आधारित एक्सेस कंट्रोल (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) और व्यापक डेटा प्रबंधन समाधानों के पूरक के रूप में माना जाना चाहिए। इसका उद्देश्य एक सुरक्षा रणनीति को पूरक बनाना है जो गोपनीयता और सुरक्षा को ध्यान में रखते हुए सुरक्षित और जिम्मेदार AI समाधानों को लागू करने पर केंद्रित है, जबकि पूर्वाग्रह, हानिकारक सामग्री और गलत जानकारी को कम करने की कोशिश करता है जो उपयोगकर्ता के विश्वास को कमजोर कर सकते हैं।

यहाँ कुछ अतिरिक्त पढ़ने की सामग्री की सूची दी गई है जो आपको यह बेहतर समझने में मदद कर सकती है कि रेड टीमिंग आपके AI सिस्टम में जोखिमों की पहचान और उन्हें कम करने में कैसे मदद कर सकती है:

- [बड़े भाषा मॉडल (LLMs) और उनके अनुप्रयोगों के लिए रेड टीमिंग की योजना बनाना](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [OpenAI रेड टीमिंग नेटवर्क क्या है?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI रेड टीमिंग - सुरक्षित और अधिक जिम्मेदार AI समाधान बनाने के लिए एक प्रमुख अभ्यास](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), AI सिस्टम पर वास्तविक दुनिया के हमलों में उपयोग की जाने वाली रणनीतियों और तकनीकों का एक ज्ञानकोष।

## ज्ञान जांच

डेटा की अखंडता बनाए रखने और दुरुपयोग को रोकने के लिए एक अच्छा दृष्टिकोण क्या हो सकता है?

1. डेटा एक्सेस और डेटा प्रबंधन के लिए मजबूत रोल-आधारित नियंत्रण रखें  
2. डेटा के गलत प्रतिनिधित्व या दुरुपयोग को रोकने के लिए डेटा लेबलिंग को लागू करें और उसका ऑडिट करें  
3. सुनिश्चित करें कि आपका AI इंफ्रास्ट्रक्चर सामग्री फ़िल्टरिंग का समर्थन करता है  

उत्तर: 1, हालांकि ये तीनों ही सिफारिशें बहुत अच्छी हैं, यह सुनिश्चित करना कि आप उपयोगकर्ताओं को उचित डेटा एक्सेस विशेषाधिकार सौंप रहे हैं, LLMs द्वारा उपयोग किए जाने वाले डेटा में हेरफेर और गलत प्रतिनिधित्व को रोकने में बहुत मदद करेगा।

## 🚀 चुनौती

यह जानने के लिए और पढ़ें कि आप AI के युग में [संवेदनशील जानकारी को कैसे प्रबंधित और संरक्षित कर सकते हैं](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)।

## शानदार काम, अपनी शिक्षा जारी रखें

इस पाठ को पूरा करने के बाद, हमारे [Generative AI Learning संग्रह](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) को देखें और अपने जनरेटिव AI ज्ञान को और बढ़ाएं!

पाठ 14 पर जाएं, जहाँ हम [जनरेटिव AI एप्लिकेशन जीवनचक्र](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) पर चर्चा करेंगे!

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।