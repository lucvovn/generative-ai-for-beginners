<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-18T00:13:51+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "hi"
}
-->
# रिट्रीवल ऑगमेंटेड जनरेशन (RAG) और वेक्टर डेटाबेस

[![रिट्रीवल ऑगमेंटेड जनरेशन (RAG) और वेक्टर डेटाबेस](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.hi.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

सर्च एप्लिकेशन के पाठ में, हमने संक्षेप में सीखा कि कैसे अपने डेटा को बड़े भाषा मॉडल (LLMs) में एकीकृत किया जा सकता है। इस पाठ में, हम आपके LLM एप्लिकेशन में डेटा को ग्राउंड करने की अवधारणा, प्रक्रिया की यांत्रिकी और डेटा को संग्रहीत करने के तरीकों, जिसमें एम्बेडिंग और टेक्स्ट दोनों शामिल हैं, पर गहराई से चर्चा करेंगे।

> **वीडियो जल्द ही उपलब्ध होगा**

## परिचय

इस पाठ में हम निम्नलिखित विषयों को कवर करेंगे:

- RAG का परिचय, यह क्या है और AI (कृत्रिम बुद्धिमत्ता) में इसका उपयोग क्यों किया जाता है।

- वेक्टर डेटाबेस क्या हैं और हमारे एप्लिकेशन के लिए एक कैसे बनाएं।

- एक व्यावहारिक उदाहरण कि कैसे RAG को एप्लिकेशन में एकीकृत किया जाए।

## सीखने के लक्ष्य

इस पाठ को पूरा करने के बाद, आप सक्षम होंगे:

- डेटा रिट्रीवल और प्रोसेसिंग में RAG के महत्व को समझा सकें।

- RAG एप्लिकेशन सेटअप करें और अपने डेटा को LLM में ग्राउंड करें।

- LLM एप्लिकेशन में RAG और वेक्टर डेटाबेस का प्रभावी एकीकरण करें।

## हमारा परिदृश्य: अपने डेटा के साथ LLM को बेहतर बनाना

इस पाठ के लिए, हम अपनी खुद की नोट्स को शिक्षा स्टार्टअप में जोड़ना चाहते हैं, जिससे चैटबॉट विभिन्न विषयों पर अधिक जानकारी प्राप्त कर सके। हमारे पास जो नोट्स हैं, उनका उपयोग करके, शिक्षार्थी बेहतर अध्ययन कर सकेंगे और विभिन्न विषयों को समझ सकेंगे, जिससे उनकी परीक्षाओं की तैयारी आसान हो जाएगी। इस परिदृश्य को बनाने के लिए, हम उपयोग करेंगे:

- `Azure OpenAI:` LLM जिसे हम अपने चैटबॉट बनाने के लिए उपयोग करेंगे।

- `AI for beginners' lesson on Neural Networks:` यह वह डेटा होगा जिस पर हम अपने LLM को ग्राउंड करेंगे।

- `Azure AI Search` और `Azure Cosmos DB:` वेक्टर डेटाबेस जिसमें हम अपना डेटा संग्रहीत करेंगे और एक सर्च इंडेक्स बनाएंगे।

उपयोगकर्ता अपने नोट्स से प्रैक्टिस क्विज़ बना सकेंगे, रिवीजन फ्लैश कार्ड्स और इसे संक्षिप्त सारांश में बदल सकेंगे। शुरू करने के लिए, आइए देखें कि RAG क्या है और यह कैसे काम करता है:

## रिट्रीवल ऑगमेंटेड जनरेशन (RAG)

एक LLM संचालित चैटबॉट उपयोगकर्ता के प्रॉम्प्ट को प्रोसेस करता है और उत्तर उत्पन्न करता है। इसे इंटरैक्टिव होने के लिए डिज़ाइन किया गया है और यह उपयोगकर्ताओं के साथ विभिन्न विषयों पर संवाद करता है। हालांकि, इसके उत्तर केवल प्रदान किए गए संदर्भ और इसके मूलभूत प्रशिक्षण डेटा तक सीमित होते हैं। उदाहरण के लिए, GPT-4 का ज्ञान कटऑफ सितंबर 2021 है, जिसका मतलब है कि इसे इस अवधि के बाद हुई घटनाओं का ज्ञान नहीं है। इसके अलावा, LLMs को प्रशिक्षित करने के लिए उपयोग किए गए डेटा में गोपनीय जानकारी जैसे व्यक्तिगत नोट्स या किसी कंपनी के उत्पाद मैनुअल शामिल नहीं होते हैं।

### RAGs (रिट्रीवल ऑगमेंटेड जनरेशन) कैसे काम करते हैं

![RAGs कैसे काम करते हैं इसका चित्रण](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.hi.png)

मान लीजिए आप एक ऐसा चैटबॉट तैनात करना चाहते हैं जो आपके नोट्स से क्विज़ बनाता है, तो आपको नॉलेज बेस से कनेक्शन की आवश्यकता होगी। यही वह जगह है जहां RAG मदद करता है। RAGs निम्नलिखित तरीके से काम करते हैं:

- **नॉलेज बेस:** रिट्रीवल से पहले, इन दस्तावेजों को इनजेस्ट और प्रीप्रोसेस करने की आवश्यकता होती है, आमतौर पर बड़े दस्तावेजों को छोटे हिस्सों में तोड़कर, उन्हें टेक्स्ट एम्बेडिंग में बदलकर और उन्हें डेटाबेस में संग्रहीत करके।

- **उपयोगकर्ता क्वेरी:** उपयोगकर्ता एक प्रश्न पूछता है।

- **रिट्रीवल:** जब उपयोगकर्ता एक प्रश्न पूछता है, तो एम्बेडिंग मॉडल हमारे नॉलेज बेस से प्रासंगिक जानकारी को पुनः प्राप्त करता है ताकि अधिक संदर्भ प्रदान किया जा सके जो प्रॉम्प्ट में शामिल किया जाएगा।

- **ऑगमेंटेड जनरेशन:** LLM अपने उत्तर को पुनः प्राप्त डेटा के आधार पर बढ़ाता है। यह सुनिश्चित करता है कि उत्पन्न उत्तर न केवल पूर्व-प्रशिक्षित डेटा पर आधारित हो बल्कि जोड़े गए संदर्भ से प्रासंगिक जानकारी भी शामिल हो। पुनः प्राप्त डेटा का उपयोग LLM के उत्तरों को बढ़ाने के लिए किया जाता है। इसके बाद LLM उपयोगकर्ता के प्रश्न का उत्तर देता है।

![RAGs आर्किटेक्चर का चित्रण](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.hi.png)

RAGs के लिए आर्किटेक्चर ट्रांसफॉर्मर्स का उपयोग करके लागू किया जाता है जिसमें दो भाग होते हैं: एक एन्कोडर और एक डिकोडर। उदाहरण के लिए, जब उपयोगकर्ता एक प्रश्न पूछता है, तो इनपुट टेक्स्ट को 'एन्कोड' किया जाता है और इसे वेक्टर में बदला जाता है जो शब्दों के अर्थ को कैप्चर करता है और वेक्टर को हमारे डॉक्यूमेंट इंडेक्स में 'डिकोड' किया जाता है और उपयोगकर्ता क्वेरी के आधार पर नया टेक्स्ट उत्पन्न करता है। LLM आउटपुट उत्पन्न करने के लिए एन्कोडर-डिकोडर मॉडल का उपयोग करता है।

प्रस्तावित पेपर के अनुसार RAG को लागू करने के दो दृष्टिकोण हैं: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst):

- **_RAG-Sequence_** पुनः प्राप्त दस्तावेजों का उपयोग करके उपयोगकर्ता क्वेरी का सबसे अच्छा संभव उत्तर भविष्यवाणी करना।

- **RAG-Token** दस्तावेजों का उपयोग करके अगला टोकन उत्पन्न करना, फिर उन्हें उपयोगकर्ता की क्वेरी का उत्तर देने के लिए पुनः प्राप्त करना।

### आप RAGs का उपयोग क्यों करेंगे? 

- **जानकारी की समृद्धता:** यह सुनिश्चित करता है कि टेक्स्ट उत्तर अद्यतन और वर्तमान हैं। यह, इसलिए, आंतरिक नॉलेज बेस तक पहुंचकर डोमेन विशिष्ट कार्यों पर प्रदर्शन को बढ़ाता है।

- **सत्यापन योग्य डेटा** का उपयोग करके उपयोगकर्ता क्वेरी को संदर्भ प्रदान करके गलत जानकारी को कम करता है।

- यह **लागत प्रभावी** है क्योंकि वे LLM को फाइन-ट्यून करने की तुलना में अधिक किफायती हैं।

## नॉलेज बेस बनाना

हमारा एप्लिकेशन हमारे व्यक्तिगत डेटा पर आधारित है, यानी AI For Beginners पाठ्यक्रम पर न्यूरल नेटवर्क पाठ।

### वेक्टर डेटाबेस

एक पारंपरिक डेटाबेस के विपरीत, वेक्टर डेटाबेस एक विशेष प्रकार का डेटाबेस है जिसे एम्बेडेड वेक्टर को संग्रहीत, प्रबंधित और खोजने के लिए डिज़ाइन किया गया है। यह दस्तावेजों के संख्यात्मक प्रतिनिधित्व को संग्रहीत करता है। डेटा को संख्यात्मक एम्बेडिंग में तोड़ने से हमारे AI सिस्टम के लिए डेटा को समझना और प्रोसेस करना आसान हो जाता है।

हम अपने एम्बेडिंग को वेक्टर डेटाबेस में संग्रहीत करते हैं क्योंकि LLMs के इनपुट के रूप में स्वीकार किए जाने वाले टोकन की संख्या सीमित होती है। चूंकि आप पूरे एम्बेडिंग को LLM में पास नहीं कर सकते, हमें उन्हें छोटे हिस्सों में तोड़ने की आवश्यकता होगी और जब उपयोगकर्ता एक प्रश्न पूछता है, तो प्रश्न के सबसे समान एम्बेडिंग प्रॉम्प्ट के साथ वापस कर दिए जाएंगे। चंकिंग LLM के माध्यम से पास किए गए टोकन की संख्या पर लागत को भी कम करता है।

कुछ लोकप्रिय वेक्टर डेटाबेस में Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant और DeepLake शामिल हैं। आप निम्नलिखित कमांड का उपयोग करके Azure CLI के साथ Azure Cosmos DB मॉडल बना सकते हैं:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### टेक्स्ट से एम्बेडिंग तक

हमारे डेटा को संग्रहीत करने से पहले, हमें इसे डेटाबेस में संग्रहीत करने से पहले वेक्टर एम्बेडिंग में बदलने की आवश्यकता होगी। यदि आप बड़े दस्तावेजों या लंबे टेक्स्ट के साथ काम कर रहे हैं, तो आप उन्हें अपेक्षित क्वेरी के आधार पर चंक कर सकते हैं। चंकिंग वाक्य स्तर पर या पैराग्राफ स्तर पर की जा सकती है। चूंकि चंकिंग आसपास के शब्दों से अर्थ प्राप्त करती है, आप चंक में कुछ अन्य संदर्भ जोड़ सकते हैं, उदाहरण के लिए, दस्तावेज़ शीर्षक जोड़कर या चंक से पहले या बाद में कुछ टेक्स्ट शामिल करके। आप डेटा को निम्नलिखित तरीके से चंक कर सकते हैं:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

एक बार चंक हो जाने के बाद, हम अपने टेक्स्ट को विभिन्न एम्बेडिंग मॉडल का उपयोग करके एम्बेड कर सकते हैं। आप जिन मॉडलों का उपयोग कर सकते हैं उनमें शामिल हैं: word2vec, ada-002 OpenAI द्वारा, Azure Computer Vision और कई अन्य। उपयोग करने के लिए मॉडल का चयन करना इस बात पर निर्भर करेगा कि आप कौन सी भाषाएँ उपयोग कर रहे हैं, एन्कोड की गई सामग्री का प्रकार (टेक्स्ट/इमेज/ऑडियो), एन्कोड कर सकने वाले इनपुट का आकार और एम्बेडिंग आउटपुट की लंबाई।

OpenAI के `text-embedding-ada-002` मॉडल का उपयोग करके एम्बेडेड टेक्स्ट का एक उदाहरण है:
![शब्द 'cat' का एम्बेडिंग](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.hi.png)

## रिट्रीवल और वेक्टर सर्च

जब उपयोगकर्ता एक प्रश्न पूछता है, तो रिट्रीवर इसे क्वेरी एन्कोडर का उपयोग करके वेक्टर में बदल देता है, फिर यह हमारे डॉक्यूमेंट सर्च इंडेक्स में इनपुट से संबंधित दस्तावेज़ों के प्रासंगिक वेक्टर के लिए खोज करता है। एक बार हो जाने के बाद, यह इनपुट वेक्टर और डॉक्यूमेंट वेक्टर दोनों को टेक्स्ट में बदल देता है और इसे LLM के माध्यम से पास करता है।

### रिट्रीवल

रिट्रीवल तब होता है जब सिस्टम इंडेक्स से उन दस्तावेजों को जल्दी से खोजने की कोशिश करता है जो सर्च मानदंडों को पूरा करते हैं। रिट्रीवर का लक्ष्य ऐसे दस्तावेज़ प्राप्त करना है जो संदर्भ प्रदान करने और आपके डेटा पर LLM को ग्राउंड करने के लिए उपयोग किए जाएंगे।

हमारे डेटाबेस के भीतर सर्च करने के कई तरीके हैं जैसे:

- **कीवर्ड सर्च** - टेक्स्ट सर्च के लिए उपयोग किया जाता है।

- **सेमांटिक सर्च** - शब्दों के सेमांटिक अर्थ का उपयोग करता है।

- **वेक्टर सर्च** - एम्बेडिंग मॉडल का उपयोग करके दस्तावेजों को टेक्स्ट से वेक्टर प्रतिनिधित्व में बदलता है। रिट्रीवल उपयोगकर्ता प्रश्न के सबसे निकट वेक्टर प्रतिनिधित्व वाले दस्तावेजों को क्वेरी करके किया जाएगा।

- **हाइब्रिड** - कीवर्ड और वेक्टर सर्च दोनों का संयोजन।

रिट्रीवल के साथ एक चुनौती तब आती है जब डेटाबेस में क्वेरी के समान कोई प्रतिक्रिया नहीं होती है, तो सिस्टम सबसे अच्छी जानकारी लौटाएगा जो वह प्राप्त कर सकता है, हालांकि, आप ऐसी रणनीतियों का उपयोग कर सकते हैं जैसे प्रासंगिकता के लिए अधिकतम दूरी सेट करना या कीवर्ड और वेक्टर सर्च दोनों को मिलाकर हाइब्रिड सर्च का उपयोग करना। इस पाठ में हम हाइब्रिड सर्च का उपयोग करेंगे, जो वेक्टर और कीवर्ड सर्च दोनों का संयोजन है। हम अपने डेटा को एक डेटा फ्रेम में संग्रहीत करेंगे जिसमें चंक्स और एम्बेडिंग्स वाले कॉलम होंगे।

### वेक्टर समानता

रिट्रीवर नॉलेज डेटाबेस के माध्यम से उन एम्बेडिंग्स की खोज करेगा जो एक-दूसरे के करीब हैं, सबसे नजदीकी पड़ोसी, क्योंकि वे समान टेक्स्ट हैं। परिदृश्य में उपयोगकर्ता एक क्वेरी पूछता है, इसे पहले एम्बेड किया जाता है और फिर समान एम्बेडिंग्स के साथ मिलान किया जाता है। सामान्य माप जो यह पता लगाने के लिए उपयोग किया जाता है कि विभिन्न वेक्टर कितने समान हैं, वह है कोसाइन समानता जो दो वेक्टर के बीच कोण पर आधारित है।

हम समानता को मापने के लिए अन्य विकल्पों का उपयोग कर सकते हैं जैसे कि यूक्लिडियन दूरी जो वेक्टर एंडपॉइंट्स के बीच सीधी रेखा है और डॉट प्रोडक्ट जो दो वेक्टर के संबंधित तत्वों के उत्पादों के योग को मापता है।

### सर्च इंडेक्स

रिट्रीवल करते समय, हमें सर्च करने से पहले अपने नॉलेज बेस के लिए एक सर्च इंडेक्स बनाना होगा। एक इंडेक्स हमारे एम्बेडिंग्स को संग्रहीत करेगा और बड़ी डेटाबेस में भी सबसे समान चंक्स को जल्दी से पुनः प्राप्त कर सकता है। हम अपने इंडेक्स को स्थानीय रूप से निम्नलिखित तरीके से बना सकते हैं:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### री-रैंकिंग

एक बार जब आप डेटाबेस को क्वेरी कर लेते हैं, तो आपको परिणामों को सबसे प्रासंगिक से क्रमबद्ध करने की आवश्यकता हो सकती है। एक री-रैंकिंग LLM मशीन लर्निंग का उपयोग करके सर्च परिणामों की प्रासंगिकता में सुधार करता है और उन्हें सबसे प्रासंगिक से क्रमबद्ध करता है। Azure AI Search का उपयोग करके, री-रैंकिंग आपके लिए स्वचालित रूप से की जाती है, जिसमें एक सेमांटिक री-रैंकर का उपयोग होता है। निकटतम पड़ोसियों का उपयोग करके री-रैंकिंग कैसे काम करती है इसका एक उदाहरण:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## इसे सब एक साथ लाना

अंतिम चरण हमारे डेटा पर आधारित उत्तर प्राप्त करने के लिए हमारे LLM को जोड़ना है। हम इसे निम्नलिखित तरीके से लागू कर सकते हैं:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## हमारे एप्लिकेशन का मूल्यांकन

### मूल्यांकन मेट्रिक्स

- प्रदान किए गए उत्तरों की गुणवत्ता सुनिश्चित करना कि वे प्राकृतिक, प्रवाहपूर्ण और मानव-समान लगें।

- डेटा का ग्राउंडेडनेस: मूल्यांकन करना कि उत्तर प्रदान किए गए दस्तावेजों से आया है या नहीं।

- प्रासंगिकता: मूल्यांकन करना कि उत्तर प्रश्न से मेल खाता है और संबंधित है।

- प्रवाह - यह सुनिश्चित करना कि उत्तर व्याकरणिक रूप से सही है।

## RAG (रिट्रीवल ऑगमेंटेड जनरेशन) और वेक्टर डेटाबेस का उपयोग करने के उपयोग के मामले

ऐसे कई उपयोग के मामले हैं जहां फ़ंक्शन कॉल आपके ऐप को बेहतर बना सकते हैं जैसे:

- प्रश्न और उत्तर: आपके कंपनी डेटा को एक चैट में ग्राउंड करना जिसे कर्मचारी प्रश्न पूछने के लिए उपयोग कर सकते हैं।

- सिफारिश प्रणाली: जहां आप एक प्रणाली बना सकते हैं जो सबसे समान मानों को मिलाती है जैसे कि फिल्में, रेस्तरां और कई अन्य।

- चैटबॉट सेवाएं: आप चैट इतिहास को संग्रहीत कर सकते हैं और उपयोगकर्ता डेटा के आधार पर बातचीत को व्यक्तिगत बना सकते हैं।

- वेक्टर एम्बेडिंग्स के आधार पर इमेज सर्च, जो इमेज पहचान और विसंगति का पता लगाने में उपयोगी है।

## सारांश

हमने RAG के मौलिक क्षेत्रों को कवर किया है, जिसमें हमारे डेटा को एप्लिकेशन में जोड़ना, उपयोगकर्ता क्वेरी और आउटपुट शामिल हैं। RAG बनाने को सरल बनाने के लिए, आप Semanti Kernel, Langchain या Autogen जैसे फ्रेमवर्क का उपयोग कर सकते हैं।

## असाइनमेंट

रिट्रीवल ऑगमेंटेड जनरेशन (RAG) के अपने सीखने को जारी रखने के लिए आप निम्नलिखित बना सकते हैं:

- अपनी पसंद के फ्रेमवर्क का उपयोग करके एप्लिकेशन के लिए एक फ्रंट-एंड बनाएं।

- LangChain या Semantic Kernel में से किसी एक फ्रेमवर्क का उपयोग करें और अपने एप्लिकेशन को फिर से बनाएं।

पाठ पूरा करने के लिए बधाई 👏।

## सीखना यहीं समाप्त नहीं होता, यात्रा जारी रखें

इस पाठ को पूरा करने के बाद, हमारे [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) को देखें और अपनी जनरेटिव AI ज्ञान को और बढ़ाएं!

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।