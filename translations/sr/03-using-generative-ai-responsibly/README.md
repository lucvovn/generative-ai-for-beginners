<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-18T01:18:56+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "sr"
}
-->
# Одговорна употреба генеративне вештачке интелигенције

[![Одговорна употреба генеративне вештачке интелигенције](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.sr.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _Кликните на слику изнад да бисте погледали видео лекцију_

Лако је бити фасциниран вештачком интелигенцијом, а посебно генеративном вештачком интелигенцијом, али је важно размотрити како је користити на одговоран начин. Треба узети у обзир како осигурати да резултати буду праведни, нештетни и још много тога. Ово поглавље има за циљ да вам пружи контекст, на шта треба обратити пажњу и како предузети активне кораке за побољшање употребе вештачке интелигенције.

## Увод

Ова лекција ће обухватити:

- Зашто је важно приоритетно поставити одговорну вештачку интелигенцију приликом креирања апликација заснованих на генеративној вештачкој интелигенцији.
- Основне принципе одговорне вештачке интелигенције и њихову повезаност са генеративном вештачком интелигенцијом.
- Како применити ове принципе одговорне вештачке интелигенције кроз стратегију и алате.

## Циљеви учења

Након завршетка ове лекције, знаћете:

- Значај одговорне вештачке интелигенције приликом креирања апликација заснованих на генеративној вештачкој интелигенцији.
- Када размишљати и применити основне принципе одговорне вештачке интелигенције приликом креирања апликација заснованих на генеративној вештачкој интелигенцији.
- Који алати и стратегије су вам доступни за примену концепта одговорне вештачке интелигенције.

## Принципи одговорне вештачке интелигенције

Узбуђење око генеративне вештачке интелигенције никада није било веће. Ово узбуђење је привукло много нових програмера, пажње и финансирања у ову област. Иако је ово веома позитивно за све који желе да граде производе и компаније користећи генеративну вештачку интелигенцију, важно је да поступамо одговорно.

Током овог курса, фокусираћемо се на изградњу нашег стартапа и нашег образовног производа заснованог на вештачкој интелигенцији. Користићемо принципе одговорне вештачке интелигенције: праведност, инклузивност, поузданост/безбедност, сигурност и приватност, транспарентност и одговорност. Уз ове принципе, истражићемо како се они односе на нашу употребу генеративне вештачке интелигенције у нашим производима.

## Зашто треба приоритетно поставити одговорну вештачку интелигенцију

Приликом креирања производа, приступ усмерен на људе, који узима у обзир најбољи интерес корисника, доводи до најбољих резултата.

Јединственост генеративне вештачке интелигенције лежи у њеној способности да креира корисне одговоре, информације, смернице и садржај за кориснике. Ово се може постићи без много ручних корака, што може довести до веома импресивних резултата. Међутим, без правилног планирања и стратегија, то може, нажалост, довести до штетних резултата за ваше кориснике, ваш производ и друштво у целини.

Хајде да погледамо неке (али не све) потенцијално штетне резултате:

### Халуцинације

Халуцинације су термин који описује ситуацију када LLM генерише садржај који је или потпуно бесмислен или нешто што знамо да је чињенично нетачно на основу других извора информација.

На пример, ако направимо функцију за наш стартап која омогућава студентима да постављају историјска питања моделу. Студент постави питање `Ко је био једини преживели са Титаника?`

Модел генерише одговор као што је следећи:

![Питање: "Ко је био једини преживели са Титаника"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Извор: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Ово је веома самоуверен и детаљан одговор. Нажалост, он је нетачан. Чак и уз минимално истраживање, могло би се открити да је било више преживелих у катастрофи Титаника. За студента који тек почиње да истражује ову тему, овај одговор може бити довољно убедљив да га не доводи у питање и да га прихвати као чињеницу. Последице овога могу довести до тога да систем вештачке интелигенције буде непоуздан и негативно утиче на репутацију нашег стартапа.

Са сваком итерацијом било ког датог LLM-а, видели смо побољшања у перформансама у смањењу халуцинација. Чак и уз ова побољшања, ми као програмери апликација и корисници и даље морамо бити свесни ових ограничења.

### Штетан садржај

У претходном делу смо покрили ситуације када LLM генерише нетачне или бесмислене одговоре. Још један ризик који морамо узети у обзир је када модел одговара штетним садржајем.

Штетан садржај може бити дефинисан као:

- Пружање упутстава или подстицање на самоповређивање или наношење штете одређеним групама.
- Говор мржње или омаловажавајући садржај.
- Упутства за планирање било које врсте напада или насилних дела.
- Пружање упутстава о томе како пронаћи незаконит садржај или починити незаконите радње.
- Приказивање сексуално експлицитног садржаја.

За наш стартап, желимо да се уверимо да имамо праве алате и стратегије како бисмо спречили да овај тип садржаја буде доступан студентима.

### Недостатак праведности

Праведност се дефинише као „осигурање да је систем вештачке интелигенције ослобођен пристрасности и дискриминације и да третира све људе праведно и једнако.“ У свету генеративне вештачке интелигенције, желимо да осигурамо да искључиви погледи на свет маргинализованих група нису ојачани излазом модела.

Овакви излази не само да су деструктивни за изградњу позитивног искуства производа за наше кориснике, већ узрокују и додатну друштвену штету. Као програмери апликација, увек треба да имамо на уму широку и разнолику базу корисника приликом креирања решења са генеративном вештачком интелигенцијом.

## Како одговорно користити генеративну вештачку интелигенцију

Сада када смо идентификовали значај одговорне генеративне вештачке интелигенције, хајде да погледамо 4 корака које можемо предузети да одговорно изградимо наша AI решења:

![Циклус ублажавања](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.sr.png)

### Мерење потенцијалних штета

У тестирању софтвера, тестирају се очекиване радње корисника на апликацији. Слично томе, тестирање разноврсног скупа упита које корисници највероватније користе је добар начин за мерење потенцијалне штете.

Пошто наш стартап гради образовни производ, било би добро припремити листу упита везаних за образовање. Ово би могло обухватити одређене предмете, историјске чињенице и упите о студентском животу.

### Ублажавање потенцијалних штета

Сада је време да пронађемо начине како можемо спречити или ограничити потенцијалну штету коју модел и његови одговори могу узроковати. Ово можемо посматрати кроз 4 различита слоја:

![Слојеви ублажавања](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.sr.png)

- **Модел**. Одабир правог модела за праву намену. Већи и сложенији модели као што је GPT-4 могу представљати већи ризик од штетног садржаја када се примењују на мање и специфичне намене. Коришћење ваших података за фино подешавање такође смањује ризик од штетног садржаја.

- **Систем безбедности**. Систем безбедности је скуп алата и конфигурација на платформи која служи моделу и помаже у ублажавању штете. Пример за ово је систем за филтрирање садржаја на Azure OpenAI услузи. Системи такође треба да детектују нападе на безбедност и нежељене активности као што су захтеви од ботова.

- **Метапромпт**. Метапромпти и ослањање на одређене информације су начини на које можемо усмерити или ограничити модел на основу одређених понашања и информација. Ово може укључивати коришћење системских уноса за дефинисање одређених ограничења модела. Поред тога, пружање излаза који су релевантнији за обим или домен система.

Такође се могу користити технике као што је Retrieval Augmented Generation (RAG) како би модел извлачио информације само из одабраних поузданих извора. Постоји лекција касније у овом курсу о [изградњи апликација за претрагу](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Корисничко искуство**. Завршни слој је место где корисник директно комуницира са моделом кроз интерфејс наше апликације на неки начин. На овај начин можемо дизајнирати UI/UX да ограничимо корисника у врстама уноса које може послати моделу, као и текст или слике које се приказују кориснику. Када се AI апликација имплементира, такође морамо бити транспарентни о томе шта наша генеративна AI апликација може, а шта не може да уради.

Имамо целу лекцију посвећену [Дизајнирању UX-а за AI апликације](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Евалуација модела**. Рад са LLM-овима може бити изазов јер немамо увек контролу над подацима на којима је модел обучен. Без обзира на то, увек треба да процењујемо перформансе и излазе модела. И даље је важно мерити тачност модела, сличност, ослањање на проверене изворе и релевантност излаза. Ово помаже у пружању транспарентности и поверења заинтересованим странама и корисницима.

### Оперативно управљање одговорним генеративним AI решењем

Изградња оперативне праксе око ваших AI апликација је завршна фаза. Ово укључује сарадњу са другим деловима нашег стартапа, као што су правни и безбедносни тимови, како бисмо осигурали усклађеност са свим регулаторним политикама. Пре лансирања, такође желимо да изградимо планове око испоруке, управљања инцидентима и повратка на претходно стање како бисмо спречили било какву штету корисницима.

## Алати

Иако се рад на развоју одговорних AI решења може чинити као велики задатак, то је рад који се свакако исплати. Како област генеративне вештачке интелигенције расте, више алата који помажу програмерима да ефикасно интегришу одговорност у своје радне токове ће се развијати. На пример, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) може помоћи у детекцији штетног садржаја и слика путем API захтева.

## Провера знања

На шта треба обратити пажњу како бисте осигурали одговорну употребу вештачке интелигенције?

1. Да ли је одговор тачан.  
2. Штетна употреба, да се AI не користи за криминалне сврхе.  
3. Осигурање да AI није пристрасан и дискриминаторски.  

О: 2 и 3 су тачни. Одговорна AI вам помаже да размотрите како да ублажите штетне ефекте, пристрасности и још много тога.

## 🚀 Изазов

Прочитајте више о [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) и видите шта можете применити у својој употреби.

## Одличан рад, наставите са учењем

Након завршетка ове лекције, погледајте нашу [колекцију за учење о генеративној вештачкој интелигенцији](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) како бисте наставили да унапређујете своје знање о генеративној вештачкој интелигенцији!

Прелазимо на лекцију 4 где ћемо погледати [Основе инжењеринга упита](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем услуге за превођење помоћу вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.