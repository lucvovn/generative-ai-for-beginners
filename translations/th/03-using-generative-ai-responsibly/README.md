<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-17T18:36:15+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "th"
}
-->
# การใช้ Generative AI อย่างรับผิดชอบ

[![การใช้ Generative AI อย่างรับผิดชอบ](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.th.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _คลิกที่ภาพด้านบนเพื่อดูวิดีโอของบทเรียนนี้_

AI และ Generative AI นั้นน่าตื่นเต้นและน่าสนใจ แต่คุณต้องพิจารณาว่าจะใช้งานมันอย่างรับผิดชอบได้อย่างไร คุณต้องคำนึงถึงสิ่งต่าง ๆ เช่น การทำให้ผลลัพธ์มีความยุติธรรม ไม่เป็นอันตราย และอื่น ๆ บทนี้มีเป้าหมายที่จะให้บริบทที่กล่าวถึง สิ่งที่ควรพิจารณา และวิธีการดำเนินการเพื่อปรับปรุงการใช้งาน AI ของคุณ

## บทนำ

บทเรียนนี้จะครอบคลุม:

- เหตุผลที่คุณควรให้ความสำคัญกับ Responsible AI เมื่อสร้างแอปพลิเคชัน Generative AI
- หลักการสำคัญของ Responsible AI และความสัมพันธ์กับ Generative AI
- วิธีการนำหลักการ Responsible AI ไปปฏิบัติผ่านกลยุทธ์และเครื่องมือ

## เป้าหมายการเรียนรู้

หลังจากจบบทเรียนนี้ คุณจะทราบว่า:

- ความสำคัญของ Responsible AI เมื่อสร้างแอปพลิเคชัน Generative AI
- เมื่อใดควรคิดและนำหลักการสำคัญของ Responsible AI ไปใช้เมื่อสร้างแอปพลิเคชัน Generative AI
- เครื่องมือและกลยุทธ์ที่มีอยู่เพื่อช่วยให้คุณนำแนวคิดของ Responsible AI ไปปฏิบัติ

## หลักการของ Responsible AI

ความตื่นเต้นเกี่ยวกับ Generative AI นั้นสูงกว่าที่เคย ความตื่นเต้นนี้ได้นำพานักพัฒนารายใหม่ ความสนใจ และเงินทุนจำนวนมากมายังพื้นที่นี้ แม้ว่าสิ่งนี้จะเป็นเรื่องดีสำหรับผู้ที่ต้องการสร้างผลิตภัณฑ์และบริษัทโดยใช้ Generative AI แต่ก็สำคัญที่เราจะดำเนินการอย่างรับผิดชอบ

ตลอดหลักสูตรนี้ เรามุ่งเน้นไปที่การสร้างสตาร์ทอัพและผลิตภัณฑ์การศึกษา AI ของเรา เราจะใช้หลักการของ Responsible AI: ความยุติธรรม ความครอบคลุม ความน่าเชื่อถือ/ความปลอดภัย ความปลอดภัยและความเป็นส่วนตัว ความโปร่งใส และความรับผิดชอบ ด้วยหลักการเหล่านี้ เราจะสำรวจว่ามันเกี่ยวข้องกับการใช้ Generative AI ในผลิตภัณฑ์ของเราอย่างไร

## ทำไมคุณควรให้ความสำคัญกับ Responsible AI

เมื่อสร้างผลิตภัณฑ์ การใช้วิธีการที่มุ่งเน้นมนุษย์โดยคำนึงถึงผลประโยชน์สูงสุดของผู้ใช้จะนำไปสู่ผลลัพธ์ที่ดีที่สุด

ความพิเศษของ Generative AI คือพลังในการสร้างคำตอบที่เป็นประโยชน์ ข้อมูล คำแนะนำ และเนื้อหาสำหรับผู้ใช้ สิ่งนี้สามารถทำได้โดยไม่ต้องมีขั้นตอนที่ซับซ้อนมากมาย ซึ่งสามารถนำไปสู่ผลลัพธ์ที่น่าประทับใจมาก อย่างไรก็ตาม หากไม่มีการวางแผนและกลยุทธ์ที่เหมาะสม ก็อาจนำไปสู่ผลลัพธ์ที่เป็นอันตรายต่อผู้ใช้ ผลิตภัณฑ์ และสังคมโดยรวมได้

ลองมาดูผลลัพธ์ที่อาจเป็นอันตรายบางส่วน (แต่ไม่ทั้งหมด):

### การสร้างข้อมูลผิดพลาด (Hallucinations)

Hallucinations เป็นคำที่ใช้เพื่ออธิบายเมื่อ LLM สร้างเนื้อหาที่ไม่มีความหมายหรือสิ่งที่เรารู้ว่าไม่ถูกต้องตามข้อเท็จจริงจากแหล่งข้อมูลอื่น

ลองสมมติว่าเราสร้างฟีเจอร์สำหรับสตาร์ทอัพของเราที่ให้นักเรียนถามคำถามเกี่ยวกับประวัติศาสตร์กับโมเดล นักเรียนถามคำถามว่า `ใครเป็นผู้รอดชีวิตเพียงคนเดียวจากเรือ Titanic?`

โมเดลสร้างคำตอบเช่นนี้:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(แหล่งที่มา: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

นี่เป็นคำตอบที่มั่นใจและละเอียดมาก แต่โชคร้ายที่มันไม่ถูกต้อง แม้จะมีการค้นคว้าเพียงเล็กน้อย ก็จะพบว่ามีผู้รอดชีวิตมากกว่าหนึ่งคนจากเหตุการณ์เรือ Titanic สำหรับนักเรียนที่เพิ่งเริ่มต้นค้นคว้าหัวข้อนี้ คำตอบนี้อาจโน้มน้าวใจพอที่จะไม่ถูกตั้งคำถามและถูกมองว่าเป็นข้อเท็จจริง ผลกระทบของสิ่งนี้อาจทำให้ระบบ AI ไม่น่าเชื่อถือและส่งผลเสียต่อชื่อเสียงของสตาร์ทอัพของเรา

ด้วยการปรับปรุงในแต่ละรุ่นของ LLM เราได้เห็นการพัฒนาประสิทธิภาพในการลด Hallucinations แม้จะมีการปรับปรุงนี้ เราในฐานะผู้สร้างแอปพลิเคชันและผู้ใช้ยังคงต้องตระหนักถึงข้อจำกัดเหล่านี้

### เนื้อหาที่เป็นอันตราย

เราได้กล่าวถึงในส่วนก่อนหน้านี้เมื่อ LLM สร้างคำตอบที่ไม่ถูกต้องหรือไม่มีความหมาย อีกหนึ่งความเสี่ยงที่เราต้องตระหนักคือเมื่อโมเดลตอบกลับด้วยเนื้อหาที่เป็นอันตราย

เนื้อหาที่เป็นอันตรายสามารถนิยามได้ว่า:

- ให้คำแนะนำหรือสนับสนุนการทำร้ายตัวเองหรือกลุ่มคนบางกลุ่ม
- เนื้อหาที่แสดงความเกลียดชังหรือดูถูก
- แนะนำการวางแผนการโจมตีหรือการกระทำที่รุนแรง
- ให้คำแนะนำเกี่ยวกับวิธีการค้นหาเนื้อหาที่ผิดกฎหมายหรือกระทำการผิดกฎหมาย
- แสดงเนื้อหาที่มีลักษณะทางเพศอย่างชัดเจน

สำหรับสตาร์ทอัพของเรา เราต้องการให้แน่ใจว่าเรามีเครื่องมือและกลยุทธ์ที่เหมาะสมเพื่อป้องกันไม่ให้เนื้อหาประเภทนี้ถูกมองเห็นโดยนักเรียน

### การขาดความยุติธรรม

ความยุติธรรมถูกนิยามว่า “การทำให้ระบบ AI ปราศจากอคติและการเลือกปฏิบัติ และทำให้มันปฏิบัติต่อทุกคนอย่างยุติธรรมและเท่าเทียมกัน” ในโลกของ Generative AI เราต้องการให้แน่ใจว่ามุมมองที่กีดกันของกลุ่มที่ถูกกดขี่ไม่ได้ถูกเสริมสร้างโดยผลลัพธ์ของโมเดล

ผลลัพธ์ประเภทนี้ไม่เพียงแต่ทำลายประสบการณ์การใช้งานผลิตภัณฑ์ที่ดีสำหรับผู้ใช้ของเรา แต่ยังก่อให้เกิดอันตรายต่อสังคมในวงกว้าง ในฐานะผู้สร้างแอปพลิเคชัน เราควรคำนึงถึงฐานผู้ใช้ที่หลากหลายและกว้างขวางเมื่อสร้างโซลูชันด้วย Generative AI

## วิธีการใช้ Generative AI อย่างรับผิดชอบ

ตอนนี้เราได้ระบุความสำคัญของ Responsible Generative AI แล้ว ลองมาดู 4 ขั้นตอนที่เราสามารถทำได้เพื่อสร้างโซลูชัน AI ของเราอย่างรับผิดชอบ:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.th.png)

### วัดผลกระทบที่อาจเกิดขึ้น

ในการทดสอบซอฟต์แวร์ เราทดสอบการกระทำที่คาดหวังของผู้ใช้บนแอปพลิเคชัน ในทำนองเดียวกัน การทดสอบชุดคำถามที่หลากหลายที่ผู้ใช้อาจใช้เป็นวิธีที่ดีในการวัดผลกระทบที่อาจเกิดขึ้น

เนื่องจากสตาร์ทอัพของเรากำลังสร้างผลิตภัณฑ์การศึกษา การเตรียมรายการคำถามที่เกี่ยวข้องกับการศึกษาจะเป็นประโยชน์ สิ่งนี้อาจครอบคลุมหัวข้อบางอย่าง ข้อเท็จจริงทางประวัติศาสตร์ และคำถามเกี่ยวกับชีวิตนักเรียน

### ลดผลกระทบที่อาจเกิดขึ้น

ถึงเวลาหาวิธีที่เราสามารถป้องกันหรือจำกัดผลกระทบที่อาจเกิดขึ้นจากโมเดลและคำตอบของมัน เราสามารถมองสิ่งนี้ใน 4 ชั้น:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.th.png)

- **โมเดล**. เลือกโมเดลที่เหมาะสมกับกรณีการใช้งานที่เหมาะสม โมเดลที่ใหญ่และซับซ้อนกว่า เช่น GPT-4 อาจก่อให้เกิดความเสี่ยงของเนื้อหาที่เป็นอันตรายมากขึ้นเมื่อใช้กับกรณีการใช้งานที่เล็กและเฉพาะเจาะจง การใช้ข้อมูลการฝึกอบรมของคุณเพื่อปรับแต่งโมเดลยังช่วยลดความเสี่ยงของเนื้อหาที่เป็นอันตราย

- **ระบบความปลอดภัย**. ระบบความปลอดภัยคือชุดเครื่องมือและการกำหนดค่าบนแพลตฟอร์มที่ให้บริการโมเดลที่ช่วยลดผลกระทบ ตัวอย่างเช่น ระบบกรองเนื้อหาบน Azure OpenAI service ระบบควรตรวจจับการโจมตีแบบ jailbreak และกิจกรรมที่ไม่พึงประสงค์ เช่น คำขอจากบอท

- **Metaprompt**. Metaprompt และ grounding เป็นวิธีที่เราสามารถกำหนดหรือจำกัดโมเดลตามพฤติกรรมและข้อมูลบางอย่าง สิ่งนี้อาจเป็นการใช้ข้อมูลระบบเพื่อกำหนดขอบเขตของโมเดล นอกจากนี้ยังสามารถใช้เทคนิคเช่น Retrieval Augmented Generation (RAG) เพื่อให้โมเดลดึงข้อมูลจากแหล่งข้อมูลที่เชื่อถือได้เท่านั้น มีบทเรียนในหลักสูตรนี้เกี่ยวกับ [การสร้างแอปพลิเคชันการค้นหา](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **ประสบการณ์ผู้ใช้**. ชั้นสุดท้ายคือที่ที่ผู้ใช้โต้ตอบโดยตรงกับโมเดลผ่านอินเทอร์เฟซของแอปพลิเคชันของเรา ในวิธีนี้เราสามารถออกแบบ UI/UX เพื่อจำกัดผู้ใช้ในประเภทของข้อมูลที่พวกเขาสามารถส่งไปยังโมเดล รวมถึงข้อความหรือภาพที่แสดงให้ผู้ใช้เห็น เมื่อเปิดตัวแอปพลิเคชัน AI เราต้องโปร่งใสเกี่ยวกับสิ่งที่แอปพลิเคชัน Generative AI ของเราสามารถทำได้และไม่สามารถทำได้

เรามีบทเรียนทั้งหมดที่อุทิศให้กับ [การออกแบบ UX สำหรับแอปพลิเคชัน AI](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **ประเมินโมเดล**. การทำงานกับ LLM อาจเป็นเรื่องท้าทายเพราะเราไม่ได้ควบคุมข้อมูลที่โมเดลถูกฝึกมาเสมอไป อย่างไรก็ตาม เราควรประเมินประสิทธิภาพและผลลัพธ์ของโมเดลเสมอ การวัดความถูกต้อง ความคล้ายคลึง ความเชื่อมโยง และความเกี่ยวข้องของผลลัพธ์ยังคงเป็นสิ่งสำคัญ สิ่งนี้ช่วยให้เกิดความโปร่งใสและความไว้วางใจต่อผู้มีส่วนได้ส่วนเสียและผู้ใช้

### การดำเนินการโซลูชัน Generative AI อย่างรับผิดชอบ

การสร้างแนวปฏิบัติการดำเนินงานรอบแอปพลิเคชัน AI ของคุณเป็นขั้นตอนสุดท้าย ซึ่งรวมถึงการร่วมมือกับส่วนอื่น ๆ ของสตาร์ทอัพของเรา เช่น ฝ่ายกฎหมายและความปลอดภัย เพื่อให้แน่ใจว่าเราปฏิบัติตามนโยบายข้อบังคับทั้งหมด ก่อนเปิดตัว เราควรสร้างแผนเกี่ยวกับการส่งมอบ การจัดการเหตุการณ์ และการย้อนกลับเพื่อป้องกันไม่ให้เกิดอันตรายต่อผู้ใช้ของเรา

## เครื่องมือ

แม้ว่างานพัฒนาโซลูชัน Responsible AI อาจดูเหมือนเป็นงานที่มาก แต่ก็เป็นงานที่คุ้มค่ากับความพยายาม เมื่อพื้นที่ของ Generative AI เติบโตขึ้น เครื่องมือที่ช่วยให้นักพัฒนารวมความรับผิดชอบเข้ากับกระบวนการทำงานของพวกเขาอย่างมีประสิทธิภาพจะพัฒนาไปด้วย ตัวอย่างเช่น [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) สามารถช่วยตรวจจับเนื้อหาและภาพที่เป็นอันตรายผ่านการร้องขอ API

## ตรวจสอบความรู้

สิ่งที่คุณต้องใส่ใจเพื่อให้มั่นใจว่าการใช้งาน AI อย่างรับผิดชอบมีอะไรบ้าง?

1. คำตอบต้องถูกต้อง
1. การใช้งานที่เป็นอันตราย เช่น AI ไม่ถูกใช้ในทางอาชญากรรม
1. การทำให้ AI ปราศจากอคติและการเลือกปฏิบัติ

A: ข้อ 2 และ 3 ถูกต้อง Responsible AI ช่วยให้คุณพิจารณาวิธีการลดผลกระทบที่เป็นอันตรายและอคติ รวมถึงอื่น ๆ

## 🚀 ความท้าทาย

ศึกษาข้อมูลเกี่ยวกับ [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) และดูว่าคุณสามารถนำไปใช้ประโยชน์ได้อย่างไร

## ทำได้ดีมาก! เรียนรู้เพิ่มเติมกันต่อ

หลังจากจบบทเรียนนี้ ลองดู [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ของเราเพื่อเพิ่มพูนความรู้เกี่ยวกับ Generative AI ของคุณ!

ไปที่บทเรียนที่ 4 ซึ่งเราจะศึกษาพื้นฐานของ [Prompt Engineering](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้