<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T18:35:28+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "th"
}
-->
# การรักษาความปลอดภัยให้กับแอปพลิเคชัน AI เชิงสร้างสรรค์ของคุณ

[![การรักษาความปลอดภัยให้กับแอปพลิเคชัน AI เชิงสร้างสรรค์ของคุณ](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.th.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## บทนำ

บทเรียนนี้จะครอบคลุมถึง:

- ความปลอดภัยในบริบทของระบบ AI
- ความเสี่ยงและภัยคุกคามที่พบบ่อยต่อระบบ AI
- วิธีการและข้อควรพิจารณาในการรักษาความปลอดภัยของระบบ AI

## เป้าหมายการเรียนรู้

หลังจากจบบทเรียนนี้ คุณจะมีความเข้าใจเกี่ยวกับ:

- ภัยคุกคามและความเสี่ยงต่อระบบ AI
- วิธีการและแนวปฏิบัติทั่วไปในการรักษาความปลอดภัยของระบบ AI
- วิธีการทดสอบความปลอดภัยที่สามารถป้องกันผลลัพธ์ที่ไม่คาดคิดและการสูญเสียความไว้วางใจจากผู้ใช้

## ความหมายของความปลอดภัยในบริบทของ AI เชิงสร้างสรรค์คืออะไร?

เมื่อเทคโนโลยีปัญญาประดิษฐ์ (AI) และการเรียนรู้ของเครื่อง (ML) มีบทบาทสำคัญในชีวิตของเรามากขึ้น การปกป้องข้อมูลลูกค้าและระบบ AI เองจึงเป็นสิ่งสำคัญ AI/ML ถูกนำมาใช้มากขึ้นในการสนับสนุนกระบวนการตัดสินใจที่มีมูลค่าสูงในอุตสาหกรรมที่การตัดสินใจผิดพลาดอาจส่งผลกระทบอย่างร้ายแรง

นี่คือประเด็นสำคัญที่ควรพิจารณา:

- **ผลกระทบของ AI/ML**: AI/ML มีผลกระทบอย่างมากต่อชีวิตประจำวัน ดังนั้นการปกป้องระบบเหล่านี้จึงกลายเป็นสิ่งจำเป็น
- **ความท้าทายด้านความปลอดภัย**: ผลกระทบที่ AI/ML มีต่อชีวิตประจำวันต้องได้รับการดูแลอย่างเหมาะสมเพื่อป้องกันผลิตภัณฑ์ที่ใช้ AI จากการโจมตีที่ซับซ้อน ไม่ว่าจะเป็นโดยกลุ่มผู้ก่อกวนหรือกลุ่มที่มีการจัดตั้ง
- **ปัญหาทางยุทธศาสตร์**: อุตสาหกรรมเทคโนโลยีต้องแก้ไขปัญหาทางยุทธศาสตร์อย่างเชิงรุกเพื่อให้มั่นใจในความปลอดภัยของลูกค้าและข้อมูลในระยะยาว

นอกจากนี้ โมเดลการเรียนรู้ของเครื่องส่วนใหญ่ไม่สามารถแยกแยะระหว่างข้อมูลที่เป็นอันตรายและข้อมูลที่ผิดปกติแต่ไม่เป็นอันตรายได้ แหล่งข้อมูลการฝึกอบรมที่สำคัญมักมาจากชุดข้อมูลสาธารณะที่ไม่ได้รับการคัดกรองและตรวจสอบ ซึ่งเปิดโอกาสให้บุคคลที่สามสามารถมีส่วนร่วมได้ ผู้โจมตีไม่จำเป็นต้องเจาะชุดข้อมูลเมื่อพวกเขาสามารถเพิ่มข้อมูลลงไปได้ เมื่อเวลาผ่านไป ข้อมูลที่เป็นอันตรายที่มีความมั่นใจต่ำอาจกลายเป็นข้อมูลที่ได้รับความไว้วางใจสูง หากโครงสร้างหรือรูปแบบของข้อมูลยังคงถูกต้อง

นี่คือเหตุผลที่การรักษาความสมบูรณ์และการปกป้องแหล่งข้อมูลที่โมเดลของคุณใช้ในการตัดสินใจเป็นสิ่งสำคัญอย่างยิ่ง

## การทำความเข้าใจภัยคุกคามและความเสี่ยงของ AI

ในแง่ของ AI และระบบที่เกี่ยวข้อง การปนเปื้อนข้อมูลถือเป็นภัยคุกคามด้านความปลอดภัยที่สำคัญที่สุดในปัจจุบัน การปนเปื้อนข้อมูลเกิดขึ้นเมื่อมีผู้ตั้งใจเปลี่ยนแปลงข้อมูลที่ใช้ในการฝึกอบรม AI ทำให้ AI เกิดข้อผิดพลาด นี่เป็นผลมาจากการขาดวิธีการตรวจจับและลดผลกระทบที่ได้มาตรฐาน รวมถึงการพึ่งพาชุดข้อมูลสาธารณะที่ไม่ได้รับการตรวจสอบหรือคัดกรองสำหรับการฝึกอบรม เพื่อรักษาความสมบูรณ์ของข้อมูลและป้องกันกระบวนการฝึกอบรมที่มีข้อบกพร่อง การติดตามแหล่งที่มาและสายสัมพันธ์ของข้อมูลจึงเป็นสิ่งสำคัญ มิฉะนั้นคำกล่าวที่ว่า "ข้อมูลขยะเข้า ผลลัพธ์ขยะออก" จะเป็นจริง ส่งผลให้ประสิทธิภาพของโมเดลลดลง

ตัวอย่างของการปนเปื้อนข้อมูลที่อาจส่งผลต่อโมเดลของคุณ:

1. **การพลิกกลับป้ายกำกับ**: ในงานการจำแนกประเภทแบบไบนารี ผู้ไม่หวังดีตั้งใจพลิกกลับป้ายกำกับของข้อมูลฝึกอบรมบางส่วน เช่น ตัวอย่างที่ไม่เป็นอันตรายถูกระบุว่าเป็นอันตราย ทำให้โมเดลเรียนรู้ความสัมพันธ์ที่ไม่ถูกต้อง\
   **ตัวอย่าง**: ตัวกรองสแปมที่ระบุอีเมลที่ถูกต้องว่าเป็นสแปมเนื่องจากป้ายกำกับที่ถูกปรับเปลี่ยน
2. **การปนเปื้อนคุณลักษณะ**: ผู้โจมตีปรับเปลี่ยนคุณลักษณะในข้อมูลฝึกอบรมอย่างละเอียดเพื่อแนะนำอคติหรือทำให้โมเดลเข้าใจผิด\
   **ตัวอย่าง**: การเพิ่มคำสำคัญที่ไม่เกี่ยวข้องในคำอธิบายผลิตภัณฑ์เพื่อจัดการระบบแนะนำสินค้า
3. **การฉีดข้อมูล**: การฉีดข้อมูลที่เป็นอันตรายลงในชุดข้อมูลฝึกอบรมเพื่อมีอิทธิพลต่อพฤติกรรมของโมเดล\
   **ตัวอย่าง**: การเพิ่มรีวิวผู้ใช้ปลอมเพื่อบิดเบือนผลลัพธ์การวิเคราะห์ความรู้สึก
4. **การโจมตีแบบฝังหลัง**: ผู้ไม่หวังดีแทรกลวดลายที่ซ่อนอยู่ (ฝังหลัง) ลงในข้อมูลฝึกอบรม โมเดลเรียนรู้ที่จะจดจำลวดลายนี้และแสดงพฤติกรรมที่เป็นอันตรายเมื่อถูกกระตุ้น\
   **ตัวอย่าง**: ระบบจดจำใบหน้าที่ฝึกด้วยภาพที่มีลวดลายฝังหลังซึ่งระบุบุคคลผิดพลาด

องค์กร MITRE Corporation ได้สร้าง [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) ซึ่งเป็นฐานข้อมูลความรู้เกี่ยวกับยุทธวิธีและเทคนิคที่ผู้ไม่หวังดีใช้ในการโจมตีระบบ AI ในโลกจริง

> มีช่องโหว่ในระบบที่ใช้ AI เพิ่มขึ้นเรื่อย ๆ เนื่องจากการรวม AI เข้ากับระบบที่มีอยู่ทำให้พื้นที่การโจมตีขยายตัวเกินกว่าการโจมตีทางไซเบอร์แบบดั้งเดิม เราได้พัฒนา ATLAS เพื่อสร้างความตระหนักถึงช่องโหว่ที่เป็นเอกลักษณ์และพัฒนาอย่างต่อเนื่องเหล่านี้ เนื่องจากชุมชนทั่วโลกได้รวม AI เข้ากับระบบต่าง ๆ มากขึ้นเรื่อย ๆ ATLAS ถูกสร้างแบบจำลองตามกรอบ MITRE ATT&CK® และยุทธวิธี เทคนิค และกระบวนการ (TTPs) ของมันเป็นส่วนเสริมของ ATT&CK

เช่นเดียวกับกรอบ MITRE ATT&CK® ซึ่งถูกใช้กันอย่างแพร่หลายในด้านความปลอดภัยไซเบอร์แบบดั้งเดิมสำหรับการวางแผนสถานการณ์จำลองการเลียนแบบภัยคุกคามขั้นสูง ATLAS ให้ชุด TTPs ที่สามารถค้นหาได้ง่ายเพื่อช่วยให้เข้าใจและเตรียมพร้อมสำหรับการป้องกันการโจมตีที่เกิดขึ้นใหม่

นอกจากนี้ โครงการ Open Web Application Security Project (OWASP) ได้สร้าง "[รายการ 10 อันดับแรก](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" ของช่องโหว่ที่สำคัญที่สุดที่พบในแอปพลิเคชันที่ใช้ LLMs รายการนี้เน้นถึงความเสี่ยงของภัยคุกคาม เช่น การปนเปื้อนข้อมูลที่กล่าวถึงข้างต้น รวมถึงภัยคุกคามอื่น ๆ เช่น:

- **การฉีดคำสั่ง**: เทคนิคที่ผู้โจมตีจัดการโมเดลภาษาขนาดใหญ่ (LLM) ผ่านการป้อนข้อมูลที่ถูกสร้างขึ้นอย่างรอบคอบ ทำให้มันแสดงพฤติกรรมที่อยู่นอกเหนือพฤติกรรมที่ตั้งใจไว้
- **ช่องโหว่ในห่วงโซ่อุปทาน**: ส่วนประกอบและซอฟต์แวร์ที่ประกอบขึ้นเป็นแอปพลิเคชันที่ใช้ LLM เช่น โมดูล Python หรือชุดข้อมูลภายนอก อาจถูกโจมตีได้เอง ส่งผลให้เกิดผลลัพธ์ที่ไม่คาดคิด อคติที่ถูกแนะนำ และแม้กระทั่งช่องโหว่ในโครงสร้างพื้นฐานพื้นฐาน
- **การพึ่งพามากเกินไป**: LLMs มีข้อผิดพลาดและมีแนวโน้มที่จะสร้างข้อมูลที่ไม่ถูกต้องหรือไม่ปลอดภัย ในหลายกรณีที่มีการบันทึกไว้ ผู้คนมักเชื่อผลลัพธ์โดยไม่ตรวจสอบ นำไปสู่ผลกระทบเชิงลบในโลกจริงที่ไม่ได้ตั้งใจ

Rod Trent ผู้สนับสนุนระบบคลาวด์ของ Microsoft ได้เขียนหนังสืออิเล็กทรอนิกส์ฟรี [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) ซึ่งเจาะลึกถึงภัยคุกคาม AI ที่เกิดขึ้นใหม่เหล่านี้และให้คำแนะนำอย่างละเอียดเกี่ยวกับวิธีการจัดการสถานการณ์เหล่านี้อย่างดีที่สุด

## การทดสอบความปลอดภัยสำหรับระบบ AI และ LLMs

ปัญญาประดิษฐ์ (AI) กำลังเปลี่ยนแปลงหลาย ๆ ด้านและอุตสาหกรรม โดยนำเสนอความเป็นไปได้และประโยชน์ใหม่ ๆ ให้กับสังคม อย่างไรก็ตาม AI ก็สร้างความท้าทายและความเสี่ยงที่สำคัญ เช่น ความเป็นส่วนตัวของข้อมูล อคติ การขาดความสามารถในการอธิบาย และการใช้งานในทางที่ผิด ดังนั้นจึงเป็นสิ่งสำคัญที่จะต้องมั่นใจว่าระบบ AI มีความปลอดภัยและมีความรับผิดชอบ หมายถึงการปฏิบัติตามมาตรฐานทางจริยธรรมและกฎหมาย และสามารถได้รับความไว้วางใจจากผู้ใช้และผู้มีส่วนได้ส่วนเสีย

การทดสอบความปลอดภัยคือกระบวนการประเมินความปลอดภัยของระบบ AI หรือ LLM โดยการระบุและใช้ประโยชน์จากช่องโหว่ของมัน การทดสอบนี้สามารถดำเนินการโดยนักพัฒนา ผู้ใช้ หรือผู้ตรวจสอบบุคคลที่สาม ขึ้นอยู่กับวัตถุประสงค์และขอบเขตของการทดสอบ วิธีการทดสอบความปลอดภัยที่พบบ่อยที่สุดสำหรับระบบ AI และ LLMs ได้แก่:

- **การทำความสะอาดข้อมูล**: กระบวนการลบหรือทำให้ข้อมูลที่ละเอียดอ่อนหรือส่วนตัวในชุดข้อมูลฝึกอบรมหรือข้อมูลที่ป้อนเข้าสู่ระบบ AI หรือ LLM เป็นนิรนาม การทำความสะอาดข้อมูลสามารถช่วยป้องกันการรั่วไหลของข้อมูลและการจัดการที่เป็นอันตรายโดยลดการเปิดเผยข้อมูลที่เป็นความลับหรือข้อมูลส่วนบุคคล
- **การทดสอบเชิงรุก**: กระบวนการสร้างและใช้ตัวอย่างเชิงรุกกับข้อมูลที่ป้อนหรือผลลัพธ์ของระบบ AI หรือ LLM เพื่อประเมินความแข็งแกร่งและความยืดหยุ่นต่อการโจมตีเชิงรุก การทดสอบเชิงรุกสามารถช่วยระบุและลดช่องโหว่และจุดอ่อนของระบบ AI หรือ LLM ที่อาจถูกโจมตีโดยผู้ไม่หวังดี
- **การตรวจสอบโมเดล**: กระบวนการตรวจสอบความถูกต้องและความสมบูรณ์ของพารามิเตอร์หรือสถาปัตยกรรมของโมเดลในระบบ AI หรือ LLM การตรวจสอบโมเดลสามารถช่วยตรวจจับและป้องกันการขโมยโมเดลโดยการรับรองว่าโมเดลได้รับการปกป้องและรับรองความถูกต้อง
- **การตรวจสอบผลลัพธ์**: กระบวนการตรวจสอบคุณภาพและความน่าเชื่อถือของผลลัพธ์ของระบบ AI หรือ LLM การตรวจสอบผลลัพธ์สามารถช่วยตรวจจับและแก้ไขการจัดการที่เป็นอันตรายโดยการรับรองว่าผลลัพธ์มีความสม่ำเสมอและถูกต้อง

OpenAI ซึ่งเป็นผู้นำในระบบ AI ได้จัดตั้งชุดการประเมินความปลอดภัยเป็นส่วนหนึ่งของโครงการเครือข่าย red teaming โดยมีเป้าหมายเพื่อทดสอบผลลัพธ์ของระบบ AI เพื่อมีส่วนร่วมในการรักษาความปลอดภัยของ AI

> การประเมินสามารถมีตั้งแต่การทดสอบ Q&A แบบง่ายไปจนถึงการจำลองที่ซับซ้อน ตัวอย่างที่เป็นรูปธรรมคือการประเมินตัวอย่างที่พัฒนาโดย OpenAI เพื่อประเมินพฤติกรรมของ AI จากหลายมุมมอง:

#### การโน้มน้าวใจ

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI สามารถหลอก AI อีกตัวให้พูดคำลับได้ดีแค่ไหน?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI สามารถโน้มน้าว AI อีกตัวให้บริจาคเงินได้ดีแค่ไหน?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI สามารถมีอิทธิพลต่อการสนับสนุนข้อเสนอทางการเมืองของ AI อีกตัวได้ดีแค่ไหน?

#### สเตกาโนกราฟี (การส่งข้อความที่ซ่อนอยู่)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI สามารถส่งข้อความลับโดยไม่ถูกจับได้ดีแค่ไหน?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI สามารถบีบอัดและคลายข้อความเพื่อซ่อนข้อความลับได้ดีแค่ไหน?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI สามารถประสานงานกับ AI อีกตัวโดยไม่ต้องสื่อสารโดยตรงได้ดีแค่ไหน?

### ความปลอดภัยของ AI

สิ่งสำคัญคือเราต้องมุ่งมั่นที่จะปกป้องระบบ AI จากการโจมตีที่เป็นอันตราย การใช้งานในทางที่ผิด หรือผลลัพธ์ที่ไม่ตั้งใจ ซึ่งรวมถึงการดำเนินการเพื่อให้มั่นใจในความปลอดภัย ความน่าเชื่อถือ และความไว้วางใจในระบบ AI เช่น:

- การรักษาความปลอดภัยของข้อมูลและอัลกอริทึมที่ใช้ในการฝึกอบรมและใช้งานโมเดล AI
- การป้องกันการเข้าถึง การจัดการ หรือการก่อวินาศกรรมระบบ AI โดยไม่ได้รับอนุญาต
- การตรวจจับและลดอคติ การเลือกปฏิบัติ หรือปัญหาด้านจริยธรรมในระบบ AI
- การรับรองความรับผิดชอบ ความโปร่งใส และความสามารถในการอธิบายของการตัดสินใจและการกระทำของ AI
- การปรับเป้าหมายและค่านิยมของระบบ AI ให้สอดคล้องกับมนุษย์และสังคม

ความปลอดภัยของ AI มีความสำคัญต่อการรักษาความสมบูรณ์ ความพร้อมใช้งาน และความลับของระบบ AI และข้อมูล ความท้าทายและโอกาสของความปลอดภัยของ AI ได้แก่:

- **โอกาส**: การรวม AI เข้ากับกลยุทธ์ความปลอดภัยไซเบอร์ เนื่องจาก AI สามารถมีบทบาทสำคัญในการระบุภัยคุกคามและปรับปรุงเวลาการตอบสนอง AI สามารถช่วยทำให้การตรวจจับและลดการโจมตีทางไซเบอร์ เช่น ฟิชชิง มัลแวร์ หรือแรนซัมแวร์ เป็นไปโดยอัตโนมัติและมีประสิทธิภาพมากขึ้น
- **ความท้าทาย**: AI ยังสามารถถูกใช้โดยผู้ไม่หวังดีในการเปิดตัวการโจมตีที่ซับซ้อน เช่น การสร้างเนื้อหาปลอมหรือทำให้เข้าใจผิด การแอบอ้างเป็นผู้ใช้ หรือการใช้ช่องโหว่ในระบบ AI ดังนั้นนักพัฒนา AI จึงมีความรับผิดชอบพิเศษในการออกแบบระบบที่แข็งแกร่งและยืดหยุ่นต่อการใช้งานในทางที่ผิด

### การปกป้องข้อมูล

LLMs อาจก่อให้เกิดความเสี่ยงต่อความเป็นส่วนตัวและความปลอดภัยของข้อมูลที่พวกเขาใช้ ตัวอย่างเช่น LLMs อาจจดจำและเปิดเผยข้อมูลที่ละเอียดอ่อนจาก
การจำลองภัยคุกคามในโลกจริงถือเป็นแนวปฏิบัติมาตรฐานในการสร้างระบบ AI ที่มีความยืดหยุ่น โดยใช้เครื่องมือ กลยุทธ์ และกระบวนการที่คล้ายคลึงกันเพื่อระบุความเสี่ยงต่อระบบและทดสอบการตอบสนองของผู้ป้องกัน

> การปฏิบัติของ AI red teaming ได้พัฒนาไปสู่ความหมายที่กว้างขึ้น: ไม่เพียงแต่ครอบคลุมการตรวจสอบช่องโหว่ด้านความปลอดภัย แต่ยังรวมถึงการตรวจสอบความล้มเหลวอื่น ๆ ของระบบ เช่น การสร้างเนื้อหาที่อาจเป็นอันตราย ระบบ AI มาพร้อมกับความเสี่ยงใหม่ ๆ และ red teaming เป็นหัวใจสำคัญในการทำความเข้าใจความเสี่ยงใหม่เหล่านั้น เช่น การฉีดคำสั่ง (prompt injection) และการสร้างเนื้อหาที่ไม่มีพื้นฐาน - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![คำแนะนำและทรัพยากรสำหรับ red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.th.png)]()

ด้านล่างนี้คือข้อมูลสำคัญที่ได้หล่อหลอมโปรแกรม AI Red Team ของ Microsoft

1. **ขอบเขตที่กว้างขวางของ AI Red Teaming:**
   AI red teaming ในปัจจุบันครอบคลุมทั้งผลลัพธ์ด้านความปลอดภัยและ AI ที่มีความรับผิดชอบ (RAI) โดยปกติแล้ว red teaming จะมุ่งเน้นไปที่ด้านความปลอดภัย โดยมองโมเดลเป็นเวกเตอร์ (เช่น การขโมยโมเดลพื้นฐาน) อย่างไรก็ตาม ระบบ AI นำเสนอช่องโหว่ด้านความปลอดภัยใหม่ ๆ (เช่น การฉีดคำสั่ง การปนเปื้อนข้อมูล) ซึ่งจำเป็นต้องให้ความสนใจเป็นพิเศษ นอกเหนือจากความปลอดภัยแล้ว AI red teaming ยังตรวจสอบปัญหาด้านความเป็นธรรม (เช่น การเหมารวม) และเนื้อหาที่เป็นอันตราย (เช่น การยกย่องความรุนแรง) การระบุปัญหาเหล่านี้ตั้งแต่เนิ่น ๆ ช่วยให้สามารถจัดลำดับความสำคัญของการลงทุนในการป้องกันได้
2. **ความล้มเหลวทั้งที่เป็นอันตรายและไม่เป็นอันตราย:**
   AI red teaming พิจารณาความล้มเหลวทั้งจากมุมมองที่เป็นอันตรายและไม่เป็นอันตราย ตัวอย่างเช่น เมื่อ red teaming Bing ใหม่ เราไม่เพียงแต่สำรวจว่าผู้ไม่หวังดีสามารถบิดเบือนระบบได้อย่างไร แต่ยังพิจารณาว่าผู้ใช้ทั่วไปอาจพบเนื้อหาที่มีปัญหาหรือเป็นอันตรายได้อย่างไร แตกต่างจาก red teaming ด้านความปลอดภัยแบบดั้งเดิมที่มุ่งเน้นไปที่ผู้ไม่หวังดีเป็นหลัก AI red teaming คำนึงถึงบุคคลและความล้มเหลวที่อาจเกิดขึ้นในวงกว้างมากขึ้น
3. **ธรรมชาติที่เปลี่ยนแปลงได้ของระบบ AI:**
   แอปพลิเคชัน AI มีการพัฒนาอย่างต่อเนื่อง ในแอปพลิเคชันโมเดลภาษาขนาดใหญ่ นักพัฒนาปรับตัวให้เข้ากับความต้องการที่เปลี่ยนแปลง การทำ red teaming อย่างต่อเนื่องช่วยให้มั่นใจได้ถึงการเฝ้าระวังและการปรับตัวต่อความเสี่ยงที่เปลี่ยนแปลงไป

AI red teaming ไม่ได้ครอบคลุมทุกด้านและควรถือเป็นการเคลื่อนไหวเสริมต่อการควบคุมเพิ่มเติม เช่น [การควบคุมการเข้าถึงตามบทบาท (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) และโซลูชันการจัดการข้อมูลที่ครอบคลุม มันถูกออกแบบมาเพื่อเสริมกลยุทธ์ด้านความปลอดภัยที่มุ่งเน้นการใช้โซลูชัน AI ที่ปลอดภัยและมีความรับผิดชอบ โดยคำนึงถึงความเป็นส่วนตัวและความปลอดภัย พร้อมทั้งมุ่งลดอคติ เนื้อหาที่เป็นอันตราย และข้อมูลที่ผิดพลาดซึ่งอาจทำลายความเชื่อมั่นของผู้ใช้

นี่คือรายการการอ่านเพิ่มเติมที่สามารถช่วยให้คุณเข้าใจว่า red teaming สามารถช่วยระบุและลดความเสี่ยงในระบบ AI ของคุณได้อย่างไร:

- [การวางแผน red teaming สำหรับโมเดลภาษาขนาดใหญ่ (LLMs) และแอปพลิเคชันของพวกมัน](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [เครือข่าย OpenAI Red Teaming คืออะไร?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - แนวปฏิบัติสำคัญในการสร้างโซลูชัน AI ที่ปลอดภัยและมีความรับผิดชอบมากขึ้น](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) ฐานความรู้เกี่ยวกับกลยุทธ์และเทคนิคที่ผู้ไม่หวังดีใช้ในการโจมตีระบบ AI ในโลกจริง

## ทดสอบความรู้

วิธีการที่ดีในการรักษาความสมบูรณ์ของข้อมูลและป้องกันการใช้งานในทางที่ผิดคืออะไร?

1. มีการควบคุมการเข้าถึงข้อมูลและการจัดการข้อมูลที่แข็งแกร่งตามบทบาท
1. ดำเนินการและตรวจสอบการติดป้ายข้อมูลเพื่อป้องกันการแสดงข้อมูลผิดหรือการใช้งานในทางที่ผิด
1. ตรวจสอบให้แน่ใจว่าโครงสร้างพื้นฐาน AI ของคุณรองรับการกรองเนื้อหา

A:1, แม้ว่าทั้งสามข้อจะเป็นคำแนะนำที่ดี การตรวจสอบให้แน่ใจว่าคุณกำหนดสิทธิ์การเข้าถึงข้อมูลที่เหมาะสมแก่ผู้ใช้จะช่วยป้องกันการบิดเบือนและการแสดงข้อมูลผิดที่ใช้ใน LLMs ได้อย่างมาก

## 🚀 ความท้าทาย

อ่านเพิ่มเติมเกี่ยวกับวิธีที่คุณสามารถ [ควบคุมและปกป้องข้อมูลที่ละเอียดอ่อน](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) ในยุคของ AI

## ทำได้ดีมาก, เรียนรู้ต่อไป

หลังจากจบบทเรียนนี้ ลองดู [คอลเลกชันการเรียนรู้ Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) เพื่อเพิ่มพูนความรู้เกี่ยวกับ Generative AI ของคุณ!

ไปที่บทเรียนที่ 14 ซึ่งเราจะดู [วงจรชีวิตของแอปพลิเคชัน Generative AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้