<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T23:55:09+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "ja"
}
-->
[![オープンソースモデル](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.ja.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# LLMのファインチューニング

生成AIアプリケーションを構築するために大規模言語モデルを使用することには、新たな課題が伴います。主な問題は、特定のユーザーリクエストに対してモデルが生成するコンテンツの品質（正確性と関連性）を確保することです。前回のレッスンでは、プロンプトエンジニアリングやリトリーバル強化生成のような技術について議論しました。これらは、既存のモデルへのプロンプト入力を変更することで問題を解決しようとするものです。

今日のレッスンでは、3つ目の技術である**ファインチューニング**について説明します。この技術は、追加データを使用してモデル自体を再訓練することで課題に取り組むものです。それでは詳細を見ていきましょう。

## 学習目標

このレッスンでは、事前学習済み言語モデルのファインチューニングの概念を紹介し、このアプローチの利点と課題を探り、生成AIモデルの性能を向上させるためにファインチューニングをいつ、どのように使用するべきかについての指針を提供します。

このレッスンの終わりまでに、以下の質問に答えられるようになることを目指します：

- 言語モデルのファインチューニングとは何か？
- ファインチューニングはいつ、なぜ有用なのか？
- 事前学習済みモデルをどのようにファインチューニングできるのか？
- ファインチューニングの限界は何か？

準備はいいですか？それでは始めましょう。

## イラスト付きガイド

詳細に入る前に、今回のレッスンで扱う内容の全体像を把握したいですか？ファインチューニングの核心的な概念と動機から、そのプロセスや実行のベストプラクティスまでを説明するイラスト付きガイドをご覧ください。このトピックは非常に興味深いので、自己学習をサポートする追加リンクについては[リソース](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)ページを忘れずにチェックしてください！

![言語モデルのファインチューニングに関するイラスト付きガイド](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.ja.png)

## 言語モデルのファインチューニングとは？

定義によれば、大規模言語モデルはインターネットを含む多様なソースから収集された大量のテキストで事前学習されています。前回のレッスンで学んだように、ユーザーの質問（「プロンプト」）に対するモデルの応答の品質を向上させるためには、_プロンプトエンジニアリング_や_リトリーバル強化生成_のような技術が必要です。

一般的なプロンプトエンジニアリング技術では、モデルに応答で期待される内容についてより具体的な指示を与えるか、いくつかの例を示すことでガイドします。これは_少数ショット学習_と呼ばれますが、以下の2つの制限があります：

- モデルのトークン制限により、提供できる例の数が制限され、効果が制限される。
- モデルのトークンコストにより、すべてのプロンプトに例を追加することが高価になり、柔軟性が制限される。

ファインチューニングは、事前学習済みモデルを新しいデータで再訓練し、特定のタスクでの性能を向上させる機械学習システムで一般的な手法です。言語モデルの文脈では、事前学習済みモデルを_特定のタスクやアプリケーションドメインのためにキュレーションされた例のセットでファインチューニングする_ことで、特定のタスクやドメインに対してより正確で関連性の高い**カスタムモデル**を作成できます。ファインチューニングの副次的な利点として、少数ショット学習に必要な例の数を減らすことができ、トークン使用量や関連コストを削減することが挙げられます。

## モデルをいつ、なぜファインチューニングするべきか？

この文脈では、ファインチューニングについて話す際、**監督型**ファインチューニングを指します。これは、元のトレーニングデータセットに含まれていなかった**新しいデータを追加**して再訓練を行うものです。元のデータで異なるハイパーパラメータを使用してモデルを再訓練する非監督型ファインチューニングアプローチとは異なります。

重要なのは、ファインチューニングは望む結果を得るために一定の専門知識が必要な高度な技術であるということです。誤った方法で行うと、期待される改善が得られないだけでなく、ターゲットドメインにおけるモデルの性能を低下させる可能性もあります。

したがって、言語モデルを「どのように」ファインチューニングするかを学ぶ前に、「なぜ」この方法を選ぶべきか、そして「いつ」ファインチューニングのプロセスを開始するべきかを知る必要があります。以下の質問を自問してみてください：

- **ユースケース**: ファインチューニングの_ユースケース_は何ですか？現在の事前学習済みモデルのどの側面を改善したいですか？
- **代替案**: 望む結果を達成するために_他の技術_を試しましたか？それらを使用して比較のためのベースラインを作成してください。
  - プロンプトエンジニアリング: 関連するプロンプト応答の例を使用した少数ショットプロンプティングの技術を試してください。応答の品質を評価してください。
  - リトリーバル強化生成: データ検索結果をプロンプトに追加する方法を試してください。応答の品質を評価してください。
- **コスト**: ファインチューニングのコストを特定しましたか？
  - 調整可能性 - 事前学習済みモデルはファインチューニング可能ですか？
  - 労力 - トレーニングデータの準備、モデルの評価と改良にかかる労力。
  - コンピュート - ファインチューニングジョブの実行とファインチューニングモデルのデプロイにかかる計算リソース。
  - データ - ファインチューニングの効果を得るための十分な品質の例へのアクセス。
- **利点**: ファインチューニングの利点を確認しましたか？
  - 品質 - ファインチューニングされたモデルはベースラインを上回りましたか？
  - コスト - プロンプトを簡素化することでトークン使用量を削減できますか？
  - 拡張性 - ベースモデルを新しいドメインに再利用できますか？

これらの質問に答えることで、ファインチューニングがあなたのユースケースに適したアプローチかどうかを判断できるはずです。理想的には、利点がコストを上回る場合にのみこのアプローチが有効です。進めることを決めたら、事前学習済みモデルを_どのように_ファインチューニングするかを考える時です。

意思決定プロセスについてさらに詳しく知りたいですか？[ファインチューニングするべきかどうか](https://www.youtube.com/watch?v=0Jo-z-MFxJs)をご覧ください。

## 事前学習済みモデルをどのようにファインチューニングできるか？

事前学習済みモデルをファインチューニングするには、以下が必要です：

- ファインチューニングする事前学習済みモデル
- ファインチューニングに使用するデータセット
- ファインチューニングジョブを実行するトレーニング環境
- ファインチューニングされたモデルをデプロイするホスティング環境

## ファインチューニングの実践

以下のリソースでは、選択されたモデルとキュレーションされたデータセットを使用した実例をステップバイステップで解説しています。これらのチュートリアルを進めるには、特定のプロバイダーのアカウントと、関連するモデルおよびデータセットへのアクセスが必要です。

| プロバイダー | チュートリアル                                                                                                                                                                       | 説明                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [チャットモデルのファインチューニング方法](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | `gpt-35-turbo`を特定のドメイン（「レシピアシスタント」）向けにファインチューニングする方法を学びます。トレーニングデータの準備、ファインチューニングジョブの実行、ファインチューニングされたモデルを推論に使用する方法を解説します。                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turboファインチューニングチュートリアル](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | **Azure上で**`gpt-35-turbo-0613`モデルをファインチューニングする方法を学びます。トレーニングデータの作成とアップロード、ファインチューニングジョブの実行、そして新しいモデルのデプロイと使用方法を解説します。                                                                                                                                                                                                                                                                 |
| Hugging Face | [Hugging FaceでのLLMファインチューニング](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | このブログ記事では、[transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst)ライブラリと[Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst)を使用して、オープンLLM（例：`CodeLlama 7B`）をファインチューニングする方法を解説します。オープン[データセット](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)を使用します。 |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🤗 AutoTrain | [AutoTrainでのLLMファインチューニング](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain（またはAutoTrain Advanced）は、Hugging Faceが開発したPythonライブラリで、LLMのファインチューニングを含む多くの異なるタスクのファインチューニングを可能にします。AutoTrainはコード不要のソリューションであり、ファインチューニングは独自のクラウド、Hugging Face Spaces、またはローカルで行うことができます。WebベースのGUI、CLI、yaml設定ファイルを使用したトレーニングをサポートしています。                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## 課題

上記のチュートリアルのいずれかを選択し、それを進めてください。_これらのチュートリアルのバージョンをこのリポジトリのJupyter Notebookで参考用に再現する場合があります。最新バージョンを取得するために、必ず元のソースを直接使用してください_。

## 素晴らしい仕事です！学習を続けましょう。

このレッスンを完了した後は、[生成AI学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成AIの知識をさらに深めてください！

おめでとうございます！！このコースのv2シリーズの最終レッスンを完了しました！学びと構築を止めないでください。**[リソース](RESOURCES.md?WT.mc_id=academic-105485-koreyst)**ページで、このトピックに関する追加の提案を確認してください。

また、v1シリーズのレッスンも更新され、より多くの課題や概念が追加されています。ぜひ知識をリフレッシュするために時間を取ってください。そして、これらのレッスンをコミュニティのために改善するために、[質問やフィードバックを共有](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)してください。

---

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてご参照ください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は責任を負いません。