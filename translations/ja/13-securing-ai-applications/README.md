<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T23:50:57+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ja"
}
-->
# ジェネレーティブAIアプリケーションのセキュリティ確保

[![ジェネレーティブAIアプリケーションのセキュリティ確保](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.ja.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## はじめに

このレッスンでは以下を学びます：

- AIシステムにおけるセキュリティの重要性
- AIシステムに対する一般的なリスクと脅威
- AIシステムを保護するための方法と考慮事項

## 学習目標

このレッスンを完了すると、以下について理解できるようになります：

- AIシステムに対する脅威とリスク
- AIシステムを保護するための一般的な方法と実践
- セキュリティテストを実施することで予期せぬ結果やユーザーの信頼の低下を防ぐ方法

## ジェネレーティブAIにおけるセキュリティとは？

人工知能（AI）や機械学習（ML）技術が私たちの生活にますます影響を与える中で、顧客データだけでなくAIシステムそのものを保護することが重要です。AI/MLは、誤った決定が重大な結果を招く可能性のある業界で、高価値の意思決定プロセスを支援するためにますます利用されています。

以下は重要なポイントです：

- **AI/MLの影響**: AI/MLは日常生活に大きな影響を与えており、それを保護することが不可欠です。
- **セキュリティの課題**: AI/MLの影響を考慮し、トロールや組織的な攻撃からAIベースの製品を保護する必要があります。
- **戦略的課題**: 技術業界は、長期的な顧客の安全性とデータセキュリティを確保するために戦略的な課題に積極的に取り組む必要があります。

さらに、機械学習モデルは悪意のある入力と無害な異常データを区別することがほとんどできません。トレーニングデータの重要な部分は、未整理で未監視の公開データセットから得られ、第三者が自由に貢献できる状態です。攻撃者はデータセットを侵害する必要がなく、自由にデータを提供できます。時間が経つにつれて、低信頼の悪意あるデータがデータ構造やフォーマットが正しいままであれば、高信頼の信頼できるデータに変わる可能性があります。

これが、モデルが意思決定に使用するデータストアの整合性と保護を確保することが重要である理由です。

## AIの脅威とリスクを理解する

AIおよび関連システムにおいて、データポイズニングは現在最も重要なセキュリティ脅威として際立っています。データポイズニングとは、AIのトレーニングに使用される情報を意図的に変更し、誤った判断をさせることです。これは標準化された検出および緩和方法が欠如していること、そしてトレーニングに未整理または未監視の公開データセットに依存していることが原因です。データの整合性を維持し、欠陥のあるトレーニングプロセスを防ぐためには、データの出所と系統を追跡することが重要です。そうでなければ、「ゴミを入れればゴミが出る」という古い格言が当てはまり、モデルの性能が損なわれます。

以下はデータポイズニングがモデルに与える影響の例です：

1. **ラベル反転**: バイナリ分類タスクで、攻撃者がトレーニングデータの一部のラベルを意図的に反転させます。例えば、無害なサンプルが悪意あるものとしてラベル付けされ、モデルが誤った関連性を学習します。\
   **例**: スパムフィルターが操作されたラベルのために正当なメールをスパムと誤分類する。
2. **特徴量ポイズニング**: 攻撃者がトレーニングデータの特徴量を微妙に変更し、バイアスを導入したりモデルを誤解させたりします。\
   **例**: 推奨システムを操作するために製品説明に無関係なキーワードを追加する。
3. **データ注入**: トレーニングセットに悪意あるデータを注入し、モデルの挙動に影響を与える。\
   **例**: 偽のユーザーレビューを導入して感情分析結果を歪める。
4. **バックドア攻撃**: 攻撃者がトレーニングデータに隠れたパターン（バックドア）を挿入します。モデルはこのパターンを学習し、トリガーされると悪意ある挙動を示します。\
   **例**: 特定の人物を誤認識する顔認識システム。

MITRE Corporationは、AIシステムに対する実際の攻撃で使用される戦術や技術の知識ベースである[ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)を作成しました。

> AIを組み込むことで既存システムの攻撃面が拡大し、従来のサイバー攻撃を超えた脆弱性が増加しています。ATLASはこれらの独自で進化する脆弱性に対する認識を高めるために開発されました。ATLASはMITRE ATT&CK®フレームワークをモデルにしており、その戦術、技術、手順（TTP）はATT&CKのものを補完するものです。

従来のサイバーセキュリティで高度な脅威エミュレーションシナリオの計画に広く使用されているMITRE ATT&CK®フレームワークと同様に、ATLASは新たな攻撃に対する防御準備をより良く理解するための検索可能なTTPセットを提供します。

さらに、Open Web Application Security Project (OWASP)は、LLMを利用するアプリケーションで見られる最も重大な脆弱性の[「トップ10リスト」](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)を作成しました。このリストは、前述のデータポイズニングや以下のような脅威のリスクを強調しています：

- **プロンプトインジェクション**: 攻撃者が慎重に作成した入力を通じて大規模言語モデル（LLM）を操作し、意図された挙動を超えた動作をさせる技術。
- **サプライチェーンの脆弱性**: LLMが使用するアプリケーションを構成するコンポーネントやソフトウェア（Pythonモジュールや外部データセットなど）が侵害されることで、予期しない結果やバイアス、基盤インフラの脆弱性が生じる可能性。
- **過度な依存**: LLMは誤りを犯しやすく、不正確または安全でない結果を提供することがあります。いくつかの記録された事例では、人々が結果をそのまま受け入れ、意図しない現実世界での悪影響を招いています。

Microsoft Cloud AdvocateのRod Trentは、これらおよびその他の新たなAI脅威について深く掘り下げ、これらのシナリオに最善の対処法を提供する無料の電子書籍[Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)を執筆しています。

## AIシステムとLLMのセキュリティテスト

人工知能（AI）はさまざまな分野や産業を変革し、社会に新たな可能性と利益をもたらしています。しかし、AIはデータプライバシー、バイアス、説明可能性の欠如、悪用の可能性など、重大な課題やリスクも伴います。そのため、AIシステムが倫理的および法的基準を遵守し、ユーザーや関係者から信頼されるようにすることが重要です。

セキュリティテストは、AIシステムやLLMのセキュリティを評価し、その脆弱性を特定して利用するプロセスです。これは開発者、ユーザー、または第三者の監査人によって、テストの目的や範囲に応じて実施されます。AIシステムやLLMに対する一般的なセキュリティテスト方法には以下があります：

- **データのサニタイズ**: トレーニングデータやAIシステムまたはLLMの入力から機密情報や個人情報を削除または匿名化するプロセス。データ漏洩や悪意ある操作を防ぐために、機密データや個人データの露出を減らします。
- **敵対的テスト**: 敵対的な例を生成してAIシステムやLLMの入力または出力に適用し、その堅牢性や敵対的攻撃への耐性を評価するプロセス。敵対的テストは、攻撃者によって悪用される可能性のあるAIシステムやLLMの脆弱性や弱点を特定し、緩和するのに役立ちます。
- **モデル検証**: AIシステムやLLMのモデルパラメータやアーキテクチャの正確性と完全性を検証するプロセス。モデルが保護され、認証されていることを確認することで、モデルの盗難を防ぐのに役立ちます。
- **出力の検証**: AIシステムやLLMの出力の品質と信頼性を検証するプロセス。出力が一貫性があり正確であることを確認することで、悪意ある操作を検出し修正するのに役立ちます。

OpenAIは、AIの安全性向上に貢献することを目的としたレッドチーミングネットワークイニシアチブの一環として、一連の_安全性評価_を設定しています。

> 評価は簡単なQ&Aテストからより複雑なシミュレーションまで幅広く行われます。具体例として、OpenAIがAIの挙動をさまざまな角度から評価するために開発した評価例を以下に示します：

#### 説得力

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AIシステムが別のAIシステムに秘密の言葉を言わせる能力を評価。
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AIシステムが別のAIシステムに寄付を促す能力を評価。
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AIシステムが別のAIシステムの政治的提案への支持を影響する能力を評価。

#### ステガノグラフィー（隠れたメッセージ）

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AIシステムが別のAIシステムに気付かれずに秘密のメッセージを送る能力を評価。
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AIシステムがメッセージを圧縮・解凍して秘密のメッセージを隠す能力を評価。
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AIシステムが直接的なコミュニケーションなしで別のAIシステムと協調する能力を評価。

### AIセキュリティ

AIシステムを悪意ある攻撃、誤用、または意図しない結果から保護することを目指すことが重要です。これには以下のような措置が含まれます：

- AIモデルのトレーニングや実行に使用されるデータとアルゴリズムの保護
- AIシステムへの不正アクセス、操作、または妨害の防止
- AIシステムにおけるバイアス、差別、倫理的問題の検出と緩和
- AIの意思決定や行動の責任性、透明性、説明可能性の確保
- AIシステムの目標や価値を人間や社会のものと一致させること

AIセキュリティは、AIシステムとデータの整合性、可用性、機密性を確保するために重要です。AIセキュリティの課題と機会には以下があります：

- **機会**: サイバーセキュリティ戦略にAIを組み込むことで、脅威の特定や対応時間の改善に重要な役割を果たすことができます。AIはフィッシング、マルウェア、ランサムウェアなどのサイバー攻撃の検出と緩和を自動化および強化するのに役立ちます。
- **課題**: AIは攻撃者によっても利用され、偽のコンテンツ生成、ユーザーのなりすまし、AIシステムの脆弱性の悪用などの高度な攻撃を仕掛けることができます。そのため、AI開発者は誤用に対して堅牢で耐性のあるシステムを設計する責任があります。

### データ保護

LLMは使用するデータのプライバシーとセキュリティにリスクをもたらす可能性があります。例えば、LLMはトレーニングデータから個人名、住所、パスワード、クレジットカード番号などの機密情報を記憶して漏洩する可能性があります。また、悪意ある行為者によって操作されたり攻撃されたりして、その脆弱性やバイアスを悪用される可能性もあります。そのため、これらのリスクを認識し、LLMで使用するデータを保護するための適切な措置を講じることが重要です。LLMで使用するデータを保護するために取るべきステップには以下があります：

- **LLMに共有するデータの量と種類を制限する**: 必要かつ関連性のあるデータのみを共有し、機密性、秘密性、個人情報を含むデータの共有を避けます。ユーザーはまた、共有するデータを匿名化または暗号化するべきです。例えば、識別情報を削除またはマスクしたり、安全な通信チャネルを使用したりすることが挙げられます。
- **LLMが生成するデータを検証する**: LLMが生成する出力の正確性と品質を常に確認し、不適切な情報が含まれていないことを保証します。
- **データ漏洩やインシデントを報告し警告する**: LLMの異常な活動や挙動に注意を払い、例えば無関係、不正確、攻撃的、有害なテキストを生成する場合は、データ漏洩やセキュリティインシデントの兆候である可能性があります。

データセキュリティ、ガバナンス、コンプライアンスは、マルチクラウド環境でデータとAIの力を活用したいと考える組織にとって重要です。すべてのデータを保護し、管理することは複雑で多面的な取り組みです。構造化データ、非構造化データ、AIによって生成されたデータなど、さまざまな種類のデータを複数のクラウドにわたって保護し、既
現実世界の脅威を模倣することは、AIシステムの強靭性を構築するための標準的な手法とされており、同様のツール、戦術、手順を使用してシステムのリスクを特定し、防御側の対応をテストします。

> AIのレッドチーミングの実践は進化し、より広範な意味を持つようになりました。それはセキュリティの脆弱性を探るだけでなく、潜在的に有害なコンテンツの生成など、他のシステム障害を探ることも含まれます。AIシステムには新しいリスクが伴い、レッドチーミングはプロンプトインジェクションや根拠のないコンテンツの生成など、これらの新しいリスクを理解するための重要な手法です。 - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![レッドチーミングのためのガイダンスとリソース](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.ja.png)]()

以下は、MicrosoftのAIレッドチームプログラムを形成した重要な洞察です。

1. **AIレッドチーミングの広範な範囲:**
   AIレッドチーミングは、セキュリティと責任あるAI（RAI）の成果の両方を含むようになりました。従来、レッドチーミングはセキュリティ面に焦点を当て、モデルをベクトルとして扱っていました（例: 基盤となるモデルの盗難）。しかし、AIシステムは新しいセキュリティ脆弱性（例: プロンプトインジェクション、ポイズニング）をもたらし、特別な注意が必要です。セキュリティを超えて、AIレッドチーミングは公平性の問題（例: ステレオタイプ化）や有害なコンテンツ（例: 暴力の美化）も探ります。これらの問題を早期に特定することで、防御への投資を優先することができます。
2. **悪意ある失敗と善意の失敗:**
   AIレッドチーミングは、悪意ある視点と善意の視点の両方から失敗を考慮します。例えば、新しいBingをレッドチーミングする際には、悪意ある攻撃者がシステムをどのように改ざんするかだけでなく、通常のユーザーが問題のあるコンテンツや有害なコンテンツに遭遇する可能性も探ります。従来のセキュリティレッドチーミングが主に悪意ある行為者に焦点を当てていたのに対し、AIレッドチーミングはより広範なペルソナと潜在的な失敗を考慮します。
3. **AIシステムの動的な性質:**
   AIアプリケーションは常に進化しています。大規模言語モデルのアプリケーションでは、開発者が変化する要件に適応します。継続的なレッドチーミングは、進化するリスクに対する継続的な警戒と適応を保証します。

AIレッドチーミングはすべてを網羅するものではなく、[役割ベースのアクセス制御（RBAC）](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst)や包括的なデータ管理ソリューションなどの追加の制御を補完する動きとして考慮されるべきです。これは、プライバシーとセキュリティを考慮しながら、偏見、有害なコンテンツ、誤情報を最小限に抑え、ユーザーの信頼を損なわないようにする安全で責任あるAIソリューションを採用することに焦点を当てたセキュリティ戦略を補完するものです。

以下は、AIシステムのリスクを特定し軽減するためにレッドチーミングがどのように役立つかをより深く理解するための追加の参考資料です：

- [大規模言語モデル（LLM）とそのアプリケーションのためのレッドチーミング計画](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [OpenAI Red Teaming Networkとは？](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AIレッドチーミング - より安全で責任あるAIソリューションを構築するための重要な実践](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)、AIシステムに対する現実世界の攻撃で使用される戦術と技術の知識ベース。

## 知識チェック

データの整合性を維持し、悪用を防ぐための良いアプローチは何でしょうか？

1. データアクセスとデータ管理のための強力な役割ベースの制御を持つ
1. データの誤表現や悪用を防ぐためにデータラベリングを実施し監査する
1. AIインフラストラクチャがコンテンツフィルタリングをサポートしていることを確認する

A:1、3つの推奨事項はすべて素晴らしいですが、適切なデータアクセス権限をユーザーに割り当てることは、LLMが使用するデータの操作や誤表現を防ぐ上で非常に効果的です。

## 🚀 チャレンジ

AI時代における[機密情報の管理と保護](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)についてさらに学びましょう。

## 素晴らしい仕事、学習を続けましょう

このレッスンを完了した後は、[生成AI学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成AIの知識をさらに深めましょう！

次のレッスン14では、[生成AIアプリケーションライフサイクル](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)について学びます！

---

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてご参照ください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は責任を負いません。